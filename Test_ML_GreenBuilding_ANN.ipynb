{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T12:14:14.528231Z",
     "start_time": "2024-10-27T12:14:14.522015Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('Model', exist_ok=True)\n",
    "os.makedirs('Excel', exist_ok=True)\n",
    "os.makedirs('Gambar', exist_ok=True)\n",
    "os.makedirs('Gambar/Heatmap', exist_ok=True)\n",
    "os.makedirs('Gambar/Best Worst', exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:14:15.910046Z",
     "start_time": "2024-10-27T12:14:15.477894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the provided file paths\n",
    "input_data_path = 'Data/Data Testing Real (Input).csv'\n",
    "output_data_path = 'Data/Data Testing Real (Output).csv'\n",
    "\n",
    "# Read the CSV files without headers\n",
    "input_data = pd.read_csv(input_data_path)\n",
    "output_data = pd.read_csv(output_data_path)\n",
    "\n",
    "# Split input data into two arrays: first column and the rest of the columns\n",
    "data_coor = input_data.iloc[:, 0].values  # First column\n",
    "data_x = input_data.iloc[:, 1:].values  # Remaining columns\n",
    "data_y = output_data.values # Output data"
   ],
   "id": "9c5b6f1700b5371",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:14:17.363164Z",
     "start_time": "2024-10-27T12:14:16.837930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from joblib import load\n",
    "\n",
    "# Assign the Column Name\n",
    "columns_name = ['Window', 'AC', 'Fan', 'Solar Radiation', 'Outdoor Temperature', 'Wind Velocity', 'Wind Direction', 'Outdoor Relative Humidity']\n",
    "\n",
    "# Load the Scaler\n",
    "scaler_X = load('Model/Scaler_X.joblib')\n",
    "scaler_Y = load('Model/Scaler_Y.joblib')\n",
    "\n",
    "# Transform the data\n",
    "data_x = pd.DataFrame(data_x)\n",
    "data_x.columns = columns_name\n",
    "data_x = scaler_X.transform(data_x)\n"
   ],
   "id": "9883fceda5d13e02",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:14:21.564823Z",
     "start_time": "2024-10-27T12:14:18.867913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to tensor if needed and move to the correct device\n",
    "Tensor_X = torch.tensor(data_x, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define your model architecture\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
    "        super(ANNModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))  # Dropout to prevent overfitting\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))  # Output layer (no activation for regression)\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Load the best hyperparameters\n",
    "best_hyperparameters = torch.load('Model/Best_Hyperparameters.pth')\n",
    "\n",
    "# Extract each parameter\n",
    "best_hidden_layers = best_hyperparameters['hidden_layers']\n",
    "best_hidden_units = [best_hyperparameters[f'n_units_l{i}'] for i in range(best_hidden_layers)]\n",
    "best_dropout_rate = best_hyperparameters['dropout_rate']\n",
    "best_lr = best_hyperparameters['lr']\n",
    "best_optimizer_name = best_hyperparameters['optimizer']\n",
    "\n",
    "# Re-initialize the model with the loaded best hyperparameters\n",
    "model = ANNModel(\n",
    "    input_dim=8,\n",
    "    hidden_dims=best_hidden_units,\n",
    "    output_dim=462,\n",
    "    dropout_rate=best_dropout_rate\n",
    ").to(device)  # Ensure 'device' is defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Set up the optimizer using the loaded hyperparameter\n",
    "best_optimizer = getattr(optim, best_optimizer_name)(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('Model/ANN_Model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Predict with the model\n",
    "with torch.no_grad():  # No gradients needed for inference\n",
    "    predictions = model(Tensor_X)\n",
    "\n",
    "# Convert predictions to a numpy array or DataFrame as needed\n",
    "y_pred = predictions.cpu().numpy()"
   ],
   "id": "ac42849d648e92bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Athal\\AppData\\Local\\Temp\\ipykernel_12436\\1626799341.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_hyperparameters = torch.load('Model/Best_Hyperparameters.pth')\n",
      "C:\\Users\\Athal\\AppData\\Local\\Temp\\ipykernel_12436\\1626799341.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Model/ANN_Model.pt'))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:14:27.275275Z",
     "start_time": "2024-10-27T12:14:27.266753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inverse transform the scaled data\n",
    "y_pred = scaler_Y.inverse_transform(y_pred)\n",
    "\n",
    "# Initialize a list to store each reshaped array\n",
    "reshaped_data = []\n",
    "\n",
    "# Iterate through each row and reshape it into (154, 3)\n",
    "for row in y_pred:\n",
    "    reshaped_row = row.reshape(154, 3)\n",
    "    reshaped_data.append(reshaped_row)\n",
    "\n",
    "# Convert the list to a NumPy array if needed\n",
    "reshaped_data = np.array(reshaped_data)"
   ],
   "id": "9164120f834ccf9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:16:48.874898Z",
     "start_time": "2024-10-27T12:16:48.869361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dictionary coordinate ID to coordinate\n",
    "case_dict = {\n",
    "    1: 123,\n",
    "    2: 118,\n",
    "    3: 114,\n",
    "    4: 39,\n",
    "    5: 34,\n",
    "    6: 30,\n",
    "}\n",
    "\n",
    "result_data = []\n",
    "\n",
    "# Get the point coordinate values\n",
    "for i, coor in enumerate(data_coor):\n",
    "    # Get the reshaped data for the current point\n",
    "    data_point = reshaped_data[i, case_dict[coor]]\n",
    "    result_data.append(data_point)\n",
    "\n",
    "result_array = np.array([arr.reshape(3) for arr in result_data])"
   ],
   "id": "ca870dac62c118cb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T12:20:21.102363Z",
     "start_time": "2024-10-27T12:20:21.095850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calculate metrics definition\n",
    "def calculate_metrics(true, pred):\n",
    "    r2 = r2_score(true, pred, multioutput='raw_values')\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    return r2, mse, mae\n",
    "\n",
    "# Calculate score\n",
    "r2_train, mse_train, mae_train = calculate_metrics(data_y, result_array)\n",
    "\n",
    "\n",
    "# Output the results\n",
    "print(\"\\n===== Model Performance =====\")\n",
    "print(f\"  R2 Score: {r2_train}\")\n",
    "print(f\"  MSE: {mse_train:.4f}\")\n",
    "print(f\"  MAE: {mae_train:.4f}\")\n"
   ],
   "id": "f692719b8e86208b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Performance =====\n",
      "  R2 Score: [-0.19682902 -0.85306168 -1.59268381]\n",
      "  MSE: 48.1364\n",
      "  MAE: 3.6711\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c90640f9b6b410f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
