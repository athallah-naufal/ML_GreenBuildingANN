{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:20:39.590757Z",
     "start_time": "2024-09-26T15:20:39.586862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Path to Save\n",
    "file_model_path = \"D:\\\\Cloud Folder\\\\OneDrive - UGM 365\\\\(00) Penelitian Ibuk\\\\Model\\\\240924 - Model ANN\\\\\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(f'{file_model_path}Model', exist_ok=True)\n",
    "os.makedirs(f'{file_model_path}Excel', exist_ok=True)\n",
    "os.makedirs(f'{file_model_path}Gambar', exist_ok=True)\n",
    "os.makedirs(f'{file_model_path}Gambar\\\\Heatmap', exist_ok=True)\n",
    "os.makedirs(f'{file_model_path}Gambar\\\\Best Worst', exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNK0E1PYbMYP"
   },
   "source": [
    "# **CREATE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lskXaeCLhtgo",
    "ExecuteTime": {
     "end_time": "2024-09-25T09:40:49.136663Z",
     "start_time": "2024-09-25T09:40:46.571138Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "input_x = np.load('D:\\\\Cloud Folder\\\\OneDrive - UGM 365\\\\(00) Penelitian Ibuk\\\\Model\\\\input_x.npy')\n",
    "output_y = np.load('D:\\\\Cloud Folder\\\\OneDrive - UGM 365\\\\(00) Penelitian Ibuk\\\\Model\\\\input_x.npy')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:47:35.539534Z",
     "start_time": "2024-09-25T07:31:11.126593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===========================\n",
    "# 1. Data Preparation\n",
    "# ===========================\n",
    "\n",
    "data_x = input_x  # Placeholder input features\n",
    "data_y = input_y  # Placeholder output variables\n",
    "\n",
    "# ===========================\n",
    "# 2. Model Definition\n",
    "# ===========================\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
    "        super(ANNModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))  # Dropout to prevent overfitting\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))  # Output layer (no activation for regression)\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ===========================\n",
    "# 3. Hyperparameter Definition with Optuna\n",
    "# ===========================\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_layers = trial.suggest_int('hidden_layers', 2, 5)  # Number of layers\n",
    "    hidden_units = [trial.suggest_int(f'n_units_l{i}', 128, 1024) for i in range(hidden_layers)]  # Units per layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)  # Dropout rate\n",
    "    learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)  # Learning rate\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD', 'AdamW'])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ANNModel(input_dim=8, hidden_dims=hidden_units, output_dim=462, dropout_rate=dropout_rate).cuda()\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training the model\n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs)\n",
    "            val_preds.append(outputs.cpu().numpy())\n",
    "            val_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    val_preds = np.vstack(val_preds)\n",
    "    val_targets = np.vstack(val_targets)\n",
    "\n",
    "    # Return validation MSE as the objective to minimize\n",
    "    r2 = r2_score(val_targets, val_preds)\n",
    "    return r2\n",
    "\n",
    "# Calculate metrics definition\n",
    "def calculate_metrics(true, pred):\n",
    "    r2 = r2_score(true, pred, multioutput='uniform_average')\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    return r2, mse, mae\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Split the data into training (80%) and testing (20%) datasets\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        data_x, data_y, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Further split the training data into training and validation sets (80:20 of training data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_train_full = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "    y_train_full = torch.tensor(y_train_full, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Create PyTorch Datasets and Dataloaders\n",
    "    train_dataset = data.TensorDataset(X_train, y_train)\n",
    "    train_full_dataset = data.TensorDataset(X_train_full, y_train_full)\n",
    "    val_dataset = data.TensorDataset(X_val, y_val)\n",
    "    test_dataset = data.TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    train_full_loader = data.DataLoader(train_full_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create Optuna study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    print(\"Hyperparameter optimization completed.\")\n",
    "    \n",
    "    # Display the best hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best Trial:\")\n",
    "    print(f\"  Value (MSE): {best_trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Extract the best hyperparameters\n",
    "    best_hidden_layers = best_trial.params['hidden_layers']\n",
    "    best_hidden_units = [best_trial.params[f'n_units_l{i}'] for i in range(best_hidden_layers)]\n",
    "    best_dropout_rate = best_trial.params['dropout_rate']\n",
    "    best_lr = best_trial.params['lr']\n",
    "    best_optimizer_name = best_trial.params['optimizer']\n",
    "    \n",
    "    # ===========================\n",
    "    # 4. Training the Best Model\n",
    "    # ===========================\n",
    "    \n",
    "    # Initialize the best model\n",
    "    best_model = ANNModel(input_dim=8, hidden_dims=best_hidden_units, output_dim=462, dropout_rate=best_dropout_rate).to(device)\n",
    "    best_optimizer = getattr(optim, best_optimizer_name)(best_model.parameters(), lr=best_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Define number of epochs for final training\n",
    "    final_epochs = 500\n",
    "    \n",
    "    # Lists to store loss values\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"Starting training of the best model...\")\n",
    "    for epoch in range(1, final_epochs + 1):\n",
    "        # Training phase\n",
    "        best_model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, targets in train_full_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            best_optimizer.zero_grad()\n",
    "            outputs = best_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            best_optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "    \n",
    "        # Validation phase\n",
    "        best_model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = best_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "    \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch}/{final_epochs} - Training Loss: {epoch_train_loss:.4f} - Validation Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training of the best model completed.\")\n",
    "    \n",
    "    # ===========================\n",
    "    # 5. Evaluation\n",
    "    # ===========================\n",
    "    \n",
    "    best_model.eval()\n",
    "    \n",
    "    # Function to get predictions from a loader\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        targets_all = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = best_model(inputs)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                targets_all.append(targets.numpy())\n",
    "        return np.vstack(preds), np.vstack(targets_all)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_train_pred, y_train_true = get_predictions(train_full_loader)\n",
    "    y_test_pred, y_test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Training Metrics\n",
    "    r2_train, mse_train, mae_train = calculate_metrics(y_train_true, y_train_pred)\n",
    "    \n",
    "    # Testing Metrics\n",
    "    r2_test, mse_test, mae_test = calculate_metrics(y_test_true, y_test_pred)\n",
    "    \n",
    "    # Output the results\n",
    "    print(\"\\n===== Model Performance =====\")\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"  R2 Score: {r2_train:.4f}\")\n",
    "    print(f\"  MSE: {mse_train:.4f}\")\n",
    "    print(f\"  MAE: {mae_train:.4f}\")\n",
    "    \n",
    "    print(\"\\nTesting Set:\")\n",
    "    print(f\"  R2 Score: {r2_test:.4f}\")\n",
    "    print(f\"  MSE: {mse_test:.4f}\")\n",
    "    print(f\"  MAE: {mae_test:.4f}\")\n",
    "    \n",
    "    if r2_test > 0.8:\n",
    "        print(\"Desired R2 score achieved! Stopping hyperparameter tuning.\")\n",
    "        # Save the model\n",
    "        torch.save(best_model.state_dict(), f'{file_model_path}Model/ANN_Model.pt')\n",
    "        \n",
    "        # Save predictions and actual values as NumPy arrays\n",
    "        np.save(f'{file_model_path}Array/y_test_pred.npy', y_test_pred)\n",
    "        np.save(f'{file_model_path}Array/y_test_true.npy', y_test_true)\n",
    "        np.save(f'{file_model_path}Array/y_train_pred.npy', y_train_pred)\n",
    "        np.save(f'{file_model_path}Array/y_train_true.npy', y_train_true)\n",
    "    \n",
    "        break  # Exit the loop if the desired R2 is achieved\n",
    "    \n",
    "# ===========================\n",
    "# 6. Plotting Training and Validation Loss\n",
    "# ===========================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, final_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, final_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check some predictions vs actual values (for a few samples)\n",
    "sample_indices = np.random.choice(len(X_test), size=5)\n",
    "y_pred_samples = best_model(X_test[sample_indices].to(device)).cpu().detach().numpy()\n",
    "y_true_samples = y_test[sample_indices].numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"True: {y_true_samples[i][:5]}\")  # Print first 5 outputs as example\n",
    "    print(f\"Predicted: {y_pred_samples[i][:5]}\\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:31:11,146] A new study created in memory with name: no-name-07ef1dd7-86f1-4205-8f5f-501828a16324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:31:12,696] Trial 0 finished with value: -170931664.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 297, 'n_units_l1': 620, 'n_units_l2': 1010, 'n_units_l3': 329, 'dropout_rate': 0.3354800326102301, 'lr': 0.0001319850867789603, 'optimizer': 'AdamW'}. Best is trial 0 with value: -170931664.0.\n",
      "[I 2024-09-25 14:31:13,355] Trial 1 finished with value: -211855776.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 319, 'n_units_l1': 621, 'dropout_rate': 0.45407831345942223, 'lr': 1.3424524881423998e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: -170931664.0.\n",
      "[I 2024-09-25 14:31:14,258] Trial 2 finished with value: -41771964.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 208, 'n_units_l1': 436, 'n_units_l2': 247, 'n_units_l3': 678, 'n_units_l4': 153, 'dropout_rate': 0.5672626850378772, 'lr': 0.00445958766140791, 'optimizer': 'Adam'}. Best is trial 2 with value: -41771964.0.\n",
      "[I 2024-09-25 14:31:14,887] Trial 3 finished with value: -174708912.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 298, 'n_units_l1': 734, 'dropout_rate': 0.35827764982357746, 'lr': 1.7012049147397766e-05, 'optimizer': 'RMSprop'}. Best is trial 2 with value: -41771964.0.\n",
      "[I 2024-09-25 14:31:15,489] Trial 4 finished with value: -1505723136.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 970, 'n_units_l1': 682, 'n_units_l2': 936, 'dropout_rate': 0.43784378414549513, 'lr': 0.001912000430306349, 'optimizer': 'SGD'}. Best is trial 2 with value: -41771964.0.\n",
      "[I 2024-09-25 14:31:16,588] Trial 5 finished with value: -550050048.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 688, 'n_units_l1': 1021, 'n_units_l2': 195, 'n_units_l3': 941, 'n_units_l4': 981, 'dropout_rate': 0.5674779934868881, 'lr': 1.4587122884894712e-05, 'optimizer': 'AdamW'}. Best is trial 2 with value: -41771964.0.\n",
      "[I 2024-09-25 14:31:17,431] Trial 6 finished with value: -14877260.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 912, 'n_units_l1': 856, 'n_units_l2': 783, 'dropout_rate': 0.28113028036204346, 'lr': 0.001390694313345759, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:18,474] Trial 7 finished with value: -42356828.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 811, 'n_units_l1': 938, 'n_units_l2': 684, 'n_units_l3': 539, 'n_units_l4': 437, 'dropout_rate': 0.37813615104075304, 'lr': 0.00048220857528636973, 'optimizer': 'Adam'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:19,075] Trial 8 finished with value: -92760320.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 911, 'n_units_l1': 237, 'dropout_rate': 0.4261305480661298, 'lr': 0.0012118331745830312, 'optimizer': 'RMSprop'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:19,878] Trial 9 finished with value: -61949268.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 617, 'n_units_l1': 192, 'n_units_l2': 774, 'n_units_l3': 217, 'dropout_rate': 0.22395476416170634, 'lr': 0.00030731731566325117, 'optimizer': 'Adam'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:20,695] Trial 10 finished with value: -52981344.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 491, 'n_units_l1': 841, 'n_units_l2': 478, 'dropout_rate': 0.22958738503186016, 'lr': 0.006122930560313452, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:21,390] Trial 11 finished with value: -2865125888.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 167, 'n_units_l1': 400, 'n_units_l2': 152, 'n_units_l3': 862, 'dropout_rate': 0.588852684298357, 'lr': 0.007680710321188357, 'optimizer': 'SGD'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:22,158] Trial 12 finished with value: -30931664.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 481, 'n_units_l1': 428, 'n_units_l2': 449, 'dropout_rate': 0.5095353728765254, 'lr': 0.0018418600041235531, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:22,947] Trial 13 finished with value: -37644736.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 491, 'n_units_l1': 465, 'n_units_l2': 488, 'dropout_rate': 0.5036391511337783, 'lr': 0.0010525598269269288, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:23,742] Trial 14 finished with value: -41541020.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 767, 'n_units_l1': 341, 'n_units_l2': 389, 'dropout_rate': 0.2936639544151098, 'lr': 0.00011728561721947633, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:24,602] Trial 15 finished with value: -40398752.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 517, 'n_units_l1': 801, 'n_units_l2': 719, 'dropout_rate': 0.2844030125824056, 'lr': 0.002264678773021806, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:25,517] Trial 16 finished with value: -76877976.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 395, 'n_units_l1': 523, 'n_units_l2': 844, 'n_units_l3': 466, 'dropout_rate': 0.47950310944993535, 'lr': 0.0006777187509973595, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:26,198] Trial 17 finished with value: -162428000.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 842, 'n_units_l1': 317, 'dropout_rate': 0.5268080417083707, 'lr': 0.0002133838992605756, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:26,946] Trial 18 finished with value: -69600960.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 646, 'n_units_l1': 545, 'n_units_l2': 592, 'dropout_rate': 0.2908361362179992, 'lr': 0.002394333643134619, 'optimizer': 'RMSprop'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:27,835] Trial 19 finished with value: -3113657088.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 731, 'n_units_l1': 893, 'n_units_l2': 365, 'n_units_l3': 705, 'dropout_rate': 0.4044893832251904, 'lr': 0.003656578537716514, 'optimizer': 'SGD'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:28,677] Trial 20 finished with value: -119538664.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 1019, 'n_units_l1': 713, 'dropout_rate': 0.3380473905000188, 'lr': 5.1339314591004604e-05, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:29,613] Trial 21 finished with value: -23114648.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 449, 'n_units_l1': 483, 'n_units_l2': 549, 'dropout_rate': 0.505905992803663, 'lr': 0.0009670960255820002, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:30,441] Trial 22 finished with value: -23583130.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 405, 'n_units_l1': 496, 'n_units_l2': 598, 'dropout_rate': 0.5226416377062646, 'lr': 0.0008286912663433701, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:31,404] Trial 23 finished with value: -33759992.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 395, 'n_units_l1': 582, 'n_units_l2': 567, 'dropout_rate': 0.5370065803548253, 'lr': 0.0008550884353621483, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:32,369] Trial 24 finished with value: -68319576.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 572, 'n_units_l1': 506, 'n_units_l2': 602, 'dropout_rate': 0.479021975698995, 'lr': 0.0004637768070341815, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:33,450] Trial 25 finished with value: -52331748.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 396, 'n_units_l1': 303, 'n_units_l2': 833, 'n_units_l3': 175, 'dropout_rate': 0.20117494588260476, 'lr': 0.0013464116274495058, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:34,439] Trial 26 finished with value: -64786884.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 565, 'n_units_l1': 665, 'n_units_l2': 675, 'dropout_rate': 0.46675331162006006, 'lr': 0.0004983538415191757, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:35,071] Trial 27 finished with value: -2336631296.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 428, 'n_units_l1': 759, 'dropout_rate': 0.598710482995729, 'lr': 0.0001920940131929117, 'optimizer': 'SGD'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:35,848] Trial 28 finished with value: -43247724.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 237, 'n_units_l1': 378, 'n_units_l2': 549, 'dropout_rate': 0.3939551471721634, 'lr': 0.0035039247162262166, 'optimizer': 'RMSprop'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:37,005] Trial 29 finished with value: -475555808.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 348, 'n_units_l1': 600, 'n_units_l2': 646, 'n_units_l3': 1020, 'dropout_rate': 0.5497366183597395, 'lr': 0.00010404385327272606, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:37,800] Trial 30 finished with value: -89618672.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 261, 'n_units_l1': 972, 'dropout_rate': 0.4969707326146231, 'lr': 0.0008374008267125452, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:38,726] Trial 31 finished with value: -26807382.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 462, 'n_units_l1': 457, 'n_units_l2': 389, 'dropout_rate': 0.5165568711177569, 'lr': 0.0015968790922300895, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:39,662] Trial 32 finished with value: -15175152.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 358, 'n_units_l1': 484, 'n_units_l2': 392, 'dropout_rate': 0.4417211933268379, 'lr': 0.001437229791339717, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:40,612] Trial 33 finished with value: -78884680.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 338, 'n_units_l1': 636, 'n_units_l2': 302, 'dropout_rate': 0.4497724148438315, 'lr': 0.0005954865030370383, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:41,579] Trial 34 finished with value: -61056040.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 543, 'n_units_l1': 510, 'n_units_l2': 765, 'dropout_rate': 0.4190258325617597, 'lr': 0.0003054490146519082, 'optimizer': 'Adam'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:42,416] Trial 35 finished with value: -52696852.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 358, 'n_units_l1': 550, 'dropout_rate': 0.45797787120835326, 'lr': 0.004472039446529122, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:43,270] Trial 36 finished with value: -30198230.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 128, 'n_units_l1': 473, 'n_units_l2': 504, 'dropout_rate': 0.26359422799194976, 'lr': 0.0024148961224825095, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:44,076] Trial 37 finished with value: -36768016.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 274, 'n_units_l1': 260, 'n_units_l2': 421, 'dropout_rate': 0.560355665173029, 'lr': 0.0010953245393178408, 'optimizer': 'Adam'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:44,950] Trial 38 finished with value: -121151128.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 441, 'n_units_l1': 141, 'n_units_l2': 996, 'n_units_l3': 376, 'dropout_rate': 0.36005987466259337, 'lr': 0.00040376791698257665, 'optimizer': 'RMSprop'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:45,574] Trial 39 finished with value: -3664155904.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 891, 'n_units_l1': 369, 'dropout_rate': 0.4343183332784802, 'lr': 6.79410546855673e-05, 'optimizer': 'SGD'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:46,671] Trial 40 finished with value: -453383808.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 626, 'n_units_l1': 651, 'n_units_l2': 525, 'n_units_l3': 750, 'dropout_rate': 0.4849591507897546, 'lr': 2.8192605683490145e-05, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:47,584] Trial 41 finished with value: -26706300.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 440, 'n_units_l1': 462, 'n_units_l2': 307, 'dropout_rate': 0.5156821453233318, 'lr': 0.0015366180938248147, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:48,500] Trial 42 finished with value: -20023576.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 332, 'n_units_l1': 435, 'n_units_l2': 282, 'dropout_rate': 0.5446422437993458, 'lr': 0.0007333361250455151, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:49,435] Trial 43 finished with value: -26531250.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 312, 'n_units_l1': 421, 'n_units_l2': 262, 'dropout_rate': 0.5803457462391606, 'lr': 0.0008607522406142687, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:50,374] Trial 44 finished with value: -23745238.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 185, 'n_units_l1': 587, 'n_units_l2': 323, 'dropout_rate': 0.54214383182434, 'lr': 0.0006904170886584027, 'optimizer': 'AdamW'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:51,573] Trial 45 finished with value: -16440089.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 298, 'n_units_l1': 500, 'n_units_l2': 214, 'n_units_l3': 818, 'n_units_l4': 986, 'dropout_rate': 0.5702016900568795, 'lr': 0.00033705317244441007, 'optimizer': 'Adam'}. Best is trial 6 with value: -14877260.0.\n",
      "[I 2024-09-25 14:31:52,790] Trial 46 finished with value: -6262894.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 305, 'n_units_l1': 403, 'n_units_l2': 217, 'n_units_l3': 859, 'n_units_l4': 1016, 'dropout_rate': 0.5739251170532995, 'lr': 0.00023631461149432252, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:54,005] Trial 47 finished with value: -22904012.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 220, 'n_units_l1': 404, 'n_units_l2': 211, 'n_units_l3': 804, 'n_units_l4': 1024, 'dropout_rate': 0.5818122925831781, 'lr': 0.00023844854823692444, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:55,172] Trial 48 finished with value: -43218112.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 299, 'n_units_l1': 279, 'n_units_l2': 145, 'n_units_l3': 630, 'n_units_l4': 780, 'dropout_rate': 0.5519920376535427, 'lr': 0.0001580222355226638, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:56,156] Trial 49 finished with value: -54855200.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 252, 'n_units_l1': 221, 'n_units_l2': 256, 'n_units_l3': 845, 'n_units_l4': 764, 'dropout_rate': 0.5679518285137016, 'lr': 0.00029006096221750687, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:57,186] Trial 50 finished with value: -66327764.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 159, 'n_units_l1': 545, 'n_units_l2': 199, 'n_units_l3': 1019, 'n_units_l4': 804, 'dropout_rate': 0.31350716319894595, 'lr': 0.0003980316264552796, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:58,391] Trial 51 finished with value: -35299696.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 213, 'n_units_l1': 352, 'n_units_l2': 203, 'n_units_l3': 820, 'n_units_l4': 1014, 'dropout_rate': 0.5812965684964319, 'lr': 0.00022048035321553048, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:31:59,610] Trial 52 finished with value: -32963330.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 209, 'n_units_l1': 422, 'n_units_l2': 224, 'n_units_l3': 770, 'n_units_l4': 1011, 'dropout_rate': 0.5976628736033524, 'lr': 0.0002541611367519754, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:00,840] Trial 53 finished with value: -25463506.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 350, 'n_units_l1': 394, 'n_units_l2': 130, 'n_units_l3': 919, 'n_units_l4': 902, 'dropout_rate': 0.5751692506657057, 'lr': 8.901737299132517e-05, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:02,033] Trial 54 finished with value: -65605800.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 316, 'n_units_l1': 407, 'n_units_l2': 267, 'n_units_l3': 559, 'n_units_l4': 640, 'dropout_rate': 0.5323671089235238, 'lr': 0.0001703497302028414, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:03,322] Trial 55 finished with value: -21342998.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 279, 'n_units_l1': 348, 'n_units_l2': 169, 'n_units_l3': 921, 'n_units_l4': 889, 'dropout_rate': 0.5648862872793654, 'lr': 0.00013789722777107042, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:04,825] Trial 56 finished with value: -158602336.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 285, 'n_units_l1': 336, 'n_units_l2': 168, 'n_units_l3': 910, 'n_units_l4': 860, 'dropout_rate': 0.5566250515042358, 'lr': 4.4257002364205924e-05, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:06,115] Trial 57 finished with value: -85736920.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 369, 'n_units_l1': 854, 'n_units_l2': 348, 'n_units_l3': 959, 'n_units_l4': 629, 'dropout_rate': 0.2649531260851971, 'lr': 0.0001546074771740421, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:07,309] Trial 58 finished with value: -22967156.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 958, 'n_units_l1': 448, 'n_units_l2': 172, 'n_units_l3': 712, 'dropout_rate': 0.3383330883364931, 'lr': 0.0006181825243544961, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:08,514] Trial 59 finished with value: -21406506.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 778, 'n_units_l1': 696, 'n_units_l2': 282, 'n_units_l3': 878, 'n_units_l4': 901, 'dropout_rate': 0.564635198556182, 'lr': 0.000375556886732347, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:10,384] Trial 60 finished with value: -2611399936.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 515, 'n_units_l1': 766, 'n_units_l2': 230, 'n_units_l3': 614, 'dropout_rate': 0.37277566756978264, 'lr': 0.002751112840515236, 'optimizer': 'SGD'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:12,653] Trial 61 finished with value: -9661234.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 805, 'n_units_l1': 710, 'n_units_l2': 288, 'n_units_l3': 901, 'n_units_l4': 897, 'dropout_rate': 0.5643795186872572, 'lr': 0.0003459068147048881, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:15,240] Trial 62 finished with value: -50811448.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 710, 'n_units_l1': 824, 'n_units_l2': 345, 'n_units_l3': 993, 'n_units_l4': 906, 'dropout_rate': 0.5449596729338582, 'lr': 0.0005185049495690096, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:17,412] Trial 63 finished with value: -61951216.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 845, 'n_units_l1': 872, 'n_units_l2': 445, 'n_units_l3': 800, 'n_units_l4': 909, 'dropout_rate': 0.5974807214700836, 'lr': 0.0003472554150742412, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:19,560] Trial 64 finished with value: -32743114.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1022, 'n_units_l1': 920, 'n_units_l2': 908, 'n_units_l3': 891, 'n_units_l4': 706, 'dropout_rate': 0.5310183809959667, 'lr': 0.0012782493631140774, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:21,604] Trial 65 finished with value: -646348544.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 969, 'n_units_l1': 312, 'n_units_l2': 174, 'n_units_l3': 956, 'n_units_l4': 451, 'dropout_rate': 0.5695668613969523, 'lr': 1.0673103284462854e-05, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:23,615] Trial 66 finished with value: -48285000.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 906, 'n_units_l1': 524, 'n_units_l2': 288, 'n_units_l3': 748, 'n_units_l4': 959, 'dropout_rate': 0.4884990920576484, 'lr': 0.0001283311375400747, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:25,455] Trial 67 finished with value: -67551504.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 659, 'n_units_l1': 1010, 'n_units_l2': 227, 'n_units_l3': 956, 'dropout_rate': 0.5863950563931813, 'lr': 0.0018462152835935456, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:27,395] Trial 68 finished with value: -97618144.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 846, 'n_units_l1': 556, 'n_units_l2': 386, 'n_units_l3': 849, 'n_units_l4': 834, 'dropout_rate': 0.3215540475865324, 'lr': 0.00010099852134971195, 'optimizer': 'RMSprop'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:28,733] Trial 69 finished with value: -1300460416.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 930, 'n_units_l1': 612, 'n_units_l2': 183, 'dropout_rate': 0.4135177028483747, 'lr': 0.0006935312611185307, 'optimizer': 'SGD'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:30,907] Trial 70 finished with value: -48584676.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 378, 'n_units_l1': 495, 'n_units_l2': 246, 'n_units_l3': 668, 'n_units_l4': 942, 'dropout_rate': 0.3874842050152985, 'lr': 0.0004631627207890092, 'optimizer': 'Adam'}. Best is trial 46 with value: -6262894.5.\n",
      "[I 2024-09-25 14:32:32,806] Trial 71 finished with value: -2361916.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 801, 'n_units_l1': 686, 'n_units_l2': 280, 'n_units_l3': 886, 'n_units_l4': 887, 'dropout_rate': 0.5602334405275238, 'lr': 0.0003795512396619183, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:34,666] Trial 72 finished with value: -36466256.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 803, 'n_units_l1': 781, 'n_units_l2': 306, 'n_units_l3': 902, 'n_units_l4': 848, 'dropout_rate': 0.5547260969696995, 'lr': 0.00025792901001623524, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:36,561] Trial 73 finished with value: -22820380.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 738, 'n_units_l1': 703, 'n_units_l2': 244, 'n_units_l3': 848, 'n_units_l4': 947, 'dropout_rate': 0.538048771137843, 'lr': 0.0002024662657910623, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:38,367] Trial 74 finished with value: -14175024.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 338, 'n_units_l1': 746, 'n_units_l2': 323, 'n_units_l3': 500, 'n_units_l4': 729, 'dropout_rate': 0.5253334322257546, 'lr': 0.0003089561524855449, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:39,784] Trial 75 finished with value: -53176648.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 415, 'n_units_l1': 667, 'n_units_l2': 332, 'dropout_rate': 0.49764470096161995, 'lr': 0.0005430439399078164, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:41,594] Trial 76 finished with value: -27755882.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 332, 'n_units_l1': 723, 'n_units_l2': 364, 'n_units_l3': 466, 'n_units_l4': 732, 'dropout_rate': 0.5194952850963753, 'lr': 0.00030894686324594813, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:43,093] Trial 77 finished with value: -67270232.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 867, 'n_units_l1': 810, 'n_units_l2': 417, 'dropout_rate': 0.469327533372354, 'lr': 0.0004340104445150206, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:45,021] Trial 78 finished with value: -54841004.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 934, 'n_units_l1': 744, 'n_units_l2': 751, 'n_units_l3': 456, 'n_units_l4': 823, 'dropout_rate': 0.23504960644219267, 'lr': 0.0009241634562780391, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:46,807] Trial 79 finished with value: -24999722.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 806, 'n_units_l1': 784, 'n_units_l2': 283, 'n_units_l3': 278, 'dropout_rate': 0.5255224130678497, 'lr': 0.0011394958442807609, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:48,322] Trial 80 finished with value: -36621288.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 878, 'n_units_l1': 746, 'n_units_l2': 320, 'dropout_rate': 0.5448013613284632, 'lr': 0.0016042849628832413, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:50,404] Trial 81 finished with value: -52234036.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 294, 'n_units_l1': 434, 'n_units_l2': 817, 'n_units_l3': 978, 'n_units_l4': 878, 'dropout_rate': 0.5725560978257181, 'lr': 0.0003434124137128068, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:52,382] Trial 82 finished with value: -74398608.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 249, 'n_units_l1': 374, 'n_units_l2': 632, 'n_units_l3': 509, 'n_units_l4': 963, 'dropout_rate': 0.5593042848489396, 'lr': 0.00017780871418200824, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:54,201] Trial 83 finished with value: -49792084.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 268, 'n_units_l1': 629, 'n_units_l2': 154, 'n_units_l3': 379, 'n_units_l4': 951, 'dropout_rate': 0.5887482574570593, 'lr': 0.0006976198943196266, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:56,171] Trial 84 finished with value: -83339768.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 325, 'n_units_l1': 679, 'n_units_l2': 208, 'n_units_l3': 806, 'n_units_l4': 138, 'dropout_rate': 0.5674369274462105, 'lr': 0.0002921608723974089, 'optimizer': 'Adam'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:32:58,203] Trial 85 finished with value: -42078660.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 375, 'n_units_l1': 479, 'n_units_l2': 262, 'n_units_l3': 910, 'n_units_l4': 1023, 'dropout_rate': 0.4468668932519036, 'lr': 0.0001479335907088535, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:00,730] Trial 86 finished with value: -1651586944.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 236, 'n_units_l1': 570, 'n_units_l2': 377, 'n_units_l3': 875, 'n_units_l4': 430, 'dropout_rate': 0.5499361551317627, 'lr': 7.963106532456185e-05, 'optimizer': 'SGD'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:04,505] Trial 87 finished with value: -328123104.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 180, 'n_units_l1': 526, 'n_units_l2': 129, 'dropout_rate': 0.5071752624615595, 'lr': 0.00025080535933122406, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:06,713] Trial 88 finished with value: -68617728.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 597, 'n_units_l1': 921, 'n_units_l2': 195, 'n_units_l3': 759, 'n_units_l4': 515, 'dropout_rate': 0.5372451576288231, 'lr': 0.002093008004617332, 'optimizer': 'Adam'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:08,208] Trial 89 finished with value: -24797462.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 309, 'n_units_l1': 354, 'n_units_l2': 292, 'dropout_rate': 0.5776775515786368, 'lr': 0.0007646156461407713, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:10,267] Trial 90 finished with value: -343769472.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 472, 'n_units_l1': 442, 'n_units_l2': 1018, 'n_units_l3': 521, 'n_units_l4': 677, 'dropout_rate': 0.5884827894586405, 'lr': 0.00011968921368474843, 'optimizer': 'AdamW'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:12,151] Trial 91 finished with value: -16998776.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 770, 'n_units_l1': 683, 'n_units_l2': 273, 'n_units_l3': 871, 'n_units_l4': 299, 'dropout_rate': 0.5635979282517635, 'lr': 0.0004051991162180145, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:14,052] Trial 92 finished with value: -43354000.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 825, 'n_units_l1': 732, 'n_units_l2': 408, 'n_units_l3': 934, 'n_units_l4': 235, 'dropout_rate': 0.5562482074631349, 'lr': 0.0005831186911425929, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:15,996] Trial 93 finished with value: -5311925.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 764, 'n_units_l1': 390, 'n_units_l2': 247, 'n_units_l3': 827, 'n_units_l4': 364, 'dropout_rate': 0.5607589536900187, 'lr': 0.00020540810882781739, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:18,067] Trial 94 finished with value: -20185784.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 764, 'n_units_l1': 644, 'n_units_l2': 239, 'n_units_l3': 836, 'n_units_l4': 207, 'dropout_rate': 0.5461981233110637, 'lr': 0.00020707440670664223, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:20,041] Trial 95 finished with value: -14183088.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 674, 'n_units_l1': 598, 'n_units_l2': 343, 'n_units_l3': 776, 'n_units_l4': 319, 'dropout_rate': 0.5148182588047989, 'lr': 0.00038214250549228104, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:21,958] Trial 96 finished with value: -10826893.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 751, 'n_units_l1': 685, 'n_units_l2': 343, 'n_units_l3': 785, 'n_units_l4': 353, 'dropout_rate': 0.5326562442216558, 'lr': 0.0003791811415318328, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:23,924] Trial 97 finished with value: -21636370.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 682, 'n_units_l1': 610, 'n_units_l2': 310, 'n_units_l3': 789, 'n_units_l4': 328, 'dropout_rate': 0.5132125537697112, 'lr': 0.00033379540996188753, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:25,849] Trial 98 finished with value: -31631692.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 746, 'n_units_l1': 576, 'n_units_l2': 901, 'n_units_l3': 738, 'n_units_l4': 367, 'dropout_rate': 0.5284722957778545, 'lr': 0.006749739525150629, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:27,726] Trial 99 finished with value: -32357536.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 710, 'n_units_l1': 647, 'n_units_l2': 458, 'n_units_l3': 677, 'n_units_l4': 331, 'dropout_rate': 0.5214579519087147, 'lr': 0.00027526086745689694, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:29,744] Trial 100 finished with value: -18261058.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 789, 'n_units_l1': 706, 'n_units_l2': 343, 'n_units_l3': 706, 'n_units_l4': 389, 'dropout_rate': 0.4984637878366015, 'lr': 0.0002253223620819863, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:31,691] Trial 101 finished with value: -31956538.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 758, 'n_units_l1': 696, 'n_units_l2': 263, 'n_units_l3': 864, 'n_units_l4': 251, 'dropout_rate': 0.5753186722558516, 'lr': 0.0003948329447950449, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:33,688] Trial 102 finished with value: -30035032.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 706, 'n_units_l1': 669, 'n_units_l2': 353, 'n_units_l3': 819, 'n_units_l4': 299, 'dropout_rate': 0.5367007185265983, 'lr': 0.0004710077740539471, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:35,601] Trial 103 finished with value: -24511874.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 829, 'n_units_l1': 684, 'n_units_l2': 328, 'n_units_l3': 785, 'n_units_l4': 282, 'dropout_rate': 0.5624340937795483, 'lr': 0.0003714490003227193, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:37,592] Trial 104 finished with value: -39253700.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 769, 'n_units_l1': 600, 'n_units_l2': 276, 'n_units_l3': 831, 'n_units_l4': 784, 'dropout_rate': 0.5996746546321047, 'lr': 0.009874515187610495, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:39,449] Trial 105 finished with value: -49907448.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 663, 'n_units_l1': 763, 'n_units_l2': 221, 'n_units_l3': 597, 'n_units_l4': 987, 'dropout_rate': 0.43039715179326976, 'lr': 0.0001764027221821143, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:41,300] Trial 106 finished with value: -37652192.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 997, 'n_units_l1': 842, 'n_units_l2': 371, 'n_units_l3': 861, 'n_units_l4': 572, 'dropout_rate': 0.5521323827832282, 'lr': 0.00284772230217896, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:43,140] Trial 107 finished with value: -17170850.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 699, 'n_units_l1': 630, 'n_units_l2': 405, 'n_units_l3': 728, 'n_units_l4': 990, 'dropout_rate': 0.5818795497244478, 'lr': 0.0004232816031636463, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:44,976] Trial 108 finished with value: -16076202.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 729, 'n_units_l1': 722, 'n_units_l2': 299, 'n_units_l3': 768, 'n_units_l4': 378, 'dropout_rate': 0.4901701880935542, 'lr': 0.00029514848897478147, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:46,864] Trial 109 finished with value: -60089428.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 733, 'n_units_l1': 798, 'n_units_l2': 306, 'n_units_l3': 768, 'n_units_l4': 398, 'dropout_rate': 0.4917393421591856, 'lr': 0.0002997709356294995, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:48,570] Trial 110 finished with value: -36115304.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 619, 'n_units_l1': 718, 'n_units_l2': 329, 'n_units_l3': 791, 'dropout_rate': 0.4747633478992149, 'lr': 0.00024141593605555902, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:50,392] Trial 111 finished with value: -24715176.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 787, 'n_units_l1': 661, 'n_units_l2': 296, 'n_units_l3': 887, 'n_units_l4': 312, 'dropout_rate': 0.5069759369297504, 'lr': 0.0005612773877200739, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:52,405] Trial 112 finished with value: -67813408.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 684, 'n_units_l1': 739, 'n_units_l2': 253, 'n_units_l3': 828, 'n_units_l4': 363, 'dropout_rate': 0.528988476943613, 'lr': 0.0004978961333761904, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:54,470] Trial 113 finished with value: -5511128.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 752, 'n_units_l1': 718, 'n_units_l2': 273, 'n_units_l3': 931, 'n_units_l4': 429, 'dropout_rate': 0.562015105270178, 'lr': 0.0003400897097131579, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:56,329] Trial 114 finished with value: -48683500.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 751, 'n_units_l1': 778, 'n_units_l2': 233, 'n_units_l3': 133, 'n_units_l4': 472, 'dropout_rate': 0.4598073490844712, 'lr': 0.0003314699942991305, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:58,097] Trial 115 finished with value: -27480350.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 720, 'n_units_l1': 753, 'n_units_l2': 356, 'n_units_l3': 650, 'n_units_l4': 406, 'dropout_rate': 0.5177330201680983, 'lr': 0.0001998265047303653, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:33:59,309] Trial 116 finished with value: -2287228160.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 857, 'n_units_l1': 726, 'n_units_l2': 341, 'n_units_l3': 982, 'n_units_l4': 501, 'dropout_rate': 0.5327753075801079, 'lr': 0.00026693259292172577, 'optimizer': 'SGD'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:00,388] Trial 117 finished with value: -122555976.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 636, 'n_units_l1': 499, 'n_units_l2': 987, 'n_units_l3': 940, 'n_units_l4': 589, 'dropout_rate': 0.20826587805076824, 'lr': 0.000350508888226807, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:01,437] Trial 118 finished with value: -38584416.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 663, 'n_units_l1': 388, 'n_units_l2': 713, 'n_units_l3': 904, 'n_units_l4': 356, 'dropout_rate': 0.5405453147714782, 'lr': 0.00023062636280585117, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:02,529] Trial 119 finished with value: -19100316.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 817, 'n_units_l1': 825, 'n_units_l2': 313, 'n_units_l3': 814, 'n_units_l4': 919, 'dropout_rate': 0.40959686843919735, 'lr': 0.0009965510269536665, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:03,498] Trial 120 finished with value: -47750140.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 795, 'n_units_l1': 412, 'n_units_l2': 186, 'n_units_l3': 1008, 'n_units_l4': 425, 'dropout_rate': 0.4801903802733734, 'lr': 0.005002611031650335, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:04,488] Trial 121 finished with value: -57754544.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 777, 'n_units_l1': 692, 'n_units_l2': 277, 'n_units_l3': 877, 'n_units_l4': 342, 'dropout_rate': 0.5645820905490373, 'lr': 0.0004029883494044139, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:05,452] Trial 122 finished with value: -80668608.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 742, 'n_units_l1': 713, 'n_units_l2': 249, 'n_units_l3': 925, 'n_units_l4': 374, 'dropout_rate': 0.5602683848906131, 'lr': 0.0003060861343818015, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:06,412] Trial 123 finished with value: -37420188.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 836, 'n_units_l1': 658, 'n_units_l2': 289, 'n_units_l3': 852, 'n_units_l4': 254, 'dropout_rate': 0.5733966306937107, 'lr': 0.0006199419124503466, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:07,412] Trial 124 finished with value: -75486176.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 813, 'n_units_l1': 682, 'n_units_l2': 218, 'n_units_l3': 771, 'n_units_l4': 749, 'dropout_rate': 0.442300732754408, 'lr': 0.0002752262989883979, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:08,488] Trial 125 finished with value: -24575636.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 724, 'n_units_l1': 955, 'n_units_l2': 395, 'n_units_l3': 961, 'n_units_l4': 292, 'dropout_rate': 0.5507646907620676, 'lr': 0.0004495888625190619, 'optimizer': 'Adam'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:09,132] Trial 126 finished with value: -133272848.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 771, 'n_units_l1': 545, 'dropout_rate': 0.3557478722139382, 'lr': 0.0014402719087415737, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:10,118] Trial 127 finished with value: -7312158.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 899, 'n_units_l1': 323, 'n_units_l2': 263, 'n_units_l3': 404, 'n_units_l4': 869, 'dropout_rate': 0.5901543636755165, 'lr': 0.0001791963547377891, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:11,117] Trial 128 finished with value: -6201556.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 905, 'n_units_l1': 468, 'n_units_l2': 430, 'n_units_l3': 363, 'n_units_l4': 855, 'dropout_rate': 0.579854186766998, 'lr': 0.00018456405107832345, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:12,126] Trial 129 finished with value: -17817828.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 893, 'n_units_l1': 462, 'n_units_l2': 477, 'n_units_l3': 371, 'n_units_l4': 820, 'dropout_rate': 0.5952299485503931, 'lr': 0.0001651312149363565, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:13,174] Trial 130 finished with value: -11785349.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 921, 'n_units_l1': 875, 'n_units_l2': 375, 'n_units_l3': 425, 'n_units_l4': 874, 'dropout_rate': 0.5904148632051905, 'lr': 0.0001390334954133357, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:14,314] Trial 131 finished with value: -64369712.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 923, 'n_units_l1': 862, 'n_units_l2': 426, 'n_units_l3': 400, 'n_units_l4': 879, 'dropout_rate': 0.585749182762134, 'lr': 0.00010091806965100424, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:15,365] Trial 132 finished with value: -32719438.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 988, 'n_units_l1': 877, 'n_units_l2': 430, 'n_units_l3': 435, 'n_units_l4': 866, 'dropout_rate': 0.5888717224445321, 'lr': 0.00013156848043786005, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:16,342] Trial 133 finished with value: -6761371.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 940, 'n_units_l1': 337, 'n_units_l2': 368, 'n_units_l3': 295, 'n_units_l4': 928, 'dropout_rate': 0.5930562882124022, 'lr': 0.00018573522149052722, 'optimizer': 'RMSprop'}. Best is trial 71 with value: -2361916.75.\n",
      "[I 2024-09-25 14:34:17,304] Trial 134 finished with value: -902032.4375 and parameters: {'hidden_layers': 5, 'n_units_l0': 908, 'n_units_l1': 329, 'n_units_l2': 392, 'n_units_l3': 293, 'n_units_l4': 794, 'dropout_rate': 0.592676185255917, 'lr': 0.00018709965815330972, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:18,287] Trial 135 finished with value: -1830438.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 947, 'n_units_l1': 303, 'n_units_l2': 375, 'n_units_l3': 297, 'n_units_l4': 922, 'dropout_rate': 0.5927198431586093, 'lr': 0.00018769265161110678, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:19,251] Trial 136 finished with value: -3267518.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 955, 'n_units_l1': 306, 'n_units_l2': 374, 'n_units_l3': 289, 'n_units_l4': 921, 'dropout_rate': 0.5928378945245208, 'lr': 0.00018249256492074647, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:20,220] Trial 137 finished with value: -3007256.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 956, 'n_units_l1': 270, 'n_units_l2': 372, 'n_units_l3': 282, 'n_units_l4': 927, 'dropout_rate': 0.5921355620418245, 'lr': 0.00018096445189251043, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:21,191] Trial 138 finished with value: -17840222.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 960, 'n_units_l1': 292, 'n_units_l2': 374, 'n_units_l3': 281, 'n_units_l4': 915, 'dropout_rate': 0.589014433608557, 'lr': 0.0001455987417536399, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:22,144] Trial 139 finished with value: -19256962.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 944, 'n_units_l1': 328, 'n_units_l2': 456, 'n_units_l3': 281, 'n_units_l4': 848, 'dropout_rate': 0.5960672936768446, 'lr': 0.0001849319585250912, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:23,115] Trial 140 finished with value: -23968760.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 908, 'n_units_l1': 265, 'n_units_l2': 398, 'n_units_l3': 311, 'n_units_l4': 934, 'dropout_rate': 0.5790924228877037, 'lr': 0.00011191269946758519, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:24,079] Trial 141 finished with value: -2315303.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 986, 'n_units_l1': 300, 'n_units_l2': 366, 'n_units_l3': 232, 'n_units_l4': 888, 'dropout_rate': 0.5907179365495356, 'lr': 0.00019814420869393882, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:25,052] Trial 142 finished with value: -3756271.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 990, 'n_units_l1': 246, 'n_units_l2': 365, 'n_units_l3': 237, 'n_units_l4': 889, 'dropout_rate': 0.5927066764568916, 'lr': 0.00019754954285484372, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:26,031] Trial 143 finished with value: -3795443.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 946, 'n_units_l1': 235, 'n_units_l2': 365, 'n_units_l3': 224, 'n_units_l4': 902, 'dropout_rate': 0.5789215670601502, 'lr': 0.00020018776436426502, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:27,010] Trial 144 finished with value: -4157962.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 985, 'n_units_l1': 221, 'n_units_l2': 439, 'n_units_l3': 235, 'n_units_l4': 896, 'dropout_rate': 0.5999217257798165, 'lr': 0.00020471006790213846, 'optimizer': 'RMSprop'}. Best is trial 134 with value: -902032.4375.\n",
      "[I 2024-09-25 14:34:27,958] Trial 145 finished with value: -524372.0625 and parameters: {'hidden_layers': 5, 'n_units_l0': 984, 'n_units_l1': 224, 'n_units_l2': 440, 'n_units_l3': 220, 'n_units_l4': 894, 'dropout_rate': 0.5988958116197731, 'lr': 0.000187203816386306, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:28,905] Trial 146 finished with value: -8221458.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 987, 'n_units_l1': 222, 'n_units_l2': 499, 'n_units_l3': 222, 'n_units_l4': 898, 'dropout_rate': 0.598536998226109, 'lr': 0.00019949286201736022, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:29,852] Trial 147 finished with value: -2969240.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 998, 'n_units_l1': 150, 'n_units_l2': 438, 'n_units_l3': 237, 'n_units_l4': 924, 'dropout_rate': 0.5782869124003662, 'lr': 0.00021479067554631943, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:30,810] Trial 148 finished with value: -11329921.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1009, 'n_units_l1': 169, 'n_units_l2': 524, 'n_units_l3': 232, 'n_units_l4': 966, 'dropout_rate': 0.580582769185705, 'lr': 0.00015813503851775529, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:31,767] Trial 149 finished with value: -5582776.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 980, 'n_units_l1': 242, 'n_units_l2': 426, 'n_units_l3': 239, 'n_units_l4': 841, 'dropout_rate': 0.5725287338538664, 'lr': 0.0002041512531013737, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:32,749] Trial 150 finished with value: -3901878.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 975, 'n_units_l1': 242, 'n_units_l2': 445, 'n_units_l3': 244, 'n_units_l4': 835, 'dropout_rate': 0.5727109118797648, 'lr': 0.000210849256700607, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:33,739] Trial 151 finished with value: -3045162.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 976, 'n_units_l1': 244, 'n_units_l2': 469, 'n_units_l3': 248, 'n_units_l4': 795, 'dropout_rate': 0.5815455598061849, 'lr': 0.00021692871875283646, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:34,721] Trial 152 finished with value: -2771270.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 978, 'n_units_l1': 243, 'n_units_l2': 471, 'n_units_l3': 243, 'n_units_l4': 782, 'dropout_rate': 0.5733976640845015, 'lr': 0.00022660661250270626, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:35,697] Trial 153 finished with value: -2372755.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1006, 'n_units_l1': 186, 'n_units_l2': 453, 'n_units_l3': 190, 'n_units_l4': 795, 'dropout_rate': 0.5837369205518261, 'lr': 0.00022730679821168418, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:36,705] Trial 154 finished with value: -6668419.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1010, 'n_units_l1': 183, 'n_units_l2': 468, 'n_units_l3': 199, 'n_units_l4': 809, 'dropout_rate': 0.5831623298188118, 'lr': 0.00021561120790512304, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:37,690] Trial 155 finished with value: -1824042.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 956, 'n_units_l1': 206, 'n_units_l2': 449, 'n_units_l3': 251, 'n_units_l4': 789, 'dropout_rate': 0.598770484944813, 'lr': 0.0001602154558800133, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:38,660] Trial 156 finished with value: -3144136.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 955, 'n_units_l1': 205, 'n_units_l2': 444, 'n_units_l3': 189, 'n_units_l4': 792, 'dropout_rate': 0.5960964388536922, 'lr': 0.00015746095777523008, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:39,611] Trial 157 finished with value: -6489267.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 956, 'n_units_l1': 140, 'n_units_l2': 486, 'n_units_l3': 190, 'n_units_l4': 776, 'dropout_rate': 0.5727132822738011, 'lr': 0.00011730010231925823, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:40,583] Trial 158 finished with value: -31600438.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1024, 'n_units_l1': 204, 'n_units_l2': 515, 'n_units_l3': 257, 'n_units_l4': 787, 'dropout_rate': 0.5848816149787259, 'lr': 0.00014087188798306443, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:41,550] Trial 159 finished with value: -3395153.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 968, 'n_units_l1': 247, 'n_units_l2': 447, 'n_units_l3': 171, 'n_units_l4': 795, 'dropout_rate': 0.5919372908912874, 'lr': 0.00016053432475170105, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:42,508] Trial 160 finished with value: -6433904.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 950, 'n_units_l1': 165, 'n_units_l2': 555, 'n_units_l3': 166, 'n_units_l4': 713, 'dropout_rate': 0.5907585550955144, 'lr': 0.00015841209246902164, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:43,530] Trial 161 finished with value: -2761497.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 970, 'n_units_l1': 255, 'n_units_l2': 454, 'n_units_l3': 264, 'n_units_l4': 817, 'dropout_rate': 0.5793542685712961, 'lr': 0.00023655721005153124, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:44,500] Trial 162 finished with value: -5968636.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 1000, 'n_units_l1': 262, 'n_units_l2': 412, 'n_units_l3': 205, 'n_units_l4': 802, 'dropout_rate': 0.581926263034978, 'lr': 0.00024052710421959103, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:45,461] Trial 163 finished with value: -15431970.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 964, 'n_units_l1': 287, 'n_units_l2': 474, 'n_units_l3': 256, 'n_units_l4': 753, 'dropout_rate': 0.5997196484560239, 'lr': 0.00016315981018498089, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:46,423] Trial 164 finished with value: -2753759.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 970, 'n_units_l1': 191, 'n_units_l2': 455, 'n_units_l3': 166, 'n_units_l4': 796, 'dropout_rate': 0.5904636249949212, 'lr': 0.00012895201702684437, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:47,388] Trial 165 finished with value: -6396048.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 973, 'n_units_l1': 194, 'n_units_l2': 462, 'n_units_l3': 159, 'n_units_l4': 793, 'dropout_rate': 0.5924663237230234, 'lr': 8.84620708656704e-05, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:48,353] Trial 166 finished with value: -7111470.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1003, 'n_units_l1': 274, 'n_units_l2': 497, 'n_units_l3': 183, 'n_units_l4': 769, 'dropout_rate': 0.5867666912108254, 'lr': 0.00012524166522998457, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:49,199] Trial 167 finished with value: -2677834240.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 975, 'n_units_l1': 206, 'n_units_l2': 445, 'n_units_l3': 319, 'n_units_l4': 816, 'dropout_rate': 0.5920394346971314, 'lr': 0.0002436257685830652, 'optimizer': 'SGD'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:50,165] Trial 168 finished with value: -6656479.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 932, 'n_units_l1': 302, 'n_units_l2': 395, 'n_units_l3': 341, 'n_units_l4': 748, 'dropout_rate': 0.5709540523723855, 'lr': 0.00015504518217103984, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:51,114] Trial 169 finished with value: -27523888.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 994, 'n_units_l1': 161, 'n_units_l2': 417, 'n_units_l3': 261, 'n_units_l4': 946, 'dropout_rate': 0.5981887270537695, 'lr': 0.00013121729956009932, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:52,055] Trial 170 finished with value: -9040101.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 962, 'n_units_l1': 252, 'n_units_l2': 451, 'n_units_l3': 145, 'n_units_l4': 835, 'dropout_rate': 0.5844410357130524, 'lr': 0.00010942474155533838, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:53,021] Trial 171 finished with value: -2793032.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 947, 'n_units_l1': 229, 'n_units_l2': 389, 'n_units_l3': 221, 'n_units_l4': 912, 'dropout_rate': 0.5782020296632053, 'lr': 0.00017375823967516826, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:53,978] Trial 172 finished with value: -3466949.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 1018, 'n_units_l1': 221, 'n_units_l2': 408, 'n_units_l3': 206, 'n_units_l4': 969, 'dropout_rate': 0.5999020209984885, 'lr': 0.00017687731453103474, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:54,954] Trial 173 finished with value: -3643775.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 1021, 'n_units_l1': 211, 'n_units_l2': 481, 'n_units_l3': 209, 'n_units_l4': 967, 'dropout_rate': 0.5771448244977163, 'lr': 0.0001666986421564707, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:55,923] Trial 174 finished with value: -2738964.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 942, 'n_units_l1': 185, 'n_units_l2': 408, 'n_units_l3': 176, 'n_units_l4': 996, 'dropout_rate': 0.599962110126683, 'lr': 0.00025781920938298383, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:56,876] Trial 175 finished with value: -4876103.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 923, 'n_units_l1': 182, 'n_units_l2': 437, 'n_units_l3': 182, 'n_units_l4': 798, 'dropout_rate': 0.5824344845583875, 'lr': 0.0002367876307861162, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:57,872] Trial 176 finished with value: -7850424.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 945, 'n_units_l1': 151, 'n_units_l2': 386, 'n_units_l3': 265, 'n_units_l4': 771, 'dropout_rate': 0.5689704257039232, 'lr': 0.00014344786448673255, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:58,820] Trial 177 finished with value: -5993840.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 965, 'n_units_l1': 129, 'n_units_l2': 461, 'n_units_l3': 159, 'n_units_l4': 1005, 'dropout_rate': 0.587635733928574, 'lr': 0.0002683078906533471, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:34:59,765] Trial 178 finished with value: -6136672.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 946, 'n_units_l1': 190, 'n_units_l2': 416, 'n_units_l3': 296, 'n_units_l4': 930, 'dropout_rate': 0.5775260261105053, 'lr': 0.0002505792983420826, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:00,738] Trial 179 finished with value: -1898782.625 and parameters: {'hidden_layers': 5, 'n_units_l0': 927, 'n_units_l1': 281, 'n_units_l2': 395, 'n_units_l3': 177, 'n_units_l4': 731, 'dropout_rate': 0.5997762176071831, 'lr': 0.00022569391882893856, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:01,715] Trial 180 finished with value: -4604627.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 919, 'n_units_l1': 304, 'n_units_l2': 394, 'n_units_l3': 299, 'n_units_l4': 730, 'dropout_rate': 0.5991717138939588, 'lr': 0.00023372825195247667, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:02,705] Trial 181 finished with value: -3896790.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 872, 'n_units_l1': 271, 'n_units_l2': 509, 'n_units_l3': 142, 'n_units_l4': 810, 'dropout_rate': 0.589552553062887, 'lr': 0.00017401688578800318, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:03,692] Trial 182 finished with value: -3058180.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 973, 'n_units_l1': 235, 'n_units_l2': 436, 'n_units_l3': 174, 'n_units_l4': 824, 'dropout_rate': 0.5835963188166166, 'lr': 0.00015032058218335812, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:04,669] Trial 183 finished with value: -15286027.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 935, 'n_units_l1': 228, 'n_units_l2': 433, 'n_units_l3': 339, 'n_units_l4': 852, 'dropout_rate': 0.5832614666980988, 'lr': 0.00013045273014458358, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:05,630] Trial 184 finished with value: -4623592.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 999, 'n_units_l1': 284, 'n_units_l2': 383, 'n_units_l3': 216, 'n_units_l4': 820, 'dropout_rate': 0.5760756112437587, 'lr': 0.0002223954000601289, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:06,563] Trial 185 finished with value: -92187816.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 885, 'n_units_l1': 174, 'n_units_l2': 408, 'n_units_l3': 275, 'n_units_l4': 907, 'dropout_rate': 0.5659933692744312, 'lr': 9.773792757053013e-05, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:07,521] Trial 186 finished with value: -4762767.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 954, 'n_units_l1': 205, 'n_units_l2': 536, 'n_units_l3': 188, 'n_units_l4': 880, 'dropout_rate': 0.5923473357515205, 'lr': 0.00026433004345110877, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:08,465] Trial 187 finished with value: -5456549.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 989, 'n_units_l1': 258, 'n_units_l2': 470, 'n_units_l3': 215, 'n_units_l4': 758, 'dropout_rate': 0.5840672048148772, 'lr': 0.00018100208474186135, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:09,427] Trial 188 finished with value: -2468012.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 980, 'n_units_l1': 197, 'n_units_l2': 421, 'n_units_l3': 248, 'n_units_l4': 925, 'dropout_rate': 0.5997532048635258, 'lr': 0.0001450030046204427, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:10,387] Trial 189 finished with value: -2916389.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 978, 'n_units_l1': 191, 'n_units_l2': 425, 'n_units_l3': 250, 'n_units_l4': 941, 'dropout_rate': 0.5710326156071495, 'lr': 0.0001514306975718041, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:11,350] Trial 190 finished with value: -86519872.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 977, 'n_units_l1': 191, 'n_units_l2': 428, 'n_units_l3': 248, 'n_units_l4': 940, 'dropout_rate': 0.5730513916514991, 'lr': 6.829811403153372e-05, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:12,328] Trial 191 finished with value: -3095905.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 1003, 'n_units_l1': 230, 'n_units_l2': 489, 'n_units_l3': 270, 'n_units_l4': 983, 'dropout_rate': 0.5847769350023864, 'lr': 0.00014705445420587008, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:13,301] Trial 192 finished with value: -1168543.875 and parameters: {'hidden_layers': 5, 'n_units_l0': 1005, 'n_units_l1': 226, 'n_units_l2': 492, 'n_units_l3': 268, 'n_units_l4': 951, 'dropout_rate': 0.5795165816815933, 'lr': 0.000140838631085178, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:14,251] Trial 193 finished with value: -10687074.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 972, 'n_units_l1': 152, 'n_units_l2': 466, 'n_units_l3': 251, 'n_units_l4': 954, 'dropout_rate': 0.5675624381001642, 'lr': 0.00011217220802332792, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:15,207] Trial 194 finished with value: -3707702.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 982, 'n_units_l1': 220, 'n_units_l2': 419, 'n_units_l3': 221, 'n_units_l4': 1000, 'dropout_rate': 0.5774593824614194, 'lr': 0.0001246438895363684, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:16,163] Trial 195 finished with value: -4686711.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1006, 'n_units_l1': 177, 'n_units_l2': 583, 'n_units_l3': 234, 'n_units_l4': 913, 'dropout_rate': 0.5562801724041279, 'lr': 0.00021341608383283539, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:17,140] Trial 196 finished with value: -3446357.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 917, 'n_units_l1': 271, 'n_units_l2': 395, 'n_units_l3': 268, 'n_units_l4': 933, 'dropout_rate': 0.5864638433313057, 'lr': 0.00014274581625164836, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:17,948] Trial 197 finished with value: -2447974400.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 935, 'n_units_l1': 232, 'n_units_l2': 484, 'n_units_l3': 170, 'n_units_l4': 871, 'dropout_rate': 0.5993430265418229, 'lr': 0.00028930420058239254, 'optimizer': 'SGD'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:18,941] Trial 198 finished with value: -2287369.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 987, 'n_units_l1': 198, 'n_units_l2': 450, 'n_units_l3': 305, 'n_units_l4': 950, 'dropout_rate': 0.5779928485462371, 'lr': 0.00018152782244486643, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n",
      "[I 2024-09-25 14:35:19,917] Trial 199 finished with value: -5018448.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 991, 'n_units_l1': 186, 'n_units_l2': 454, 'n_units_l3': 310, 'n_units_l4': 952, 'dropout_rate': 0.5697965863166154, 'lr': 0.00019248537002592553, 'optimizer': 'RMSprop'}. Best is trial 145 with value: -524372.0625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization completed.\n",
      "Best Trial:\n",
      "  Value (MSE): -524372.0625\n",
      "  Params:\n",
      "    hidden_layers: 5\n",
      "    n_units_l0: 984\n",
      "    n_units_l1: 224\n",
      "    n_units_l2: 440\n",
      "    n_units_l3: 220\n",
      "    n_units_l4: 894\n",
      "    dropout_rate: 0.5988958116197731\n",
      "    lr: 0.000187203816386306\n",
      "    optimizer: RMSprop\n",
      "Starting training of the best model...\n",
      "Epoch 1/500 - Training Loss: 0.2121 - Validation Loss: 0.1484\n",
      "Epoch 2/500 - Training Loss: 0.0755 - Validation Loss: 0.1095\n",
      "Epoch 3/500 - Training Loss: 0.0655 - Validation Loss: 0.0908\n",
      "Epoch 4/500 - Training Loss: 0.0507 - Validation Loss: 0.0965\n",
      "Epoch 5/500 - Training Loss: 0.0467 - Validation Loss: 0.0903\n",
      "Epoch 6/500 - Training Loss: 0.0442 - Validation Loss: 0.1003\n",
      "Epoch 7/500 - Training Loss: 0.0401 - Validation Loss: 0.0977\n",
      "Epoch 8/500 - Training Loss: 0.0371 - Validation Loss: 0.0872\n",
      "Epoch 9/500 - Training Loss: 0.0350 - Validation Loss: 0.0733\n",
      "Epoch 10/500 - Training Loss: 0.0370 - Validation Loss: 0.0811\n",
      "Epoch 11/500 - Training Loss: 0.0345 - Validation Loss: 0.0767\n",
      "Epoch 12/500 - Training Loss: 0.0356 - Validation Loss: 0.0796\n",
      "Epoch 13/500 - Training Loss: 0.0319 - Validation Loss: 0.0724\n",
      "Epoch 14/500 - Training Loss: 0.0325 - Validation Loss: 0.0721\n",
      "Epoch 15/500 - Training Loss: 0.0309 - Validation Loss: 0.0623\n",
      "Epoch 16/500 - Training Loss: 0.0313 - Validation Loss: 0.0703\n",
      "Epoch 17/500 - Training Loss: 0.0312 - Validation Loss: 0.0625\n",
      "Epoch 18/500 - Training Loss: 0.0296 - Validation Loss: 0.0648\n",
      "Epoch 19/500 - Training Loss: 0.0277 - Validation Loss: 0.0624\n",
      "Epoch 20/500 - Training Loss: 0.0276 - Validation Loss: 0.0596\n",
      "Epoch 21/500 - Training Loss: 0.0277 - Validation Loss: 0.0593\n",
      "Epoch 22/500 - Training Loss: 0.0282 - Validation Loss: 0.0611\n",
      "Epoch 23/500 - Training Loss: 0.0285 - Validation Loss: 0.0544\n",
      "Epoch 24/500 - Training Loss: 0.0271 - Validation Loss: 0.0480\n",
      "Epoch 25/500 - Training Loss: 0.0270 - Validation Loss: 0.0520\n",
      "Epoch 26/500 - Training Loss: 0.0256 - Validation Loss: 0.0482\n",
      "Epoch 27/500 - Training Loss: 0.0253 - Validation Loss: 0.0506\n",
      "Epoch 28/500 - Training Loss: 0.0250 - Validation Loss: 0.0448\n",
      "Epoch 29/500 - Training Loss: 0.0252 - Validation Loss: 0.0480\n",
      "Epoch 30/500 - Training Loss: 0.0247 - Validation Loss: 0.0462\n",
      "Epoch 31/500 - Training Loss: 0.0237 - Validation Loss: 0.0420\n",
      "Epoch 32/500 - Training Loss: 0.0248 - Validation Loss: 0.0413\n",
      "Epoch 33/500 - Training Loss: 0.0223 - Validation Loss: 0.0381\n",
      "Epoch 34/500 - Training Loss: 0.0228 - Validation Loss: 0.0382\n",
      "Epoch 35/500 - Training Loss: 0.0237 - Validation Loss: 0.0387\n",
      "Epoch 36/500 - Training Loss: 0.0234 - Validation Loss: 0.0395\n",
      "Epoch 37/500 - Training Loss: 0.0234 - Validation Loss: 0.0376\n",
      "Epoch 38/500 - Training Loss: 0.0229 - Validation Loss: 0.0352\n",
      "Epoch 39/500 - Training Loss: 0.0232 - Validation Loss: 0.0369\n",
      "Epoch 40/500 - Training Loss: 0.0227 - Validation Loss: 0.0322\n",
      "Epoch 41/500 - Training Loss: 0.0214 - Validation Loss: 0.0335\n",
      "Epoch 42/500 - Training Loss: 0.0222 - Validation Loss: 0.0336\n",
      "Epoch 43/500 - Training Loss: 0.0216 - Validation Loss: 0.0325\n",
      "Epoch 44/500 - Training Loss: 0.0227 - Validation Loss: 0.0345\n",
      "Epoch 45/500 - Training Loss: 0.0215 - Validation Loss: 0.0332\n",
      "Epoch 46/500 - Training Loss: 0.0218 - Validation Loss: 0.0302\n",
      "Epoch 47/500 - Training Loss: 0.0210 - Validation Loss: 0.0309\n",
      "Epoch 48/500 - Training Loss: 0.0207 - Validation Loss: 0.0321\n",
      "Epoch 49/500 - Training Loss: 0.0218 - Validation Loss: 0.0310\n",
      "Epoch 50/500 - Training Loss: 0.0206 - Validation Loss: 0.0312\n",
      "Epoch 51/500 - Training Loss: 0.0195 - Validation Loss: 0.0297\n",
      "Epoch 52/500 - Training Loss: 0.0212 - Validation Loss: 0.0305\n",
      "Epoch 53/500 - Training Loss: 0.0209 - Validation Loss: 0.0286\n",
      "Epoch 54/500 - Training Loss: 0.0204 - Validation Loss: 0.0279\n",
      "Epoch 55/500 - Training Loss: 0.0210 - Validation Loss: 0.0277\n",
      "Epoch 56/500 - Training Loss: 0.0208 - Validation Loss: 0.0269\n",
      "Epoch 57/500 - Training Loss: 0.0200 - Validation Loss: 0.0261\n",
      "Epoch 58/500 - Training Loss: 0.0204 - Validation Loss: 0.0293\n",
      "Epoch 59/500 - Training Loss: 0.0209 - Validation Loss: 0.0273\n",
      "Epoch 60/500 - Training Loss: 0.0198 - Validation Loss: 0.0264\n",
      "Epoch 61/500 - Training Loss: 0.0198 - Validation Loss: 0.0261\n",
      "Epoch 62/500 - Training Loss: 0.0207 - Validation Loss: 0.0254\n",
      "Epoch 63/500 - Training Loss: 0.0207 - Validation Loss: 0.0263\n",
      "Epoch 64/500 - Training Loss: 0.0196 - Validation Loss: 0.0258\n",
      "Epoch 65/500 - Training Loss: 0.0195 - Validation Loss: 0.0262\n",
      "Epoch 66/500 - Training Loss: 0.0207 - Validation Loss: 0.0275\n",
      "Epoch 67/500 - Training Loss: 0.0217 - Validation Loss: 0.0269\n",
      "Epoch 68/500 - Training Loss: 0.0203 - Validation Loss: 0.0241\n",
      "Epoch 69/500 - Training Loss: 0.0196 - Validation Loss: 0.0282\n",
      "Epoch 70/500 - Training Loss: 0.0193 - Validation Loss: 0.0248\n",
      "Epoch 71/500 - Training Loss: 0.0195 - Validation Loss: 0.0244\n",
      "Epoch 72/500 - Training Loss: 0.0193 - Validation Loss: 0.0268\n",
      "Epoch 73/500 - Training Loss: 0.0195 - Validation Loss: 0.0258\n",
      "Epoch 74/500 - Training Loss: 0.0195 - Validation Loss: 0.0242\n",
      "Epoch 75/500 - Training Loss: 0.0191 - Validation Loss: 0.0242\n",
      "Epoch 76/500 - Training Loss: 0.0187 - Validation Loss: 0.0262\n",
      "Epoch 77/500 - Training Loss: 0.0195 - Validation Loss: 0.0257\n",
      "Epoch 78/500 - Training Loss: 0.0195 - Validation Loss: 0.0250\n",
      "Epoch 79/500 - Training Loss: 0.0192 - Validation Loss: 0.0260\n",
      "Epoch 80/500 - Training Loss: 0.0195 - Validation Loss: 0.0239\n",
      "Epoch 81/500 - Training Loss: 0.0188 - Validation Loss: 0.0252\n",
      "Epoch 82/500 - Training Loss: 0.0186 - Validation Loss: 0.0240\n",
      "Epoch 83/500 - Training Loss: 0.0191 - Validation Loss: 0.0253\n",
      "Epoch 84/500 - Training Loss: 0.0178 - Validation Loss: 0.0246\n",
      "Epoch 85/500 - Training Loss: 0.0187 - Validation Loss: 0.0251\n",
      "Epoch 86/500 - Training Loss: 0.0179 - Validation Loss: 0.0263\n",
      "Epoch 87/500 - Training Loss: 0.0193 - Validation Loss: 0.0243\n",
      "Epoch 88/500 - Training Loss: 0.0182 - Validation Loss: 0.0239\n",
      "Epoch 89/500 - Training Loss: 0.0187 - Validation Loss: 0.0245\n",
      "Epoch 90/500 - Training Loss: 0.0194 - Validation Loss: 0.0249\n",
      "Epoch 91/500 - Training Loss: 0.0180 - Validation Loss: 0.0258\n",
      "Epoch 92/500 - Training Loss: 0.0179 - Validation Loss: 0.0241\n",
      "Epoch 93/500 - Training Loss: 0.0176 - Validation Loss: 0.0250\n",
      "Epoch 94/500 - Training Loss: 0.0190 - Validation Loss: 0.0240\n",
      "Epoch 95/500 - Training Loss: 0.0188 - Validation Loss: 0.0235\n",
      "Epoch 96/500 - Training Loss: 0.0178 - Validation Loss: 0.0251\n",
      "Epoch 97/500 - Training Loss: 0.0183 - Validation Loss: 0.0248\n",
      "Epoch 98/500 - Training Loss: 0.0183 - Validation Loss: 0.0233\n",
      "Epoch 99/500 - Training Loss: 0.0186 - Validation Loss: 0.0245\n",
      "Epoch 100/500 - Training Loss: 0.0176 - Validation Loss: 0.0247\n",
      "Epoch 101/500 - Training Loss: 0.0188 - Validation Loss: 0.0241\n",
      "Epoch 102/500 - Training Loss: 0.0179 - Validation Loss: 0.0241\n",
      "Epoch 103/500 - Training Loss: 0.0180 - Validation Loss: 0.0247\n",
      "Epoch 104/500 - Training Loss: 0.0186 - Validation Loss: 0.0236\n",
      "Epoch 105/500 - Training Loss: 0.0169 - Validation Loss: 0.0234\n",
      "Epoch 106/500 - Training Loss: 0.0179 - Validation Loss: 0.0236\n",
      "Epoch 107/500 - Training Loss: 0.0173 - Validation Loss: 0.0247\n",
      "Epoch 108/500 - Training Loss: 0.0184 - Validation Loss: 0.0229\n",
      "Epoch 109/500 - Training Loss: 0.0177 - Validation Loss: 0.0239\n",
      "Epoch 110/500 - Training Loss: 0.0178 - Validation Loss: 0.0235\n",
      "Epoch 111/500 - Training Loss: 0.0177 - Validation Loss: 0.0241\n",
      "Epoch 112/500 - Training Loss: 0.0173 - Validation Loss: 0.0224\n",
      "Epoch 113/500 - Training Loss: 0.0176 - Validation Loss: 0.0256\n",
      "Epoch 114/500 - Training Loss: 0.0181 - Validation Loss: 0.0244\n",
      "Epoch 115/500 - Training Loss: 0.0188 - Validation Loss: 0.0222\n",
      "Epoch 116/500 - Training Loss: 0.0177 - Validation Loss: 0.0233\n",
      "Epoch 117/500 - Training Loss: 0.0178 - Validation Loss: 0.0236\n",
      "Epoch 118/500 - Training Loss: 0.0165 - Validation Loss: 0.0240\n",
      "Epoch 119/500 - Training Loss: 0.0171 - Validation Loss: 0.0242\n",
      "Epoch 120/500 - Training Loss: 0.0171 - Validation Loss: 0.0226\n",
      "Epoch 121/500 - Training Loss: 0.0165 - Validation Loss: 0.0226\n",
      "Epoch 122/500 - Training Loss: 0.0169 - Validation Loss: 0.0230\n",
      "Epoch 123/500 - Training Loss: 0.0189 - Validation Loss: 0.0229\n",
      "Epoch 124/500 - Training Loss: 0.0175 - Validation Loss: 0.0221\n",
      "Epoch 125/500 - Training Loss: 0.0160 - Validation Loss: 0.0221\n",
      "Epoch 126/500 - Training Loss: 0.0172 - Validation Loss: 0.0232\n",
      "Epoch 127/500 - Training Loss: 0.0184 - Validation Loss: 0.0237\n",
      "Epoch 128/500 - Training Loss: 0.0168 - Validation Loss: 0.0233\n",
      "Epoch 129/500 - Training Loss: 0.0164 - Validation Loss: 0.0220\n",
      "Epoch 130/500 - Training Loss: 0.0161 - Validation Loss: 0.0211\n",
      "Epoch 131/500 - Training Loss: 0.0163 - Validation Loss: 0.0218\n",
      "Epoch 132/500 - Training Loss: 0.0168 - Validation Loss: 0.0214\n",
      "Epoch 133/500 - Training Loss: 0.0162 - Validation Loss: 0.0213\n",
      "Epoch 134/500 - Training Loss: 0.0165 - Validation Loss: 0.0216\n",
      "Epoch 135/500 - Training Loss: 0.0167 - Validation Loss: 0.0216\n",
      "Epoch 136/500 - Training Loss: 0.0155 - Validation Loss: 0.0201\n",
      "Epoch 137/500 - Training Loss: 0.0154 - Validation Loss: 0.0205\n",
      "Epoch 138/500 - Training Loss: 0.0161 - Validation Loss: 0.0218\n",
      "Epoch 139/500 - Training Loss: 0.0149 - Validation Loss: 0.0204\n",
      "Epoch 140/500 - Training Loss: 0.0165 - Validation Loss: 0.0198\n",
      "Epoch 141/500 - Training Loss: 0.0154 - Validation Loss: 0.0203\n",
      "Epoch 142/500 - Training Loss: 0.0169 - Validation Loss: 0.0202\n",
      "Epoch 143/500 - Training Loss: 0.0150 - Validation Loss: 0.0188\n",
      "Epoch 144/500 - Training Loss: 0.0150 - Validation Loss: 0.0190\n",
      "Epoch 145/500 - Training Loss: 0.0144 - Validation Loss: 0.0199\n",
      "Epoch 146/500 - Training Loss: 0.0153 - Validation Loss: 0.0203\n",
      "Epoch 147/500 - Training Loss: 0.0151 - Validation Loss: 0.0199\n",
      "Epoch 148/500 - Training Loss: 0.0157 - Validation Loss: 0.0189\n",
      "Epoch 149/500 - Training Loss: 0.0148 - Validation Loss: 0.0188\n",
      "Epoch 150/500 - Training Loss: 0.0157 - Validation Loss: 0.0206\n",
      "Epoch 151/500 - Training Loss: 0.0140 - Validation Loss: 0.0206\n",
      "Epoch 152/500 - Training Loss: 0.0151 - Validation Loss: 0.0191\n",
      "Epoch 153/500 - Training Loss: 0.0138 - Validation Loss: 0.0189\n",
      "Epoch 154/500 - Training Loss: 0.0150 - Validation Loss: 0.0203\n",
      "Epoch 155/500 - Training Loss: 0.0139 - Validation Loss: 0.0182\n",
      "Epoch 156/500 - Training Loss: 0.0148 - Validation Loss: 0.0181\n",
      "Epoch 157/500 - Training Loss: 0.0143 - Validation Loss: 0.0197\n",
      "Epoch 158/500 - Training Loss: 0.0153 - Validation Loss: 0.0201\n",
      "Epoch 159/500 - Training Loss: 0.0141 - Validation Loss: 0.0204\n",
      "Epoch 160/500 - Training Loss: 0.0133 - Validation Loss: 0.0191\n",
      "Epoch 161/500 - Training Loss: 0.0134 - Validation Loss: 0.0190\n",
      "Epoch 162/500 - Training Loss: 0.0138 - Validation Loss: 0.0189\n",
      "Epoch 163/500 - Training Loss: 0.0130 - Validation Loss: 0.0197\n",
      "Epoch 164/500 - Training Loss: 0.0137 - Validation Loss: 0.0194\n",
      "Epoch 165/500 - Training Loss: 0.0128 - Validation Loss: 0.0204\n",
      "Epoch 166/500 - Training Loss: 0.0149 - Validation Loss: 0.0194\n",
      "Epoch 167/500 - Training Loss: 0.0143 - Validation Loss: 0.0202\n",
      "Epoch 168/500 - Training Loss: 0.0139 - Validation Loss: 0.0171\n",
      "Epoch 169/500 - Training Loss: 0.0147 - Validation Loss: 0.0191\n",
      "Epoch 170/500 - Training Loss: 0.0160 - Validation Loss: 0.0197\n",
      "Epoch 171/500 - Training Loss: 0.0141 - Validation Loss: 0.0186\n",
      "Epoch 172/500 - Training Loss: 0.0147 - Validation Loss: 0.0184\n",
      "Epoch 173/500 - Training Loss: 0.0126 - Validation Loss: 0.0195\n",
      "Epoch 174/500 - Training Loss: 0.0143 - Validation Loss: 0.0179\n",
      "Epoch 175/500 - Training Loss: 0.0135 - Validation Loss: 0.0189\n",
      "Epoch 176/500 - Training Loss: 0.0134 - Validation Loss: 0.0185\n",
      "Epoch 177/500 - Training Loss: 0.0138 - Validation Loss: 0.0179\n",
      "Epoch 178/500 - Training Loss: 0.0139 - Validation Loss: 0.0187\n",
      "Epoch 179/500 - Training Loss: 0.0132 - Validation Loss: 0.0174\n",
      "Epoch 180/500 - Training Loss: 0.0136 - Validation Loss: 0.0168\n",
      "Epoch 181/500 - Training Loss: 0.0121 - Validation Loss: 0.0186\n",
      "Epoch 182/500 - Training Loss: 0.0135 - Validation Loss: 0.0181\n",
      "Epoch 183/500 - Training Loss: 0.0142 - Validation Loss: 0.0182\n",
      "Epoch 184/500 - Training Loss: 0.0128 - Validation Loss: 0.0183\n",
      "Epoch 185/500 - Training Loss: 0.0127 - Validation Loss: 0.0164\n",
      "Epoch 186/500 - Training Loss: 0.0136 - Validation Loss: 0.0187\n",
      "Epoch 187/500 - Training Loss: 0.0133 - Validation Loss: 0.0197\n",
      "Epoch 188/500 - Training Loss: 0.0137 - Validation Loss: 0.0175\n",
      "Epoch 189/500 - Training Loss: 0.0124 - Validation Loss: 0.0174\n",
      "Epoch 190/500 - Training Loss: 0.0133 - Validation Loss: 0.0170\n",
      "Epoch 191/500 - Training Loss: 0.0127 - Validation Loss: 0.0178\n",
      "Epoch 192/500 - Training Loss: 0.0125 - Validation Loss: 0.0183\n",
      "Epoch 193/500 - Training Loss: 0.0130 - Validation Loss: 0.0186\n",
      "Epoch 194/500 - Training Loss: 0.0132 - Validation Loss: 0.0174\n",
      "Epoch 195/500 - Training Loss: 0.0125 - Validation Loss: 0.0187\n",
      "Epoch 196/500 - Training Loss: 0.0121 - Validation Loss: 0.0173\n",
      "Epoch 197/500 - Training Loss: 0.0123 - Validation Loss: 0.0168\n",
      "Epoch 198/500 - Training Loss: 0.0136 - Validation Loss: 0.0179\n",
      "Epoch 199/500 - Training Loss: 0.0136 - Validation Loss: 0.0191\n",
      "Epoch 200/500 - Training Loss: 0.0121 - Validation Loss: 0.0183\n",
      "Epoch 201/500 - Training Loss: 0.0134 - Validation Loss: 0.0177\n",
      "Epoch 202/500 - Training Loss: 0.0114 - Validation Loss: 0.0182\n",
      "Epoch 203/500 - Training Loss: 0.0134 - Validation Loss: 0.0174\n",
      "Epoch 204/500 - Training Loss: 0.0125 - Validation Loss: 0.0174\n",
      "Epoch 205/500 - Training Loss: 0.0118 - Validation Loss: 0.0177\n",
      "Epoch 206/500 - Training Loss: 0.0125 - Validation Loss: 0.0178\n",
      "Epoch 207/500 - Training Loss: 0.0125 - Validation Loss: 0.0174\n",
      "Epoch 208/500 - Training Loss: 0.0130 - Validation Loss: 0.0180\n",
      "Epoch 209/500 - Training Loss: 0.0128 - Validation Loss: 0.0189\n",
      "Epoch 210/500 - Training Loss: 0.0127 - Validation Loss: 0.0184\n",
      "Epoch 211/500 - Training Loss: 0.0122 - Validation Loss: 0.0192\n",
      "Epoch 212/500 - Training Loss: 0.0117 - Validation Loss: 0.0166\n",
      "Epoch 213/500 - Training Loss: 0.0120 - Validation Loss: 0.0176\n",
      "Epoch 214/500 - Training Loss: 0.0134 - Validation Loss: 0.0178\n",
      "Epoch 215/500 - Training Loss: 0.0121 - Validation Loss: 0.0189\n",
      "Epoch 216/500 - Training Loss: 0.0122 - Validation Loss: 0.0185\n",
      "Epoch 217/500 - Training Loss: 0.0130 - Validation Loss: 0.0171\n",
      "Epoch 218/500 - Training Loss: 0.0123 - Validation Loss: 0.0168\n",
      "Epoch 219/500 - Training Loss: 0.0122 - Validation Loss: 0.0165\n",
      "Epoch 220/500 - Training Loss: 0.0112 - Validation Loss: 0.0169\n",
      "Epoch 221/500 - Training Loss: 0.0110 - Validation Loss: 0.0175\n",
      "Epoch 222/500 - Training Loss: 0.0111 - Validation Loss: 0.0161\n",
      "Epoch 223/500 - Training Loss: 0.0129 - Validation Loss: 0.0172\n",
      "Epoch 224/500 - Training Loss: 0.0117 - Validation Loss: 0.0172\n",
      "Epoch 225/500 - Training Loss: 0.0119 - Validation Loss: 0.0158\n",
      "Epoch 226/500 - Training Loss: 0.0124 - Validation Loss: 0.0179\n",
      "Epoch 227/500 - Training Loss: 0.0116 - Validation Loss: 0.0177\n",
      "Epoch 228/500 - Training Loss: 0.0112 - Validation Loss: 0.0162\n",
      "Epoch 229/500 - Training Loss: 0.0108 - Validation Loss: 0.0152\n",
      "Epoch 230/500 - Training Loss: 0.0111 - Validation Loss: 0.0180\n",
      "Epoch 231/500 - Training Loss: 0.0120 - Validation Loss: 0.0169\n",
      "Epoch 232/500 - Training Loss: 0.0114 - Validation Loss: 0.0189\n",
      "Epoch 233/500 - Training Loss: 0.0110 - Validation Loss: 0.0175\n",
      "Epoch 234/500 - Training Loss: 0.0120 - Validation Loss: 0.0169\n",
      "Epoch 235/500 - Training Loss: 0.0126 - Validation Loss: 0.0171\n",
      "Epoch 236/500 - Training Loss: 0.0112 - Validation Loss: 0.0163\n",
      "Epoch 237/500 - Training Loss: 0.0123 - Validation Loss: 0.0176\n",
      "Epoch 238/500 - Training Loss: 0.0109 - Validation Loss: 0.0165\n",
      "Epoch 239/500 - Training Loss: 0.0114 - Validation Loss: 0.0165\n",
      "Epoch 240/500 - Training Loss: 0.0114 - Validation Loss: 0.0161\n",
      "Epoch 241/500 - Training Loss: 0.0114 - Validation Loss: 0.0172\n",
      "Epoch 242/500 - Training Loss: 0.0127 - Validation Loss: 0.0170\n",
      "Epoch 243/500 - Training Loss: 0.0117 - Validation Loss: 0.0175\n",
      "Epoch 244/500 - Training Loss: 0.0122 - Validation Loss: 0.0171\n",
      "Epoch 245/500 - Training Loss: 0.0115 - Validation Loss: 0.0164\n",
      "Epoch 246/500 - Training Loss: 0.0101 - Validation Loss: 0.0161\n",
      "Epoch 247/500 - Training Loss: 0.0116 - Validation Loss: 0.0163\n",
      "Epoch 248/500 - Training Loss: 0.0121 - Validation Loss: 0.0158\n",
      "Epoch 249/500 - Training Loss: 0.0112 - Validation Loss: 0.0165\n",
      "Epoch 250/500 - Training Loss: 0.0114 - Validation Loss: 0.0178\n",
      "Epoch 251/500 - Training Loss: 0.0113 - Validation Loss: 0.0163\n",
      "Epoch 252/500 - Training Loss: 0.0110 - Validation Loss: 0.0159\n",
      "Epoch 253/500 - Training Loss: 0.0100 - Validation Loss: 0.0156\n",
      "Epoch 254/500 - Training Loss: 0.0110 - Validation Loss: 0.0175\n",
      "Epoch 255/500 - Training Loss: 0.0111 - Validation Loss: 0.0165\n",
      "Epoch 256/500 - Training Loss: 0.0113 - Validation Loss: 0.0160\n",
      "Epoch 257/500 - Training Loss: 0.0111 - Validation Loss: 0.0152\n",
      "Epoch 258/500 - Training Loss: 0.0127 - Validation Loss: 0.0166\n",
      "Epoch 259/500 - Training Loss: 0.0099 - Validation Loss: 0.0159\n",
      "Epoch 260/500 - Training Loss: 0.0105 - Validation Loss: 0.0150\n",
      "Epoch 261/500 - Training Loss: 0.0114 - Validation Loss: 0.0146\n",
      "Epoch 262/500 - Training Loss: 0.0110 - Validation Loss: 0.0170\n",
      "Epoch 263/500 - Training Loss: 0.0107 - Validation Loss: 0.0152\n",
      "Epoch 264/500 - Training Loss: 0.0114 - Validation Loss: 0.0166\n",
      "Epoch 265/500 - Training Loss: 0.0111 - Validation Loss: 0.0153\n",
      "Epoch 266/500 - Training Loss: 0.0107 - Validation Loss: 0.0155\n",
      "Epoch 267/500 - Training Loss: 0.0111 - Validation Loss: 0.0159\n",
      "Epoch 268/500 - Training Loss: 0.0123 - Validation Loss: 0.0154\n",
      "Epoch 269/500 - Training Loss: 0.0102 - Validation Loss: 0.0149\n",
      "Epoch 270/500 - Training Loss: 0.0100 - Validation Loss: 0.0149\n",
      "Epoch 271/500 - Training Loss: 0.0110 - Validation Loss: 0.0151\n",
      "Epoch 272/500 - Training Loss: 0.0113 - Validation Loss: 0.0152\n",
      "Epoch 273/500 - Training Loss: 0.0102 - Validation Loss: 0.0154\n",
      "Epoch 274/500 - Training Loss: 0.0111 - Validation Loss: 0.0152\n",
      "Epoch 275/500 - Training Loss: 0.0115 - Validation Loss: 0.0156\n",
      "Epoch 276/500 - Training Loss: 0.0107 - Validation Loss: 0.0161\n",
      "Epoch 277/500 - Training Loss: 0.0105 - Validation Loss: 0.0159\n",
      "Epoch 278/500 - Training Loss: 0.0112 - Validation Loss: 0.0158\n",
      "Epoch 279/500 - Training Loss: 0.0112 - Validation Loss: 0.0144\n",
      "Epoch 280/500 - Training Loss: 0.0104 - Validation Loss: 0.0151\n",
      "Epoch 281/500 - Training Loss: 0.0112 - Validation Loss: 0.0149\n",
      "Epoch 282/500 - Training Loss: 0.0106 - Validation Loss: 0.0147\n",
      "Epoch 283/500 - Training Loss: 0.0103 - Validation Loss: 0.0156\n",
      "Epoch 284/500 - Training Loss: 0.0109 - Validation Loss: 0.0165\n",
      "Epoch 285/500 - Training Loss: 0.0105 - Validation Loss: 0.0147\n",
      "Epoch 286/500 - Training Loss: 0.0111 - Validation Loss: 0.0147\n",
      "Epoch 287/500 - Training Loss: 0.0094 - Validation Loss: 0.0158\n",
      "Epoch 288/500 - Training Loss: 0.0093 - Validation Loss: 0.0152\n",
      "Epoch 289/500 - Training Loss: 0.0101 - Validation Loss: 0.0154\n",
      "Epoch 290/500 - Training Loss: 0.0110 - Validation Loss: 0.0157\n",
      "Epoch 291/500 - Training Loss: 0.0111 - Validation Loss: 0.0151\n",
      "Epoch 292/500 - Training Loss: 0.0096 - Validation Loss: 0.0149\n",
      "Epoch 293/500 - Training Loss: 0.0106 - Validation Loss: 0.0156\n",
      "Epoch 294/500 - Training Loss: 0.0115 - Validation Loss: 0.0149\n",
      "Epoch 295/500 - Training Loss: 0.0108 - Validation Loss: 0.0157\n",
      "Epoch 296/500 - Training Loss: 0.0105 - Validation Loss: 0.0154\n",
      "Epoch 297/500 - Training Loss: 0.0102 - Validation Loss: 0.0152\n",
      "Epoch 298/500 - Training Loss: 0.0105 - Validation Loss: 0.0150\n",
      "Epoch 299/500 - Training Loss: 0.0101 - Validation Loss: 0.0148\n",
      "Epoch 300/500 - Training Loss: 0.0107 - Validation Loss: 0.0158\n",
      "Epoch 301/500 - Training Loss: 0.0105 - Validation Loss: 0.0144\n",
      "Epoch 302/500 - Training Loss: 0.0108 - Validation Loss: 0.0160\n",
      "Epoch 303/500 - Training Loss: 0.0102 - Validation Loss: 0.0169\n",
      "Epoch 304/500 - Training Loss: 0.0111 - Validation Loss: 0.0155\n",
      "Epoch 305/500 - Training Loss: 0.0095 - Validation Loss: 0.0136\n",
      "Epoch 306/500 - Training Loss: 0.0102 - Validation Loss: 0.0157\n",
      "Epoch 307/500 - Training Loss: 0.0099 - Validation Loss: 0.0150\n",
      "Epoch 308/500 - Training Loss: 0.0109 - Validation Loss: 0.0152\n",
      "Epoch 309/500 - Training Loss: 0.0101 - Validation Loss: 0.0142\n",
      "Epoch 310/500 - Training Loss: 0.0102 - Validation Loss: 0.0150\n",
      "Epoch 311/500 - Training Loss: 0.0104 - Validation Loss: 0.0143\n",
      "Epoch 312/500 - Training Loss: 0.0094 - Validation Loss: 0.0146\n",
      "Epoch 313/500 - Training Loss: 0.0097 - Validation Loss: 0.0147\n",
      "Epoch 314/500 - Training Loss: 0.0095 - Validation Loss: 0.0140\n",
      "Epoch 315/500 - Training Loss: 0.0104 - Validation Loss: 0.0153\n",
      "Epoch 316/500 - Training Loss: 0.0102 - Validation Loss: 0.0153\n",
      "Epoch 317/500 - Training Loss: 0.0100 - Validation Loss: 0.0138\n",
      "Epoch 318/500 - Training Loss: 0.0100 - Validation Loss: 0.0124\n",
      "Epoch 319/500 - Training Loss: 0.0099 - Validation Loss: 0.0149\n",
      "Epoch 320/500 - Training Loss: 0.0096 - Validation Loss: 0.0138\n",
      "Epoch 321/500 - Training Loss: 0.0090 - Validation Loss: 0.0139\n",
      "Epoch 322/500 - Training Loss: 0.0103 - Validation Loss: 0.0141\n",
      "Epoch 323/500 - Training Loss: 0.0091 - Validation Loss: 0.0135\n",
      "Epoch 324/500 - Training Loss: 0.0100 - Validation Loss: 0.0134\n",
      "Epoch 325/500 - Training Loss: 0.0099 - Validation Loss: 0.0134\n",
      "Epoch 326/500 - Training Loss: 0.0096 - Validation Loss: 0.0136\n",
      "Epoch 327/500 - Training Loss: 0.0108 - Validation Loss: 0.0138\n",
      "Epoch 328/500 - Training Loss: 0.0109 - Validation Loss: 0.0137\n",
      "Epoch 329/500 - Training Loss: 0.0096 - Validation Loss: 0.0137\n",
      "Epoch 330/500 - Training Loss: 0.0097 - Validation Loss: 0.0146\n",
      "Epoch 331/500 - Training Loss: 0.0104 - Validation Loss: 0.0139\n",
      "Epoch 332/500 - Training Loss: 0.0093 - Validation Loss: 0.0135\n",
      "Epoch 333/500 - Training Loss: 0.0107 - Validation Loss: 0.0145\n",
      "Epoch 334/500 - Training Loss: 0.0106 - Validation Loss: 0.0140\n",
      "Epoch 335/500 - Training Loss: 0.0099 - Validation Loss: 0.0141\n",
      "Epoch 336/500 - Training Loss: 0.0089 - Validation Loss: 0.0147\n",
      "Epoch 337/500 - Training Loss: 0.0106 - Validation Loss: 0.0144\n",
      "Epoch 338/500 - Training Loss: 0.0111 - Validation Loss: 0.0138\n",
      "Epoch 339/500 - Training Loss: 0.0093 - Validation Loss: 0.0138\n",
      "Epoch 340/500 - Training Loss: 0.0094 - Validation Loss: 0.0138\n",
      "Epoch 341/500 - Training Loss: 0.0086 - Validation Loss: 0.0131\n",
      "Epoch 342/500 - Training Loss: 0.0094 - Validation Loss: 0.0128\n",
      "Epoch 343/500 - Training Loss: 0.0100 - Validation Loss: 0.0150\n",
      "Epoch 344/500 - Training Loss: 0.0108 - Validation Loss: 0.0152\n",
      "Epoch 345/500 - Training Loss: 0.0101 - Validation Loss: 0.0134\n",
      "Epoch 346/500 - Training Loss: 0.0096 - Validation Loss: 0.0145\n",
      "Epoch 347/500 - Training Loss: 0.0087 - Validation Loss: 0.0134\n",
      "Epoch 348/500 - Training Loss: 0.0102 - Validation Loss: 0.0134\n",
      "Epoch 349/500 - Training Loss: 0.0100 - Validation Loss: 0.0129\n",
      "Epoch 350/500 - Training Loss: 0.0094 - Validation Loss: 0.0124\n",
      "Epoch 351/500 - Training Loss: 0.0101 - Validation Loss: 0.0139\n",
      "Epoch 352/500 - Training Loss: 0.0092 - Validation Loss: 0.0131\n",
      "Epoch 353/500 - Training Loss: 0.0094 - Validation Loss: 0.0136\n",
      "Epoch 354/500 - Training Loss: 0.0098 - Validation Loss: 0.0135\n",
      "Epoch 355/500 - Training Loss: 0.0098 - Validation Loss: 0.0131\n",
      "Epoch 356/500 - Training Loss: 0.0096 - Validation Loss: 0.0124\n",
      "Epoch 357/500 - Training Loss: 0.0086 - Validation Loss: 0.0134\n",
      "Epoch 358/500 - Training Loss: 0.0100 - Validation Loss: 0.0144\n",
      "Epoch 359/500 - Training Loss: 0.0090 - Validation Loss: 0.0124\n",
      "Epoch 360/500 - Training Loss: 0.0086 - Validation Loss: 0.0133\n",
      "Epoch 361/500 - Training Loss: 0.0098 - Validation Loss: 0.0139\n",
      "Epoch 362/500 - Training Loss: 0.0093 - Validation Loss: 0.0133\n",
      "Epoch 363/500 - Training Loss: 0.0100 - Validation Loss: 0.0129\n",
      "Epoch 364/500 - Training Loss: 0.0096 - Validation Loss: 0.0119\n",
      "Epoch 365/500 - Training Loss: 0.0086 - Validation Loss: 0.0135\n",
      "Epoch 366/500 - Training Loss: 0.0098 - Validation Loss: 0.0122\n",
      "Epoch 367/500 - Training Loss: 0.0099 - Validation Loss: 0.0132\n",
      "Epoch 368/500 - Training Loss: 0.0091 - Validation Loss: 0.0125\n",
      "Epoch 369/500 - Training Loss: 0.0093 - Validation Loss: 0.0136\n",
      "Epoch 370/500 - Training Loss: 0.0094 - Validation Loss: 0.0141\n",
      "Epoch 371/500 - Training Loss: 0.0095 - Validation Loss: 0.0130\n",
      "Epoch 372/500 - Training Loss: 0.0098 - Validation Loss: 0.0140\n",
      "Epoch 373/500 - Training Loss: 0.0088 - Validation Loss: 0.0125\n",
      "Epoch 374/500 - Training Loss: 0.0083 - Validation Loss: 0.0123\n",
      "Epoch 375/500 - Training Loss: 0.0093 - Validation Loss: 0.0125\n",
      "Epoch 376/500 - Training Loss: 0.0092 - Validation Loss: 0.0134\n",
      "Epoch 377/500 - Training Loss: 0.0088 - Validation Loss: 0.0142\n",
      "Epoch 378/500 - Training Loss: 0.0097 - Validation Loss: 0.0146\n",
      "Epoch 379/500 - Training Loss: 0.0104 - Validation Loss: 0.0136\n",
      "Epoch 380/500 - Training Loss: 0.0098 - Validation Loss: 0.0140\n",
      "Epoch 381/500 - Training Loss: 0.0095 - Validation Loss: 0.0126\n",
      "Epoch 382/500 - Training Loss: 0.0094 - Validation Loss: 0.0131\n",
      "Epoch 383/500 - Training Loss: 0.0100 - Validation Loss: 0.0121\n",
      "Epoch 384/500 - Training Loss: 0.0091 - Validation Loss: 0.0128\n",
      "Epoch 385/500 - Training Loss: 0.0086 - Validation Loss: 0.0130\n",
      "Epoch 386/500 - Training Loss: 0.0099 - Validation Loss: 0.0120\n",
      "Epoch 387/500 - Training Loss: 0.0096 - Validation Loss: 0.0131\n",
      "Epoch 388/500 - Training Loss: 0.0091 - Validation Loss: 0.0123\n",
      "Epoch 389/500 - Training Loss: 0.0103 - Validation Loss: 0.0130\n",
      "Epoch 390/500 - Training Loss: 0.0087 - Validation Loss: 0.0136\n",
      "Epoch 391/500 - Training Loss: 0.0087 - Validation Loss: 0.0131\n",
      "Epoch 392/500 - Training Loss: 0.0102 - Validation Loss: 0.0130\n",
      "Epoch 393/500 - Training Loss: 0.0091 - Validation Loss: 0.0124\n",
      "Epoch 394/500 - Training Loss: 0.0095 - Validation Loss: 0.0120\n",
      "Epoch 395/500 - Training Loss: 0.0095 - Validation Loss: 0.0131\n",
      "Epoch 396/500 - Training Loss: 0.0087 - Validation Loss: 0.0125\n",
      "Epoch 397/500 - Training Loss: 0.0089 - Validation Loss: 0.0120\n",
      "Epoch 398/500 - Training Loss: 0.0091 - Validation Loss: 0.0133\n",
      "Epoch 399/500 - Training Loss: 0.0088 - Validation Loss: 0.0122\n",
      "Epoch 400/500 - Training Loss: 0.0086 - Validation Loss: 0.0123\n",
      "Epoch 401/500 - Training Loss: 0.0083 - Validation Loss: 0.0140\n",
      "Epoch 402/500 - Training Loss: 0.0098 - Validation Loss: 0.0127\n",
      "Epoch 403/500 - Training Loss: 0.0101 - Validation Loss: 0.0133\n",
      "Epoch 404/500 - Training Loss: 0.0084 - Validation Loss: 0.0126\n",
      "Epoch 405/500 - Training Loss: 0.0090 - Validation Loss: 0.0118\n",
      "Epoch 406/500 - Training Loss: 0.0092 - Validation Loss: 0.0124\n",
      "Epoch 407/500 - Training Loss: 0.0087 - Validation Loss: 0.0116\n",
      "Epoch 408/500 - Training Loss: 0.0086 - Validation Loss: 0.0120\n",
      "Epoch 409/500 - Training Loss: 0.0082 - Validation Loss: 0.0114\n",
      "Epoch 410/500 - Training Loss: 0.0095 - Validation Loss: 0.0126\n",
      "Epoch 411/500 - Training Loss: 0.0090 - Validation Loss: 0.0119\n",
      "Epoch 412/500 - Training Loss: 0.0089 - Validation Loss: 0.0119\n",
      "Epoch 413/500 - Training Loss: 0.0095 - Validation Loss: 0.0122\n",
      "Epoch 414/500 - Training Loss: 0.0087 - Validation Loss: 0.0115\n",
      "Epoch 415/500 - Training Loss: 0.0080 - Validation Loss: 0.0127\n",
      "Epoch 416/500 - Training Loss: 0.0085 - Validation Loss: 0.0122\n",
      "Epoch 417/500 - Training Loss: 0.0100 - Validation Loss: 0.0130\n",
      "Epoch 418/500 - Training Loss: 0.0079 - Validation Loss: 0.0124\n",
      "Epoch 419/500 - Training Loss: 0.0080 - Validation Loss: 0.0119\n",
      "Epoch 420/500 - Training Loss: 0.0088 - Validation Loss: 0.0111\n",
      "Epoch 421/500 - Training Loss: 0.0088 - Validation Loss: 0.0115\n",
      "Epoch 422/500 - Training Loss: 0.0092 - Validation Loss: 0.0120\n",
      "Epoch 423/500 - Training Loss: 0.0085 - Validation Loss: 0.0114\n",
      "Epoch 424/500 - Training Loss: 0.0083 - Validation Loss: 0.0136\n",
      "Epoch 425/500 - Training Loss: 0.0089 - Validation Loss: 0.0130\n",
      "Epoch 426/500 - Training Loss: 0.0081 - Validation Loss: 0.0121\n",
      "Epoch 427/500 - Training Loss: 0.0080 - Validation Loss: 0.0108\n",
      "Epoch 428/500 - Training Loss: 0.0076 - Validation Loss: 0.0121\n",
      "Epoch 429/500 - Training Loss: 0.0083 - Validation Loss: 0.0113\n",
      "Epoch 430/500 - Training Loss: 0.0086 - Validation Loss: 0.0118\n",
      "Epoch 431/500 - Training Loss: 0.0085 - Validation Loss: 0.0115\n",
      "Epoch 432/500 - Training Loss: 0.0083 - Validation Loss: 0.0127\n",
      "Epoch 433/500 - Training Loss: 0.0086 - Validation Loss: 0.0109\n",
      "Epoch 434/500 - Training Loss: 0.0093 - Validation Loss: 0.0120\n",
      "Epoch 435/500 - Training Loss: 0.0091 - Validation Loss: 0.0124\n",
      "Epoch 436/500 - Training Loss: 0.0069 - Validation Loss: 0.0108\n",
      "Epoch 437/500 - Training Loss: 0.0085 - Validation Loss: 0.0116\n",
      "Epoch 438/500 - Training Loss: 0.0085 - Validation Loss: 0.0116\n",
      "Epoch 439/500 - Training Loss: 0.0079 - Validation Loss: 0.0117\n",
      "Epoch 440/500 - Training Loss: 0.0081 - Validation Loss: 0.0122\n",
      "Epoch 441/500 - Training Loss: 0.0088 - Validation Loss: 0.0119\n",
      "Epoch 442/500 - Training Loss: 0.0085 - Validation Loss: 0.0124\n",
      "Epoch 443/500 - Training Loss: 0.0078 - Validation Loss: 0.0119\n",
      "Epoch 444/500 - Training Loss: 0.0077 - Validation Loss: 0.0118\n",
      "Epoch 445/500 - Training Loss: 0.0087 - Validation Loss: 0.0116\n",
      "Epoch 446/500 - Training Loss: 0.0077 - Validation Loss: 0.0103\n",
      "Epoch 447/500 - Training Loss: 0.0093 - Validation Loss: 0.0108\n",
      "Epoch 448/500 - Training Loss: 0.0074 - Validation Loss: 0.0125\n",
      "Epoch 449/500 - Training Loss: 0.0082 - Validation Loss: 0.0113\n",
      "Epoch 450/500 - Training Loss: 0.0084 - Validation Loss: 0.0111\n",
      "Epoch 451/500 - Training Loss: 0.0106 - Validation Loss: 0.0111\n",
      "Epoch 452/500 - Training Loss: 0.0074 - Validation Loss: 0.0109\n",
      "Epoch 453/500 - Training Loss: 0.0082 - Validation Loss: 0.0118\n",
      "Epoch 454/500 - Training Loss: 0.0095 - Validation Loss: 0.0120\n",
      "Epoch 455/500 - Training Loss: 0.0080 - Validation Loss: 0.0120\n",
      "Epoch 456/500 - Training Loss: 0.0084 - Validation Loss: 0.0111\n",
      "Epoch 457/500 - Training Loss: 0.0090 - Validation Loss: 0.0113\n",
      "Epoch 458/500 - Training Loss: 0.0076 - Validation Loss: 0.0114\n",
      "Epoch 459/500 - Training Loss: 0.0085 - Validation Loss: 0.0111\n",
      "Epoch 460/500 - Training Loss: 0.0081 - Validation Loss: 0.0099\n",
      "Epoch 461/500 - Training Loss: 0.0090 - Validation Loss: 0.0111\n",
      "Epoch 462/500 - Training Loss: 0.0084 - Validation Loss: 0.0114\n",
      "Epoch 463/500 - Training Loss: 0.0078 - Validation Loss: 0.0108\n",
      "Epoch 464/500 - Training Loss: 0.0077 - Validation Loss: 0.0116\n",
      "Epoch 465/500 - Training Loss: 0.0077 - Validation Loss: 0.0110\n",
      "Epoch 466/500 - Training Loss: 0.0079 - Validation Loss: 0.0111\n",
      "Epoch 467/500 - Training Loss: 0.0076 - Validation Loss: 0.0112\n",
      "Epoch 468/500 - Training Loss: 0.0082 - Validation Loss: 0.0119\n",
      "Epoch 469/500 - Training Loss: 0.0080 - Validation Loss: 0.0115\n",
      "Epoch 470/500 - Training Loss: 0.0073 - Validation Loss: 0.0109\n",
      "Epoch 471/500 - Training Loss: 0.0082 - Validation Loss: 0.0105\n",
      "Epoch 472/500 - Training Loss: 0.0076 - Validation Loss: 0.0107\n",
      "Epoch 473/500 - Training Loss: 0.0072 - Validation Loss: 0.0117\n",
      "Epoch 474/500 - Training Loss: 0.0077 - Validation Loss: 0.0110\n",
      "Epoch 475/500 - Training Loss: 0.0079 - Validation Loss: 0.0113\n",
      "Epoch 476/500 - Training Loss: 0.0081 - Validation Loss: 0.0103\n",
      "Epoch 477/500 - Training Loss: 0.0078 - Validation Loss: 0.0106\n",
      "Epoch 478/500 - Training Loss: 0.0086 - Validation Loss: 0.0118\n",
      "Epoch 479/500 - Training Loss: 0.0076 - Validation Loss: 0.0105\n",
      "Epoch 480/500 - Training Loss: 0.0081 - Validation Loss: 0.0109\n",
      "Epoch 481/500 - Training Loss: 0.0086 - Validation Loss: 0.0109\n",
      "Epoch 482/500 - Training Loss: 0.0085 - Validation Loss: 0.0103\n",
      "Epoch 483/500 - Training Loss: 0.0081 - Validation Loss: 0.0107\n",
      "Epoch 484/500 - Training Loss: 0.0079 - Validation Loss: 0.0115\n",
      "Epoch 485/500 - Training Loss: 0.0084 - Validation Loss: 0.0116\n",
      "Epoch 486/500 - Training Loss: 0.0081 - Validation Loss: 0.0113\n",
      "Epoch 487/500 - Training Loss: 0.0081 - Validation Loss: 0.0111\n",
      "Epoch 488/500 - Training Loss: 0.0080 - Validation Loss: 0.0106\n",
      "Epoch 489/500 - Training Loss: 0.0075 - Validation Loss: 0.0114\n",
      "Epoch 490/500 - Training Loss: 0.0085 - Validation Loss: 0.0104\n",
      "Epoch 491/500 - Training Loss: 0.0079 - Validation Loss: 0.0106\n",
      "Epoch 492/500 - Training Loss: 0.0078 - Validation Loss: 0.0112\n",
      "Epoch 493/500 - Training Loss: 0.0077 - Validation Loss: 0.0097\n",
      "Epoch 494/500 - Training Loss: 0.0075 - Validation Loss: 0.0109\n",
      "Epoch 495/500 - Training Loss: 0.0085 - Validation Loss: 0.0097\n",
      "Epoch 496/500 - Training Loss: 0.0085 - Validation Loss: 0.0107\n",
      "Epoch 497/500 - Training Loss: 0.0072 - Validation Loss: 0.0110\n",
      "Epoch 498/500 - Training Loss: 0.0082 - Validation Loss: 0.0111\n",
      "Epoch 499/500 - Training Loss: 0.0091 - Validation Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:35:26,690] A new study created in memory with name: no-name-387c595f-41a0-4cd6-b9e5-3b6e24b43213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/500 - Training Loss: 0.0085 - Validation Loss: 0.0106\n",
      "Training of the best model completed.\n",
      "\n",
      "===== Model Performance =====\n",
      "Training Set:\n",
      "  R2 Score: 0.3587\n",
      "  MSE: 0.0076\n",
      "  MAE: 0.0589\n",
      "\n",
      "Testing Set:\n",
      "  R2 Score: 0.3360\n",
      "  MSE: 0.0084\n",
      "  MAE: 0.0596\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:35:27,620] Trial 0 finished with value: -135108464.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 745, 'n_units_l1': 845, 'n_units_l2': 842, 'n_units_l3': 193, 'n_units_l4': 149, 'dropout_rate': 0.43465278421990605, 'lr': 0.003495634984889225, 'optimizer': 'RMSprop'}. Best is trial 0 with value: -135108464.0.\n",
      "[I 2024-09-25 14:35:28,532] Trial 1 finished with value: -17904928.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 767, 'n_units_l1': 710, 'n_units_l2': 622, 'n_units_l3': 650, 'dropout_rate': 0.5686480654494892, 'lr': 0.002281333394368333, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:29,260] Trial 2 finished with value: -241688688.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 422, 'n_units_l1': 908, 'n_units_l2': 653, 'dropout_rate': 0.2655188857422537, 'lr': 0.002367026879349599, 'optimizer': 'RMSprop'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:30,117] Trial 3 finished with value: -147098352.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 1000, 'n_units_l1': 676, 'n_units_l2': 641, 'n_units_l3': 782, 'dropout_rate': 0.5615171687901332, 'lr': 0.005487308548333344, 'optimizer': 'RMSprop'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:30,728] Trial 4 finished with value: -369529664.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 516, 'n_units_l1': 488, 'dropout_rate': 0.40039989754037897, 'lr': 1.0353393771764604e-05, 'optimizer': 'RMSprop'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:31,621] Trial 5 finished with value: -233081296.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 390, 'n_units_l1': 866, 'n_units_l2': 163, 'n_units_l3': 377, 'n_units_l4': 823, 'dropout_rate': 0.46828538245164175, 'lr': 5.183465484641673e-05, 'optimizer': 'RMSprop'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:32,260] Trial 6 finished with value: -115460232.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 598, 'n_units_l1': 130, 'dropout_rate': 0.4468971664180771, 'lr': 0.00014972097931596493, 'optimizer': 'AdamW'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:32,951] Trial 7 finished with value: -119293080.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 867, 'n_units_l1': 616, 'dropout_rate': 0.3591446106453796, 'lr': 0.00011901319347408672, 'optimizer': 'AdamW'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:33,982] Trial 8 finished with value: -211463216.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 456, 'n_units_l1': 317, 'n_units_l2': 473, 'n_units_l3': 731, 'n_units_l4': 766, 'dropout_rate': 0.5757650291271488, 'lr': 0.00015657807353938808, 'optimizer': 'AdamW'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:34,978] Trial 9 finished with value: -377129184.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 322, 'n_units_l1': 433, 'n_units_l2': 618, 'n_units_l3': 338, 'n_units_l4': 917, 'dropout_rate': 0.5663187588848009, 'lr': 7.64471260737841e-05, 'optimizer': 'AdamW'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:36,112] Trial 10 finished with value: -44374264.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 712, 'n_units_l1': 1002, 'n_units_l2': 1011, 'n_units_l3': 1006, 'dropout_rate': 0.20775316033512306, 'lr': 0.0009118431838174929, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:37,234] Trial 11 finished with value: -41367712.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 725, 'n_units_l1': 725, 'n_units_l2': 937, 'n_units_l3': 997, 'dropout_rate': 0.2521987728368658, 'lr': 0.0007571209499524246, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:38,323] Trial 12 finished with value: -37616636.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 204, 'n_units_l1': 712, 'n_units_l2': 997, 'n_units_l3': 1013, 'dropout_rate': 0.3058675797709178, 'lr': 0.0007431237904941285, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:39,186] Trial 13 finished with value: -49084552.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 250, 'n_units_l1': 742, 'n_units_l2': 321, 'dropout_rate': 0.33063789348485406, 'lr': 0.0009157059323194451, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:39,930] Trial 14 finished with value: -4055130624.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 150, 'n_units_l1': 505, 'n_units_l2': 770, 'dropout_rate': 0.5106357812425563, 'lr': 0.009349315913173015, 'optimizer': 'SGD'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:41,004] Trial 15 finished with value: -33630320.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 872, 'n_units_l1': 777, 'n_units_l2': 444, 'n_units_l3': 611, 'dropout_rate': 0.326292686403955, 'lr': 0.0005414879303056384, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:42,108] Trial 16 finished with value: -42242240.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 982, 'n_units_l1': 802, 'n_units_l2': 456, 'n_units_l3': 579, 'dropout_rate': 0.3690284422361367, 'lr': 0.00035787347673358965, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:42,872] Trial 17 finished with value: -6213026304.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 859, 'n_units_l1': 994, 'n_units_l2': 419, 'dropout_rate': 0.5014871096257849, 'lr': 0.0023362025023644733, 'optimizer': 'SGD'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:43,948] Trial 18 finished with value: -25813744.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 873, 'n_units_l1': 597, 'n_units_l2': 256, 'n_units_l3': 598, 'dropout_rate': 0.4048124494769701, 'lr': 0.00035152212603359095, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:44,850] Trial 19 finished with value: -126926904.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 632, 'n_units_l1': 366, 'n_units_l2': 198, 'dropout_rate': 0.5196647793709702, 'lr': 2.1765044629657696e-05, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:46,039] Trial 20 finished with value: -30373262.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 801, 'n_units_l1': 592, 'n_units_l2': 283, 'n_units_l3': 449, 'n_units_l4': 362, 'dropout_rate': 0.3969188407294976, 'lr': 0.0017157057389420012, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:47,073] Trial 21 finished with value: -40511252.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 775, 'n_units_l1': 592, 'n_units_l2': 279, 'n_units_l3': 456, 'n_units_l4': 334, 'dropout_rate': 0.4073822430403853, 'lr': 0.0022944112963792437, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:48,080] Trial 22 finished with value: -30343018.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 914, 'n_units_l1': 542, 'n_units_l2': 292, 'n_units_l3': 703, 'n_units_l4': 496, 'dropout_rate': 0.4776548884376787, 'lr': 0.0013938246948975107, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:49,168] Trial 23 finished with value: -56814068.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 948, 'n_units_l1': 521, 'n_units_l2': 543, 'n_units_l3': 745, 'dropout_rate': 0.5949633532100562, 'lr': 0.000282864396786897, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:50,040] Trial 24 finished with value: -5406273536.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 664, 'n_units_l1': 655, 'n_units_l2': 351, 'n_units_l3': 657, 'dropout_rate': 0.48252842803233154, 'lr': 0.005692572515272426, 'optimizer': 'SGD'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:51,297] Trial 25 finished with value: -18899584.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 903, 'n_units_l1': 314, 'n_units_l2': 744, 'n_units_l3': 866, 'n_units_l4': 607, 'dropout_rate': 0.528548227222289, 'lr': 0.0011795349945956904, 'optimizer': 'Adam'}. Best is trial 1 with value: -17904928.0.\n",
      "[I 2024-09-25 14:35:52,371] Trial 26 finished with value: -15238044.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 1022, 'n_units_l1': 214, 'n_units_l2': 731, 'n_units_l3': 866, 'dropout_rate': 0.5403306025811564, 'lr': 0.0003400355173911401, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:53,493] Trial 27 finished with value: -49119728.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 995, 'n_units_l1': 198, 'n_units_l2': 729, 'n_units_l3': 855, 'n_units_l4': 574, 'dropout_rate': 0.5338294433813022, 'lr': 0.00023186796947786688, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:54,429] Trial 28 finished with value: -34940588.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 815, 'n_units_l1': 277, 'n_units_l2': 752, 'n_units_l3': 879, 'dropout_rate': 0.5432720721617083, 'lr': 0.0004591978480380792, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:55,229] Trial 29 finished with value: -4908504064.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 933, 'n_units_l1': 236, 'n_units_l2': 872, 'n_units_l3': 861, 'n_units_l4': 1020, 'dropout_rate': 0.5426775563846793, 'lr': 0.003923585727066535, 'optimizer': 'SGD'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:56,123] Trial 30 finished with value: -24339986.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 1020, 'n_units_l1': 418, 'n_units_l2': 536, 'dropout_rate': 0.5998859888930556, 'lr': 0.0014035839959121055, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:57,046] Trial 31 finished with value: -30622170.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 1011, 'n_units_l1': 381, 'n_units_l2': 546, 'dropout_rate': 0.5977099155810203, 'lr': 0.0013007913352468124, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:57,954] Trial 32 finished with value: -34409224.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 1022, 'n_units_l1': 159, 'n_units_l2': 694, 'dropout_rate': 0.5762892842065247, 'lr': 0.003658260801290374, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:58,818] Trial 33 finished with value: -31383792.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 916, 'n_units_l1': 429, 'n_units_l2': 801, 'dropout_rate': 0.5543796915900077, 'lr': 0.0014728941015901302, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:35:59,452] Trial 34 finished with value: -51171052.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 953, 'n_units_l1': 317, 'dropout_rate': 0.5158098385082376, 'lr': 0.0005770148464901737, 'optimizer': 'RMSprop'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:00,314] Trial 35 finished with value: -159311776.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 802, 'n_units_l1': 269, 'n_units_l2': 875, 'n_units_l3': 923, 'dropout_rate': 0.44642755610300183, 'lr': 0.006000312507782182, 'optimizer': 'RMSprop'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:01,103] Trial 36 finished with value: -36365636.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 683, 'n_units_l1': 458, 'n_units_l2': 661, 'dropout_rate': 0.5936308892662737, 'lr': 0.002196004508424188, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:02,152] Trial 37 finished with value: -20405342.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 574, 'n_units_l1': 363, 'n_units_l2': 587, 'n_units_l3': 819, 'dropout_rate': 0.49772566038743504, 'lr': 0.0036509759737777822, 'optimizer': 'Adam'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:03,356] Trial 38 finished with value: -41569540.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 554, 'n_units_l1': 209, 'n_units_l2': 588, 'n_units_l3': 772, 'n_units_l4': 687, 'dropout_rate': 0.4937008028407022, 'lr': 0.003585774658506497, 'optimizer': 'AdamW'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:04,362] Trial 39 finished with value: -473326976.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 483, 'n_units_l1': 332, 'n_units_l2': 691, 'n_units_l3': 804, 'dropout_rate': 0.46140745395326926, 'lr': 0.009109883698147542, 'optimizer': 'RMSprop'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:05,443] Trial 40 finished with value: -23302184.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 588, 'n_units_l1': 131, 'n_units_l2': 816, 'n_units_l3': 916, 'dropout_rate': 0.529258442286084, 'lr': 0.0029159538249739917, 'optimizer': 'AdamW'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:06,522] Trial 41 finished with value: -19312980.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 584, 'n_units_l1': 134, 'n_units_l2': 796, 'n_units_l3': 919, 'dropout_rate': 0.5286248846780394, 'lr': 0.003060171056243596, 'optimizer': 'AdamW'}. Best is trial 26 with value: -15238044.0.\n",
      "[I 2024-09-25 14:36:07,625] Trial 42 finished with value: -6726183.5 and parameters: {'hidden_layers': 4, 'n_units_l0': 407, 'n_units_l1': 179, 'n_units_l2': 616, 'n_units_l3': 835, 'dropout_rate': 0.5598812303473517, 'lr': 0.004701061920947062, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:08,722] Trial 43 finished with value: -13809629.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 380, 'n_units_l1': 168, 'n_units_l2': 635, 'n_units_l3': 949, 'dropout_rate': 0.5645099320988259, 'lr': 0.005928833614376204, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:09,827] Trial 44 finished with value: -53179560.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 377, 'n_units_l1': 198, 'n_units_l2': 631, 'n_units_l3': 946, 'dropout_rate': 0.5576532670802579, 'lr': 0.007164598292384541, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:11,062] Trial 45 finished with value: -51830644.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 305, 'n_units_l1': 246, 'n_units_l2': 700, 'n_units_l3': 849, 'n_units_l4': 439, 'dropout_rate': 0.5642772053878055, 'lr': 0.0047134021415081865, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:12,123] Trial 46 finished with value: -560556672.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 401, 'n_units_l1': 173, 'n_units_l2': 497, 'n_units_l3': 150, 'dropout_rate': 0.57638393214321, 'lr': 6.134717780201392e-05, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:13,354] Trial 47 finished with value: -23637430.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 445, 'n_units_l1': 270, 'n_units_l2': 643, 'n_units_l3': 660, 'n_units_l4': 642, 'dropout_rate': 0.545426947220469, 'lr': 0.0010545586539323472, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:14,570] Trial 48 finished with value: -466837952.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 513, 'n_units_l1': 871, 'n_units_l2': 724, 'n_units_l3': 951, 'dropout_rate': 0.5648398164361781, 'lr': 0.0001664006499872097, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:15,504] Trial 49 finished with value: -89945288.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 289, 'n_units_l1': 684, 'n_units_l2': 621, 'n_units_l3': 796, 'dropout_rate': 0.572155013080264, 'lr': 0.009953508236865994, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:16,176] Trial 50 finished with value: -25851492.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 345, 'n_units_l1': 227, 'dropout_rate': 0.5825686675224852, 'lr': 0.001931835099358432, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:17,157] Trial 51 finished with value: -30727886.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 521, 'n_units_l1': 141, 'n_units_l2': 780, 'n_units_l3': 895, 'dropout_rate': 0.5276706942586262, 'lr': 0.002742300493129274, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:18,269] Trial 52 finished with value: -22993752.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 755, 'n_units_l1': 180, 'n_units_l2': 842, 'n_units_l3': 957, 'dropout_rate': 0.5107043780069496, 'lr': 0.0044394944236626505, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:19,359] Trial 53 finished with value: -26026482.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 213, 'n_units_l1': 302, 'n_units_l2': 684, 'n_units_l3': 844, 'dropout_rate': 0.5502742278378805, 'lr': 0.007121418371533146, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:20,419] Trial 54 finished with value: -20100468.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 848, 'n_units_l1': 167, 'n_units_l2': 751, 'n_units_l3': 503, 'dropout_rate': 0.5275810917472958, 'lr': 0.002778824315395554, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:21,278] Trial 55 finished with value: -3830481152.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 629, 'n_units_l1': 131, 'n_units_l2': 912, 'n_units_l3': 970, 'dropout_rate': 0.4886815304304895, 'lr': 0.00010522816892877969, 'optimizer': 'SGD'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:22,349] Trial 56 finished with value: -553039232.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 355, 'n_units_l1': 242, 'n_units_l2': 502, 'n_units_l3': 718, 'dropout_rate': 0.5072532365821093, 'lr': 3.9782633919769724e-05, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:23,544] Trial 57 finished with value: -73637664.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 422, 'n_units_l1': 764, 'n_units_l2': 602, 'n_units_l3': 902, 'n_units_l4': 131, 'dropout_rate': 0.46913502487997866, 'lr': 0.0009876649329308234, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:24,529] Trial 58 finished with value: -39764704.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 903, 'n_units_l1': 210, 'n_units_l2': 943, 'n_units_l3': 338, 'dropout_rate': 0.26524944644838355, 'lr': 0.0006766007672265628, 'optimizer': 'RMSprop'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:25,750] Trial 59 finished with value: -28656342.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 715, 'n_units_l1': 635, 'n_units_l2': 667, 'n_units_l3': 991, 'n_units_l4': 271, 'dropout_rate': 0.5844866709159307, 'lr': 0.004969963642624822, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:26,621] Trial 60 finished with value: -4539620864.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 978, 'n_units_l1': 285, 'n_units_l2': 724, 'n_units_l3': 792, 'dropout_rate': 0.4228990041555857, 'lr': 0.007391748450322114, 'optimizer': 'SGD'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:27,501] Trial 61 finished with value: -13962608.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 846, 'n_units_l1': 153, 'n_units_l2': 763, 'n_units_l3': 479, 'dropout_rate': 0.5315664870904615, 'lr': 0.0025970671792479613, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:28,375] Trial 62 finished with value: -24447140.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 836, 'n_units_l1': 181, 'n_units_l2': 796, 'n_units_l3': 379, 'dropout_rate': 0.5370241899486643, 'lr': 0.0017633560841787527, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:29,262] Trial 63 finished with value: -25046484.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 887, 'n_units_l1': 159, 'n_units_l2': 758, 'n_units_l3': 518, 'dropout_rate': 0.5586353071092259, 'lr': 0.002983051237929483, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:30,156] Trial 64 finished with value: -31130610.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 743, 'n_units_l1': 224, 'n_units_l2': 835, 'n_units_l3': 541, 'dropout_rate': 0.5159510489409306, 'lr': 0.002102931683226632, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:31,103] Trial 65 finished with value: -19761658.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 966, 'n_units_l1': 926, 'n_units_l2': 560, 'n_units_l3': 261, 'dropout_rate': 0.5459371446083661, 'lr': 0.005816301582163072, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:31,881] Trial 66 finished with value: -31648742.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 776, 'n_units_l1': 251, 'n_units_l2': 725, 'dropout_rate': 0.5210095050686018, 'lr': 0.0011421116199742355, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:32,875] Trial 67 finished with value: -9682556.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 475, 'n_units_l1': 192, 'n_units_l2': 654, 'n_units_l3': 442, 'n_units_l4': 552, 'dropout_rate': 0.3748633006060377, 'lr': 0.0008178389326268558, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:33,861] Trial 68 finished with value: -20546854.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 481, 'n_units_l1': 195, 'n_units_l2': 647, 'n_units_l3': 437, 'n_units_l4': 556, 'dropout_rate': 0.3571222238519409, 'lr': 0.0008160017515649549, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:34,894] Trial 69 finished with value: -16870606.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 429, 'n_units_l1': 335, 'n_units_l2': 613, 'n_units_l3': 482, 'n_units_l4': 732, 'dropout_rate': 0.38028491244792645, 'lr': 0.000482051343820529, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:35,917] Trial 70 finished with value: -24400306.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 434, 'n_units_l1': 346, 'n_units_l2': 608, 'n_units_l3': 409, 'n_units_l4': 746, 'dropout_rate': 0.3896600667482881, 'lr': 0.0003958626423920045, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:36,950] Trial 71 finished with value: -13020269.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 403, 'n_units_l1': 556, 'n_units_l2': 681, 'n_units_l3': 558, 'n_units_l4': 638, 'dropout_rate': 0.38094498331376386, 'lr': 0.00046153095280268574, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:38,204] Trial 72 finished with value: -9455776.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 380, 'n_units_l1': 809, 'n_units_l2': 573, 'n_units_l3': 566, 'n_units_l4': 841, 'dropout_rate': 0.38403519936027536, 'lr': 0.00027951309944819117, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:39,246] Trial 73 finished with value: -30183898.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 402, 'n_units_l1': 547, 'n_units_l2': 520, 'n_units_l3': 487, 'n_units_l4': 861, 'dropout_rate': 0.3718709495066962, 'lr': 0.0002559055738695773, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:40,273] Trial 74 finished with value: -49288732.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 467, 'n_units_l1': 488, 'n_units_l2': 561, 'n_units_l3': 566, 'n_units_l4': 717, 'dropout_rate': 0.33772834288671183, 'lr': 0.00019473055865532304, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:41,287] Trial 75 finished with value: -26610438.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 374, 'n_units_l1': 154, 'n_units_l2': 579, 'n_units_l3': 470, 'n_units_l4': 923, 'dropout_rate': 0.38449089883801174, 'lr': 0.0004692967936364072, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:42,333] Trial 76 finished with value: -17754516.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 267, 'n_units_l1': 984, 'n_units_l2': 421, 'n_units_l3': 613, 'n_units_l4': 661, 'dropout_rate': 0.41616299034631926, 'lr': 0.00030945416511053036, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:43,413] Trial 77 finished with value: -19492128.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 507, 'n_units_l1': 807, 'n_units_l2': 624, 'n_units_l3': 558, 'n_units_l4': 806, 'dropout_rate': 0.35063781217124007, 'lr': 0.0005978538654318749, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:44,399] Trial 78 finished with value: -100921688.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 420, 'n_units_l1': 211, 'n_units_l2': 678, 'n_units_l3': 536, 'n_units_l4': 519, 'dropout_rate': 0.37583458908099876, 'lr': 0.0004156602635704683, 'optimizer': 'RMSprop'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:45,257] Trial 79 finished with value: -5634775552.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 329, 'n_units_l1': 457, 'n_units_l2': 710, 'n_units_l3': 414, 'n_units_l4': 471, 'dropout_rate': 0.30967987506056865, 'lr': 0.00019297311434813855, 'optimizer': 'SGD'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:46,392] Trial 80 finished with value: -30717336.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 545, 'n_units_l1': 398, 'n_units_l2': 657, 'n_units_l3': 600, 'n_units_l4': 628, 'dropout_rate': 0.3407890267141652, 'lr': 0.00034200513683890245, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:47,542] Trial 81 finished with value: -17989616.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 265, 'n_units_l1': 919, 'n_units_l2': 362, 'n_units_l3': 625, 'n_units_l4': 705, 'dropout_rate': 0.42801662911971067, 'lr': 0.00028971707284514977, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:48,818] Trial 82 finished with value: -28470742.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 179, 'n_units_l1': 1017, 'n_units_l2': 411, 'n_units_l3': 1024, 'n_units_l4': 659, 'dropout_rate': 0.4115837150084341, 'lr': 0.0004985099270267282, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:50,119] Trial 83 finished with value: -61465080.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 361, 'n_units_l1': 858, 'n_units_l2': 472, 'n_units_l3': 684, 'n_units_l4': 780, 'dropout_rate': 0.20197212775706477, 'lr': 0.0002224513721156429, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:51,388] Trial 84 finished with value: -167890432.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 308, 'n_units_l1': 960, 'n_units_l2': 598, 'n_units_l3': 501, 'n_units_l4': 577, 'dropout_rate': 0.3976566079427763, 'lr': 0.00011890583034857693, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:52,628] Trial 85 finished with value: -24180260.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 388, 'n_units_l1': 686, 'n_units_l2': 198, 'n_units_l3': 457, 'n_units_l4': 874, 'dropout_rate': 0.36563811039000305, 'lr': 0.0006929977887201669, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:53,931] Trial 86 finished with value: -43256956.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 257, 'n_units_l1': 813, 'n_units_l2': 637, 'n_units_l3': 339, 'n_units_l4': 731, 'dropout_rate': 0.3814312025549908, 'lr': 0.00029826150390559685, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:55,213] Trial 87 finished with value: -20886494.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 456, 'n_units_l1': 958, 'n_units_l2': 669, 'n_units_l3': 638, 'n_units_l4': 848, 'dropout_rate': 0.4420847556910479, 'lr': 0.000366254665154523, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:56,627] Trial 88 finished with value: -145924896.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 411, 'n_units_l1': 262, 'n_units_l2': 425, 'n_units_l3': 578, 'n_units_l4': 668, 'dropout_rate': 0.35150570625665245, 'lr': 0.00014472870837770133, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:58,242] Trial 89 finished with value: -30579028.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 335, 'n_units_l1': 886, 'n_units_l2': 704, 'n_units_l3': 759, 'n_units_l4': 935, 'dropout_rate': 0.3211851714531577, 'lr': 0.0005457016698112086, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:36:59,498] Trial 90 finished with value: -71297576.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 486, 'n_units_l1': 828, 'n_units_l2': 521, 'n_units_l3': 525, 'n_units_l4': 788, 'dropout_rate': 0.4075557001784466, 'lr': 0.0002251153713130425, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:00,584] Trial 91 finished with value: -41401584.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 219, 'n_units_l1': 723, 'n_units_l2': 567, 'n_units_l3': 600, 'dropout_rate': 0.4158855935971273, 'lr': 0.0008420788989404103, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:01,638] Trial 92 finished with value: -61550884.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 288, 'n_units_l1': 654, 'n_units_l2': 618, 'n_units_l3': 432, 'dropout_rate': 0.23827706819058858, 'lr': 0.0004389065554259834, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:02,706] Trial 93 finished with value: -32777784.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 130, 'n_units_l1': 776, 'n_units_l2': 651, 'n_units_l3': 677, 'dropout_rate': 0.39447107219375244, 'lr': 0.0015991788714714921, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:03,918] Trial 94 finished with value: -58320524.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 685, 'n_units_l1': 551, 'n_units_l2': 775, 'n_units_l3': 482, 'n_units_l4': 438, 'dropout_rate': 0.5861342086394725, 'lr': 0.00032134567418527017, 'optimizer': 'Adam'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:04,852] Trial 95 finished with value: -47973296.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 438, 'n_units_l1': 291, 'n_units_l2': 685, 'n_units_l3': 554, 'dropout_rate': 0.36422020302849706, 'lr': 0.0042134517013603665, 'optimizer': 'RMSprop'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:05,898] Trial 96 finished with value: -10412936.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 619, 'n_units_l1': 757, 'n_units_l2': 587, 'n_units_l3': 580, 'dropout_rate': 0.5710115578974054, 'lr': 0.0006428206872005723, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:06,739] Trial 97 finished with value: -15318498.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 393, 'n_units_l1': 188, 'n_units_l2': 538, 'dropout_rate': 0.5682006065306328, 'lr': 0.0006608549221572211, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:07,587] Trial 98 finished with value: -24136742.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 372, 'n_units_l1': 747, 'n_units_l2': 542, 'dropout_rate': 0.5726663071269525, 'lr': 0.000588764866400254, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:08,450] Trial 99 finished with value: -17629116.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 396, 'n_units_l1': 179, 'n_units_l2': 494, 'dropout_rate': 0.5595369038450452, 'lr': 0.0006857755902080371, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:09,160] Trial 100 finished with value: -67316752.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 611, 'n_units_l1': 194, 'dropout_rate': 0.5410171144117223, 'lr': 0.0009334949271131914, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:10,017] Trial 101 finished with value: -14572809.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 392, 'n_units_l1': 152, 'n_units_l2': 492, 'dropout_rate': 0.5561272744001455, 'lr': 0.0006929452454180611, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:10,901] Trial 102 finished with value: -18875744.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 348, 'n_units_l1': 222, 'n_units_l2': 594, 'dropout_rate': 0.590212181442483, 'lr': 0.0005093632133327749, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:11,721] Trial 103 finished with value: -16256852.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 460, 'n_units_l1': 146, 'n_units_l2': 525, 'dropout_rate': 0.5675488887278973, 'lr': 0.0012859433900726314, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:12,505] Trial 104 finished with value: -19115214.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 455, 'n_units_l1': 147, 'n_units_l2': 517, 'dropout_rate': 0.5667430142602257, 'lr': 0.0012910498669748212, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:13,249] Trial 105 finished with value: -11900850.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 385, 'n_units_l1': 172, 'n_units_l2': 552, 'dropout_rate': 0.5524281449817604, 'lr': 0.0006110830491909452, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:14,045] Trial 106 finished with value: -18054350.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 383, 'n_units_l1': 173, 'n_units_l2': 477, 'dropout_rate': 0.5503706824665344, 'lr': 0.0007634484709291757, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:14,836] Trial 107 finished with value: -11578110.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 320, 'n_units_l1': 198, 'n_units_l2': 551, 'dropout_rate': 0.5780139328152945, 'lr': 0.0005999881448772189, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:15,718] Trial 108 finished with value: -33968008.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 304, 'n_units_l1': 161, 'n_units_l2': 577, 'dropout_rate': 0.554442559076572, 'lr': 0.0003716498240978966, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:16,484] Trial 109 finished with value: -184675872.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 316, 'n_units_l1': 232, 'n_units_l2': 554, 'dropout_rate': 0.5755952629965672, 'lr': 1.0327588625666383e-05, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:17,300] Trial 110 finished with value: -158638256.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 366, 'n_units_l1': 212, 'n_units_l2': 734, 'dropout_rate': 0.5828585884221396, 'lr': 0.0002548380539526205, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:18,199] Trial 111 finished with value: -15364179.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 409, 'n_units_l1': 188, 'n_units_l2': 445, 'dropout_rate': 0.5518019223659597, 'lr': 0.0006038468174881376, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:19,092] Trial 112 finished with value: -29307008.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 336, 'n_units_l1': 134, 'n_units_l2': 580, 'dropout_rate': 0.5611783886309937, 'lr': 0.0010136622339871758, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:20,039] Trial 113 finished with value: -45655656.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 389, 'n_units_l1': 245, 'n_units_l2': 534, 'dropout_rate': 0.5356885450967757, 'lr': 0.008093505167038832, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:20,949] Trial 114 finished with value: -14216230.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 360, 'n_units_l1': 165, 'n_units_l2': 633, 'dropout_rate': 0.5790374685542199, 'lr': 0.0008610057526142186, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:21,833] Trial 115 finished with value: -8920979.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 360, 'n_units_l1': 128, 'n_units_l2': 634, 'dropout_rate': 0.5936582881824783, 'lr': 0.0008300598297998408, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:22,762] Trial 116 finished with value: -14450515.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 358, 'n_units_l1': 161, 'n_units_l2': 637, 'dropout_rate': 0.598551691807327, 'lr': 0.0008041155720678365, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:23,824] Trial 117 finished with value: -8891791.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 354, 'n_units_l1': 172, 'n_units_l2': 631, 'dropout_rate': 0.5992070853019553, 'lr': 0.0008965955167287368, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:24,771] Trial 118 finished with value: -19456540.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 292, 'n_units_l1': 200, 'n_units_l2': 604, 'dropout_rate': 0.5800505862271362, 'lr': 0.0008908953545219419, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:25,636] Trial 119 finished with value: -19087674.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 421, 'n_units_l1': 134, 'n_units_l2': 630, 'dropout_rate': 0.58948591707639, 'lr': 0.00041461340261516813, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:26,485] Trial 120 finished with value: -14435699.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 325, 'n_units_l1': 166, 'n_units_l2': 661, 'dropout_rate': 0.5987241613297605, 'lr': 0.0011270764529584447, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:27,342] Trial 121 finished with value: -8206397.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 330, 'n_units_l1': 169, 'n_units_l2': 653, 'dropout_rate': 0.5915904681710207, 'lr': 0.0010554459127300577, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:28,109] Trial 122 finished with value: -22437992.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 230, 'n_units_l1': 129, 'n_units_l2': 616, 'dropout_rate': 0.589921148450281, 'lr': 0.0009751894327495297, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:28,946] Trial 123 finished with value: -41053304.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 347, 'n_units_l1': 178, 'n_units_l2': 593, 'dropout_rate': 0.5740494050681453, 'lr': 0.006407138799480992, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:29,773] Trial 124 finished with value: -22745188.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 319, 'n_units_l1': 229, 'n_units_l2': 674, 'dropout_rate': 0.5795913157640162, 'lr': 0.0015305109382463145, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:30,601] Trial 125 finished with value: -22585832.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 371, 'n_units_l1': 256, 'n_units_l2': 564, 'dropout_rate': 0.5939329705495376, 'lr': 0.0007910185014746873, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:31,517] Trial 126 finished with value: -15705138.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 278, 'n_units_l1': 200, 'n_units_l2': 644, 'dropout_rate': 0.5839744678389678, 'lr': 0.0005388489199666827, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:32,337] Trial 127 finished with value: -43365836.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 349, 'n_units_l1': 149, 'n_units_l2': 709, 'dropout_rate': 0.5631685826710469, 'lr': 0.0010909871262379204, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:32,849] Trial 128 finished with value: -5647923712.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 439, 'n_units_l1': 170, 'dropout_rate': 0.5727377642365608, 'lr': 0.001858273711037141, 'optimizer': 'SGD'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:33,686] Trial 129 finished with value: -50322368.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 413, 'n_units_l1': 618, 'n_units_l2': 625, 'dropout_rate': 0.5996743954583202, 'lr': 0.005077929110359531, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:34,549] Trial 130 finished with value: -52659584.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 362, 'n_units_l1': 211, 'n_units_l2': 581, 'n_units_l3': 278, 'dropout_rate': 0.5890957676720288, 'lr': 0.0025020455139635983, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:35,312] Trial 131 finished with value: -37197660.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 332, 'n_units_l1': 165, 'n_units_l2': 654, 'dropout_rate': 0.596581540585267, 'lr': 0.0010922718252628712, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:36,174] Trial 132 finished with value: -45784896.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 321, 'n_units_l1': 792, 'n_units_l2': 674, 'dropout_rate': 0.5775635394074933, 'lr': 0.0032846719183177863, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:36,947] Trial 133 finished with value: -30511620.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 243, 'n_units_l1': 182, 'n_units_l2': 655, 'dropout_rate': 0.38873706180639106, 'lr': 0.001261448982945247, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:37,734] Trial 134 finished with value: -12784806.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 294, 'n_units_l1': 148, 'n_units_l2': 609, 'dropout_rate': 0.5812108920295878, 'lr': 0.0005990704275260021, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:38,560] Trial 135 finished with value: -16367390.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 295, 'n_units_l1': 828, 'n_units_l2': 603, 'dropout_rate': 0.5442401257365355, 'lr': 0.0006249565940426, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:39,296] Trial 136 finished with value: -14140701.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 343, 'n_units_l1': 148, 'n_units_l2': 692, 'dropout_rate': 0.5671536118342636, 'lr': 0.0007461696140126322, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:40,127] Trial 137 finished with value: -25708372.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 339, 'n_units_l1': 131, 'n_units_l2': 695, 'n_units_l3': 404, 'dropout_rate': 0.5664934731845751, 'lr': 0.0004714687002056654, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:40,848] Trial 138 finished with value: -21716016.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 273, 'n_units_l1': 183, 'n_units_l2': 557, 'dropout_rate': 0.5850557352120567, 'lr': 0.000603738877999117, 'optimizer': 'RMSprop'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:41,719] Trial 139 finished with value: -26937734.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 540, 'n_units_l1': 147, 'n_units_l2': 613, 'dropout_rate': 0.3757329399052794, 'lr': 0.000506745423670188, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:42,590] Trial 140 finished with value: -659089088.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 376, 'n_units_l1': 197, 'n_units_l2': 756, 'n_units_l3': 374, 'dropout_rate': 0.5489796836771035, 'lr': 1.805370619570954e-05, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:43,336] Trial 141 finished with value: -30054994.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 311, 'n_units_l1': 154, 'n_units_l2': 633, 'dropout_rate': 0.5708630086759804, 'lr': 0.0008739053855926273, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:44,111] Trial 142 finished with value: -17117024.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 358, 'n_units_l1': 128, 'n_units_l2': 587, 'dropout_rate': 0.5796861003813865, 'lr': 0.0007498864166527802, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:44,912] Trial 143 finished with value: -17412300.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 380, 'n_units_l1': 214, 'n_units_l2': 679, 'dropout_rate': 0.5611567363444188, 'lr': 0.0007302123091694359, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:45,691] Trial 144 finished with value: -33971640.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 402, 'n_units_l1': 165, 'n_units_l2': 640, 'dropout_rate': 0.40166085626729814, 'lr': 0.0008592315389040006, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:46,474] Trial 145 finished with value: -35771460.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 475, 'n_units_l1': 188, 'n_units_l2': 718, 'dropout_rate': 0.5894711917170812, 'lr': 0.00042978702699537045, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:47,114] Trial 146 finished with value: -5582419968.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 344, 'n_units_l1': 151, 'n_units_l2': 570, 'dropout_rate': 0.5565546499080768, 'lr': 0.0006597239814063722, 'optimizer': 'SGD'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:48,008] Trial 147 finished with value: -39900192.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 420, 'n_units_l1': 704, 'n_units_l2': 614, 'n_units_l3': 298, 'dropout_rate': 0.35960705374601454, 'lr': 0.0005498584929071839, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:48,774] Trial 148 finished with value: -15076857.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 506, 'n_units_l1': 229, 'n_units_l2': 689, 'dropout_rate': 0.569522084728928, 'lr': 0.0009776919666601301, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:49,541] Trial 149 finished with value: -17585150.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 312, 'n_units_l1': 172, 'n_units_l2': 657, 'dropout_rate': 0.5782719605460097, 'lr': 0.0007970725695469771, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:50,448] Trial 150 finished with value: -37840264.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 660, 'n_units_l1': 204, 'n_units_l2': 601, 'n_units_l3': 819, 'dropout_rate': 0.585237758947836, 'lr': 0.0003896304017756031, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:51,217] Trial 151 finished with value: -24609318.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 328, 'n_units_l1': 166, 'n_units_l2': 668, 'dropout_rate': 0.5927636080025802, 'lr': 0.0014571484395646261, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:51,964] Trial 152 finished with value: -12852018.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 291, 'n_units_l1': 149, 'n_units_l2': 623, 'dropout_rate': 0.5747154570396635, 'lr': 0.001224253023804087, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:52,674] Trial 153 finished with value: -14034162.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 246, 'n_units_l1': 142, 'n_units_l2': 622, 'dropout_rate': 0.5627083960975398, 'lr': 0.0007094873623815706, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:53,413] Trial 154 finished with value: -13456224.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 183, 'n_units_l1': 145, 'n_units_l2': 594, 'dropout_rate': 0.5528090479331407, 'lr': 0.0006656983402830259, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:54,134] Trial 155 finished with value: -12057005.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 178, 'n_units_l1': 142, 'n_units_l2': 550, 'dropout_rate': 0.5395361578677352, 'lr': 0.0006711649972168725, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:54,891] Trial 156 finished with value: -21055390.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 188, 'n_units_l1': 185, 'n_units_l2': 555, 'dropout_rate': 0.5536851486617735, 'lr': 0.0006209420919244007, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:55,714] Trial 157 finished with value: -18750028.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 158, 'n_units_l1': 145, 'n_units_l2': 580, 'dropout_rate': 0.53790974046494, 'lr': 0.0005299817603534421, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:56,645] Trial 158 finished with value: -28149366.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 128, 'n_units_l1': 577, 'n_units_l2': 506, 'n_units_l3': 222, 'dropout_rate': 0.5233110210874495, 'lr': 0.008537380713244044, 'optimizer': 'AdamW'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:57,374] Trial 159 finished with value: -292503200.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 186, 'n_units_l1': 196, 'n_units_l2': 545, 'dropout_rate': 0.5323844906555022, 'lr': 0.004070179462542789, 'optimizer': 'RMSprop'}. Best is trial 42 with value: -6726183.5.\n",
      "[I 2024-09-25 14:37:58,211] Trial 160 finished with value: -4710201.5 and parameters: {'hidden_layers': 3, 'n_units_l0': 232, 'n_units_l1': 128, 'n_units_l2': 601, 'dropout_rate': 0.5457226151986218, 'lr': 0.0004612219100824549, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:37:58,995] Trial 161 finished with value: -11461727.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 163, 'n_units_l1': 129, 'n_units_l2': 593, 'dropout_rate': 0.5438862954102306, 'lr': 0.0005688449896082611, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:37:59,719] Trial 162 finished with value: -10723484.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 149, 'n_units_l1': 129, 'n_units_l2': 596, 'dropout_rate': 0.5455196113198081, 'lr': 0.0004630975821071836, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:00,534] Trial 163 finished with value: -14879823.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 168, 'n_units_l1': 132, 'n_units_l2': 593, 'dropout_rate': 0.5437507930146958, 'lr': 0.0004589099198404977, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:01,340] Trial 164 finished with value: -13281996.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 194, 'n_units_l1': 129, 'n_units_l2': 575, 'dropout_rate': 0.5504844225456659, 'lr': 0.0003516141097323733, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:02,123] Trial 165 finished with value: -31909790.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 200, 'n_units_l1': 128, 'n_units_l2': 574, 'dropout_rate': 0.3691884969894039, 'lr': 0.00033371384650523675, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:02,874] Trial 166 finished with value: -26268532.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 154, 'n_units_l1': 129, 'n_units_l2': 535, 'dropout_rate': 0.5406334023614068, 'lr': 0.00038687894989695277, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:03,748] Trial 167 finished with value: -821157568.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 229, 'n_units_l1': 748, 'n_units_l2': 560, 'dropout_rate': 0.513205205043765, 'lr': 0.00026755396106383807, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:04,596] Trial 168 finished with value: -26134862.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 204, 'n_units_l1': 157, 'n_units_l2': 604, 'dropout_rate': 0.38341521522427735, 'lr': 0.0004894472980137946, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:05,420] Trial 169 finished with value: -13319693.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 141, 'n_units_l1': 176, 'n_units_l2': 575, 'dropout_rate': 0.5232230369470182, 'lr': 0.0005598742262168661, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:06,274] Trial 170 finished with value: -95084568.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 571, 'n_units_l1': 526, 'n_units_l2': 545, 'dropout_rate': 0.5050646979663111, 'lr': 0.00034445639297742874, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:07,092] Trial 171 finished with value: -17641344.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 172, 'n_units_l1': 180, 'n_units_l2': 575, 'dropout_rate': 0.5467995555463461, 'lr': 0.0005728583048307032, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:07,910] Trial 172 finished with value: -242808784.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 144, 'n_units_l1': 889, 'n_units_l2': 614, 'dropout_rate': 0.5268443513031729, 'lr': 0.000434547641480666, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:08,744] Trial 173 finished with value: -18391812.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 215, 'n_units_l1': 160, 'n_units_l2': 522, 'dropout_rate': 0.55876165058467, 'lr': 0.0005423714187970259, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:09,474] Trial 174 finished with value: -30660122.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 151, 'n_units_l1': 180, 'n_units_l2': 589, 'dropout_rate': 0.5348671659904013, 'lr': 0.0004368648950880626, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:10,207] Trial 175 finished with value: -9177067.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 143, 'n_units_l1': 128, 'n_units_l2': 568, 'dropout_rate': 0.5488356917749564, 'lr': 0.0006064200136271381, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:10,849] Trial 176 finished with value: -4510311424.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 170, 'n_units_l1': 146, 'n_units_l2': 627, 'dropout_rate': 0.39230038878584617, 'lr': 0.00030917369266474104, 'optimizer': 'SGD'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:11,587] Trial 177 finished with value: -7737329.5 and parameters: {'hidden_layers': 3, 'n_units_l0': 202, 'n_units_l1': 131, 'n_units_l2': 553, 'dropout_rate': 0.5526770495755478, 'lr': 0.0006517187752189913, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:12,342] Trial 178 finished with value: -10485052.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 234, 'n_units_l1': 155, 'n_units_l2': 553, 'dropout_rate': 0.5720063984682145, 'lr': 0.0009505993853501562, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:13,094] Trial 179 finished with value: -13983554.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 241, 'n_units_l1': 156, 'n_units_l2': 510, 'dropout_rate': 0.5744431040437638, 'lr': 0.0009435039671124237, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:13,848] Trial 180 finished with value: -31039772.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 258, 'n_units_l1': 145, 'n_units_l2': 554, 'dropout_rate': 0.570250145991856, 'lr': 0.001112331014762127, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:14,674] Trial 181 finished with value: -19242694.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 623, 'n_units_l1': 845, 'n_units_l2': 545, 'dropout_rate': 0.5569895703968764, 'lr': 0.0006541922045964794, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:15,448] Trial 182 finished with value: -18790284.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 279, 'n_units_l1': 158, 'n_units_l2': 606, 'dropout_rate': 0.5448219012668573, 'lr': 0.0007733882373151075, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:16,173] Trial 183 finished with value: -16338190.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 210, 'n_units_l1': 168, 'n_units_l2': 642, 'dropout_rate': 0.5846025042318532, 'lr': 0.0009358724406147527, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:16,918] Trial 184 finished with value: -6801489.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 229, 'n_units_l1': 129, 'n_units_l2': 529, 'dropout_rate': 0.5990598073225526, 'lr': 0.000663980450361615, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:17,646] Trial 185 finished with value: -8904162.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 233, 'n_units_l1': 142, 'n_units_l2': 475, 'dropout_rate': 0.5959046100721254, 'lr': 0.0006356100286731817, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:18,367] Trial 186 finished with value: -5468653.5 and parameters: {'hidden_layers': 3, 'n_units_l0': 228, 'n_units_l1': 131, 'n_units_l2': 492, 'dropout_rate': 0.593465809675736, 'lr': 0.0006074338490547735, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:19,102] Trial 187 finished with value: -7581456.5 and parameters: {'hidden_layers': 3, 'n_units_l0': 219, 'n_units_l1': 136, 'n_units_l2': 492, 'dropout_rate': 0.592103391630938, 'lr': 0.0007215447732947086, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:19,849] Trial 188 finished with value: -11389766.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 233, 'n_units_l1': 135, 'n_units_l2': 453, 'dropout_rate': 0.5983661416451114, 'lr': 0.0007959123418091414, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:20,599] Trial 189 finished with value: -10926007.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 226, 'n_units_l1': 132, 'n_units_l2': 479, 'dropout_rate': 0.5991588474205018, 'lr': 0.0008351298324698344, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:21,392] Trial 190 finished with value: -17277074.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 218, 'n_units_l1': 129, 'n_units_l2': 464, 'dropout_rate': 0.5973702355066386, 'lr': 0.0008309270480621866, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:22,170] Trial 191 finished with value: -19833684.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 234, 'n_units_l1': 131, 'n_units_l2': 447, 'dropout_rate': 0.5866808680493872, 'lr': 0.000737747158890983, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:22,874] Trial 192 finished with value: -16254966.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 255, 'n_units_l1': 160, 'n_units_l2': 404, 'dropout_rate': 0.595343294891876, 'lr': 0.0008654182764875001, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:23,595] Trial 193 finished with value: -6527355.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 223, 'n_units_l1': 133, 'n_units_l2': 480, 'dropout_rate': 0.5921827383590618, 'lr': 0.0010043394608788756, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:24,349] Trial 194 finished with value: -19061756.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 232, 'n_units_l1': 128, 'n_units_l2': 478, 'dropout_rate': 0.5916423049256604, 'lr': 0.0010305746205403702, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:25,103] Trial 195 finished with value: -11828402.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 204, 'n_units_l1': 146, 'n_units_l2': 476, 'dropout_rate': 0.5970796916185, 'lr': 0.000743466431019709, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:25,864] Trial 196 finished with value: -15389412.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 228, 'n_units_l1': 144, 'n_units_l2': 485, 'dropout_rate': 0.5990295711023088, 'lr': 0.001054211398732447, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:26,598] Trial 197 finished with value: -10050067.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 254, 'n_units_l1': 168, 'n_units_l2': 459, 'dropout_rate': 0.5899197940697862, 'lr': 0.0008552674065751886, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:27,385] Trial 198 finished with value: -7622708.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 261, 'n_units_l1': 169, 'n_units_l2': 427, 'dropout_rate': 0.5998631479968187, 'lr': 0.0008903641946839175, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n",
      "[I 2024-09-25 14:38:28,122] Trial 199 finished with value: -16825096.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 261, 'n_units_l1': 173, 'n_units_l2': 498, 'dropout_rate': 0.5898812816236081, 'lr': 0.001250507278717581, 'optimizer': 'AdamW'}. Best is trial 160 with value: -4710201.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization completed.\n",
      "Best Trial:\n",
      "  Value (MSE): -4710201.5\n",
      "  Params:\n",
      "    hidden_layers: 3\n",
      "    n_units_l0: 232\n",
      "    n_units_l1: 128\n",
      "    n_units_l2: 601\n",
      "    dropout_rate: 0.5457226151986218\n",
      "    lr: 0.0004612219100824549\n",
      "    optimizer: AdamW\n",
      "Starting training of the best model...\n",
      "Epoch 1/500 - Training Loss: 0.2793 - Validation Loss: 0.2193\n",
      "Epoch 2/500 - Training Loss: 0.1962 - Validation Loss: 0.1443\n",
      "Epoch 3/500 - Training Loss: 0.1057 - Validation Loss: 0.0559\n",
      "Epoch 4/500 - Training Loss: 0.0840 - Validation Loss: 0.0355\n",
      "Epoch 5/500 - Training Loss: 0.0672 - Validation Loss: 0.0406\n",
      "Epoch 6/500 - Training Loss: 0.0530 - Validation Loss: 0.0560\n",
      "Epoch 7/500 - Training Loss: 0.0505 - Validation Loss: 0.0499\n",
      "Epoch 8/500 - Training Loss: 0.0439 - Validation Loss: 0.0356\n",
      "Epoch 9/500 - Training Loss: 0.0407 - Validation Loss: 0.0348\n",
      "Epoch 10/500 - Training Loss: 0.0381 - Validation Loss: 0.0460\n",
      "Epoch 11/500 - Training Loss: 0.0368 - Validation Loss: 0.0483\n",
      "Epoch 12/500 - Training Loss: 0.0333 - Validation Loss: 0.0433\n",
      "Epoch 13/500 - Training Loss: 0.0343 - Validation Loss: 0.0440\n",
      "Epoch 14/500 - Training Loss: 0.0335 - Validation Loss: 0.0486\n",
      "Epoch 15/500 - Training Loss: 0.0315 - Validation Loss: 0.0474\n",
      "Epoch 16/500 - Training Loss: 0.0295 - Validation Loss: 0.0456\n",
      "Epoch 17/500 - Training Loss: 0.0307 - Validation Loss: 0.0461\n",
      "Epoch 18/500 - Training Loss: 0.0296 - Validation Loss: 0.0450\n",
      "Epoch 19/500 - Training Loss: 0.0282 - Validation Loss: 0.0431\n",
      "Epoch 20/500 - Training Loss: 0.0290 - Validation Loss: 0.0446\n",
      "Epoch 21/500 - Training Loss: 0.0273 - Validation Loss: 0.0468\n",
      "Epoch 22/500 - Training Loss: 0.0284 - Validation Loss: 0.0427\n",
      "Epoch 23/500 - Training Loss: 0.0263 - Validation Loss: 0.0407\n",
      "Epoch 24/500 - Training Loss: 0.0280 - Validation Loss: 0.0435\n",
      "Epoch 25/500 - Training Loss: 0.0252 - Validation Loss: 0.0406\n",
      "Epoch 26/500 - Training Loss: 0.0257 - Validation Loss: 0.0374\n",
      "Epoch 27/500 - Training Loss: 0.0245 - Validation Loss: 0.0374\n",
      "Epoch 28/500 - Training Loss: 0.0257 - Validation Loss: 0.0362\n",
      "Epoch 29/500 - Training Loss: 0.0267 - Validation Loss: 0.0374\n",
      "Epoch 30/500 - Training Loss: 0.0242 - Validation Loss: 0.0391\n",
      "Epoch 31/500 - Training Loss: 0.0240 - Validation Loss: 0.0380\n",
      "Epoch 32/500 - Training Loss: 0.0240 - Validation Loss: 0.0343\n",
      "Epoch 33/500 - Training Loss: 0.0224 - Validation Loss: 0.0325\n",
      "Epoch 34/500 - Training Loss: 0.0213 - Validation Loss: 0.0319\n",
      "Epoch 35/500 - Training Loss: 0.0210 - Validation Loss: 0.0319\n",
      "Epoch 36/500 - Training Loss: 0.0227 - Validation Loss: 0.0299\n",
      "Epoch 37/500 - Training Loss: 0.0223 - Validation Loss: 0.0321\n",
      "Epoch 38/500 - Training Loss: 0.0224 - Validation Loss: 0.0329\n",
      "Epoch 39/500 - Training Loss: 0.0196 - Validation Loss: 0.0312\n",
      "Epoch 40/500 - Training Loss: 0.0196 - Validation Loss: 0.0274\n",
      "Epoch 41/500 - Training Loss: 0.0205 - Validation Loss: 0.0262\n",
      "Epoch 42/500 - Training Loss: 0.0215 - Validation Loss: 0.0280\n",
      "Epoch 43/500 - Training Loss: 0.0202 - Validation Loss: 0.0278\n",
      "Epoch 44/500 - Training Loss: 0.0220 - Validation Loss: 0.0256\n",
      "Epoch 45/500 - Training Loss: 0.0195 - Validation Loss: 0.0245\n",
      "Epoch 46/500 - Training Loss: 0.0187 - Validation Loss: 0.0244\n",
      "Epoch 47/500 - Training Loss: 0.0187 - Validation Loss: 0.0227\n",
      "Epoch 48/500 - Training Loss: 0.0175 - Validation Loss: 0.0202\n",
      "Epoch 49/500 - Training Loss: 0.0182 - Validation Loss: 0.0210\n",
      "Epoch 50/500 - Training Loss: 0.0175 - Validation Loss: 0.0218\n",
      "Epoch 51/500 - Training Loss: 0.0168 - Validation Loss: 0.0218\n",
      "Epoch 52/500 - Training Loss: 0.0169 - Validation Loss: 0.0210\n",
      "Epoch 53/500 - Training Loss: 0.0166 - Validation Loss: 0.0199\n",
      "Epoch 54/500 - Training Loss: 0.0163 - Validation Loss: 0.0207\n",
      "Epoch 55/500 - Training Loss: 0.0166 - Validation Loss: 0.0200\n",
      "Epoch 56/500 - Training Loss: 0.0171 - Validation Loss: 0.0187\n",
      "Epoch 57/500 - Training Loss: 0.0151 - Validation Loss: 0.0175\n",
      "Epoch 58/500 - Training Loss: 0.0166 - Validation Loss: 0.0174\n",
      "Epoch 59/500 - Training Loss: 0.0159 - Validation Loss: 0.0179\n",
      "Epoch 60/500 - Training Loss: 0.0155 - Validation Loss: 0.0186\n",
      "Epoch 61/500 - Training Loss: 0.0167 - Validation Loss: 0.0184\n",
      "Epoch 62/500 - Training Loss: 0.0149 - Validation Loss: 0.0178\n",
      "Epoch 63/500 - Training Loss: 0.0158 - Validation Loss: 0.0170\n",
      "Epoch 64/500 - Training Loss: 0.0158 - Validation Loss: 0.0159\n",
      "Epoch 65/500 - Training Loss: 0.0153 - Validation Loss: 0.0157\n",
      "Epoch 66/500 - Training Loss: 0.0159 - Validation Loss: 0.0162\n",
      "Epoch 67/500 - Training Loss: 0.0146 - Validation Loss: 0.0156\n",
      "Epoch 68/500 - Training Loss: 0.0154 - Validation Loss: 0.0156\n",
      "Epoch 69/500 - Training Loss: 0.0165 - Validation Loss: 0.0172\n",
      "Epoch 70/500 - Training Loss: 0.0151 - Validation Loss: 0.0162\n",
      "Epoch 71/500 - Training Loss: 0.0144 - Validation Loss: 0.0146\n",
      "Epoch 72/500 - Training Loss: 0.0149 - Validation Loss: 0.0139\n",
      "Epoch 73/500 - Training Loss: 0.0149 - Validation Loss: 0.0141\n",
      "Epoch 74/500 - Training Loss: 0.0155 - Validation Loss: 0.0154\n",
      "Epoch 75/500 - Training Loss: 0.0158 - Validation Loss: 0.0162\n",
      "Epoch 76/500 - Training Loss: 0.0150 - Validation Loss: 0.0147\n",
      "Epoch 77/500 - Training Loss: 0.0133 - Validation Loss: 0.0135\n",
      "Epoch 78/500 - Training Loss: 0.0147 - Validation Loss: 0.0142\n",
      "Epoch 79/500 - Training Loss: 0.0140 - Validation Loss: 0.0146\n",
      "Epoch 80/500 - Training Loss: 0.0142 - Validation Loss: 0.0145\n",
      "Epoch 81/500 - Training Loss: 0.0142 - Validation Loss: 0.0140\n",
      "Epoch 82/500 - Training Loss: 0.0134 - Validation Loss: 0.0133\n",
      "Epoch 83/500 - Training Loss: 0.0138 - Validation Loss: 0.0131\n",
      "Epoch 84/500 - Training Loss: 0.0132 - Validation Loss: 0.0131\n",
      "Epoch 85/500 - Training Loss: 0.0130 - Validation Loss: 0.0129\n",
      "Epoch 86/500 - Training Loss: 0.0124 - Validation Loss: 0.0126\n",
      "Epoch 87/500 - Training Loss: 0.0146 - Validation Loss: 0.0132\n",
      "Epoch 88/500 - Training Loss: 0.0125 - Validation Loss: 0.0137\n",
      "Epoch 89/500 - Training Loss: 0.0131 - Validation Loss: 0.0135\n",
      "Epoch 90/500 - Training Loss: 0.0132 - Validation Loss: 0.0128\n",
      "Epoch 91/500 - Training Loss: 0.0134 - Validation Loss: 0.0119\n",
      "Epoch 92/500 - Training Loss: 0.0135 - Validation Loss: 0.0127\n",
      "Epoch 93/500 - Training Loss: 0.0130 - Validation Loss: 0.0132\n",
      "Epoch 94/500 - Training Loss: 0.0133 - Validation Loss: 0.0127\n",
      "Epoch 95/500 - Training Loss: 0.0124 - Validation Loss: 0.0121\n",
      "Epoch 96/500 - Training Loss: 0.0119 - Validation Loss: 0.0125\n",
      "Epoch 97/500 - Training Loss: 0.0136 - Validation Loss: 0.0130\n",
      "Epoch 98/500 - Training Loss: 0.0122 - Validation Loss: 0.0129\n",
      "Epoch 99/500 - Training Loss: 0.0125 - Validation Loss: 0.0123\n",
      "Epoch 100/500 - Training Loss: 0.0125 - Validation Loss: 0.0122\n",
      "Epoch 101/500 - Training Loss: 0.0121 - Validation Loss: 0.0119\n",
      "Epoch 102/500 - Training Loss: 0.0124 - Validation Loss: 0.0121\n",
      "Epoch 103/500 - Training Loss: 0.0115 - Validation Loss: 0.0126\n",
      "Epoch 104/500 - Training Loss: 0.0122 - Validation Loss: 0.0125\n",
      "Epoch 105/500 - Training Loss: 0.0115 - Validation Loss: 0.0122\n",
      "Epoch 106/500 - Training Loss: 0.0127 - Validation Loss: 0.0126\n",
      "Epoch 107/500 - Training Loss: 0.0116 - Validation Loss: 0.0120\n",
      "Epoch 108/500 - Training Loss: 0.0128 - Validation Loss: 0.0112\n",
      "Epoch 109/500 - Training Loss: 0.0114 - Validation Loss: 0.0113\n",
      "Epoch 110/500 - Training Loss: 0.0121 - Validation Loss: 0.0119\n",
      "Epoch 111/500 - Training Loss: 0.0107 - Validation Loss: 0.0120\n",
      "Epoch 112/500 - Training Loss: 0.0125 - Validation Loss: 0.0110\n",
      "Epoch 113/500 - Training Loss: 0.0120 - Validation Loss: 0.0102\n",
      "Epoch 114/500 - Training Loss: 0.0120 - Validation Loss: 0.0107\n",
      "Epoch 115/500 - Training Loss: 0.0118 - Validation Loss: 0.0114\n",
      "Epoch 116/500 - Training Loss: 0.0098 - Validation Loss: 0.0109\n",
      "Epoch 117/500 - Training Loss: 0.0128 - Validation Loss: 0.0111\n",
      "Epoch 118/500 - Training Loss: 0.0116 - Validation Loss: 0.0112\n",
      "Epoch 119/500 - Training Loss: 0.0113 - Validation Loss: 0.0105\n",
      "Epoch 120/500 - Training Loss: 0.0110 - Validation Loss: 0.0103\n",
      "Epoch 121/500 - Training Loss: 0.0102 - Validation Loss: 0.0105\n",
      "Epoch 122/500 - Training Loss: 0.0112 - Validation Loss: 0.0113\n",
      "Epoch 123/500 - Training Loss: 0.0098 - Validation Loss: 0.0115\n",
      "Epoch 124/500 - Training Loss: 0.0108 - Validation Loss: 0.0114\n",
      "Epoch 125/500 - Training Loss: 0.0117 - Validation Loss: 0.0109\n",
      "Epoch 126/500 - Training Loss: 0.0111 - Validation Loss: 0.0102\n",
      "Epoch 127/500 - Training Loss: 0.0114 - Validation Loss: 0.0101\n",
      "Epoch 128/500 - Training Loss: 0.0116 - Validation Loss: 0.0105\n",
      "Epoch 129/500 - Training Loss: 0.0116 - Validation Loss: 0.0109\n",
      "Epoch 130/500 - Training Loss: 0.0110 - Validation Loss: 0.0111\n",
      "Epoch 131/500 - Training Loss: 0.0111 - Validation Loss: 0.0104\n",
      "Epoch 132/500 - Training Loss: 0.0099 - Validation Loss: 0.0097\n",
      "Epoch 133/500 - Training Loss: 0.0113 - Validation Loss: 0.0099\n",
      "Epoch 134/500 - Training Loss: 0.0114 - Validation Loss: 0.0102\n",
      "Epoch 135/500 - Training Loss: 0.0106 - Validation Loss: 0.0104\n",
      "Epoch 136/500 - Training Loss: 0.0115 - Validation Loss: 0.0100\n",
      "Epoch 137/500 - Training Loss: 0.0109 - Validation Loss: 0.0094\n",
      "Epoch 138/500 - Training Loss: 0.0113 - Validation Loss: 0.0099\n",
      "Epoch 139/500 - Training Loss: 0.0103 - Validation Loss: 0.0099\n",
      "Epoch 140/500 - Training Loss: 0.0100 - Validation Loss: 0.0094\n",
      "Epoch 141/500 - Training Loss: 0.0094 - Validation Loss: 0.0097\n",
      "Epoch 142/500 - Training Loss: 0.0096 - Validation Loss: 0.0097\n",
      "Epoch 143/500 - Training Loss: 0.0102 - Validation Loss: 0.0093\n",
      "Epoch 144/500 - Training Loss: 0.0105 - Validation Loss: 0.0094\n",
      "Epoch 145/500 - Training Loss: 0.0114 - Validation Loss: 0.0094\n",
      "Epoch 146/500 - Training Loss: 0.0104 - Validation Loss: 0.0102\n",
      "Epoch 147/500 - Training Loss: 0.0117 - Validation Loss: 0.0104\n",
      "Epoch 148/500 - Training Loss: 0.0099 - Validation Loss: 0.0096\n",
      "Epoch 149/500 - Training Loss: 0.0098 - Validation Loss: 0.0091\n",
      "Epoch 150/500 - Training Loss: 0.0096 - Validation Loss: 0.0094\n",
      "Epoch 151/500 - Training Loss: 0.0091 - Validation Loss: 0.0094\n",
      "Epoch 152/500 - Training Loss: 0.0110 - Validation Loss: 0.0092\n",
      "Epoch 153/500 - Training Loss: 0.0092 - Validation Loss: 0.0095\n",
      "Epoch 154/500 - Training Loss: 0.0093 - Validation Loss: 0.0099\n",
      "Epoch 155/500 - Training Loss: 0.0101 - Validation Loss: 0.0101\n",
      "Epoch 156/500 - Training Loss: 0.0098 - Validation Loss: 0.0095\n",
      "Epoch 157/500 - Training Loss: 0.0086 - Validation Loss: 0.0086\n",
      "Epoch 158/500 - Training Loss: 0.0098 - Validation Loss: 0.0089\n",
      "Epoch 159/500 - Training Loss: 0.0093 - Validation Loss: 0.0097\n",
      "Epoch 160/500 - Training Loss: 0.0089 - Validation Loss: 0.0091\n",
      "Epoch 161/500 - Training Loss: 0.0106 - Validation Loss: 0.0091\n",
      "Epoch 162/500 - Training Loss: 0.0086 - Validation Loss: 0.0100\n",
      "Epoch 163/500 - Training Loss: 0.0102 - Validation Loss: 0.0103\n",
      "Epoch 164/500 - Training Loss: 0.0103 - Validation Loss: 0.0092\n",
      "Epoch 165/500 - Training Loss: 0.0101 - Validation Loss: 0.0085\n",
      "Epoch 166/500 - Training Loss: 0.0099 - Validation Loss: 0.0092\n",
      "Epoch 167/500 - Training Loss: 0.0091 - Validation Loss: 0.0101\n",
      "Epoch 168/500 - Training Loss: 0.0096 - Validation Loss: 0.0100\n",
      "Epoch 169/500 - Training Loss: 0.0099 - Validation Loss: 0.0092\n",
      "Epoch 170/500 - Training Loss: 0.0109 - Validation Loss: 0.0090\n",
      "Epoch 171/500 - Training Loss: 0.0089 - Validation Loss: 0.0098\n",
      "Epoch 172/500 - Training Loss: 0.0095 - Validation Loss: 0.0099\n",
      "Epoch 173/500 - Training Loss: 0.0095 - Validation Loss: 0.0089\n",
      "Epoch 174/500 - Training Loss: 0.0094 - Validation Loss: 0.0084\n",
      "Epoch 175/500 - Training Loss: 0.0098 - Validation Loss: 0.0090\n",
      "Epoch 176/500 - Training Loss: 0.0102 - Validation Loss: 0.0094\n",
      "Epoch 177/500 - Training Loss: 0.0093 - Validation Loss: 0.0089\n",
      "Epoch 178/500 - Training Loss: 0.0091 - Validation Loss: 0.0085\n",
      "Epoch 179/500 - Training Loss: 0.0094 - Validation Loss: 0.0087\n",
      "Epoch 180/500 - Training Loss: 0.0095 - Validation Loss: 0.0087\n",
      "Epoch 181/500 - Training Loss: 0.0108 - Validation Loss: 0.0087\n",
      "Epoch 182/500 - Training Loss: 0.0089 - Validation Loss: 0.0083\n",
      "Epoch 183/500 - Training Loss: 0.0095 - Validation Loss: 0.0084\n",
      "Epoch 184/500 - Training Loss: 0.0088 - Validation Loss: 0.0092\n",
      "Epoch 185/500 - Training Loss: 0.0085 - Validation Loss: 0.0089\n",
      "Epoch 186/500 - Training Loss: 0.0104 - Validation Loss: 0.0084\n",
      "Epoch 187/500 - Training Loss: 0.0100 - Validation Loss: 0.0087\n",
      "Epoch 188/500 - Training Loss: 0.0097 - Validation Loss: 0.0087\n",
      "Epoch 189/500 - Training Loss: 0.0095 - Validation Loss: 0.0089\n",
      "Epoch 190/500 - Training Loss: 0.0089 - Validation Loss: 0.0089\n",
      "Epoch 191/500 - Training Loss: 0.0089 - Validation Loss: 0.0090\n",
      "Epoch 192/500 - Training Loss: 0.0097 - Validation Loss: 0.0093\n",
      "Epoch 193/500 - Training Loss: 0.0085 - Validation Loss: 0.0084\n",
      "Epoch 194/500 - Training Loss: 0.0088 - Validation Loss: 0.0085\n",
      "Epoch 195/500 - Training Loss: 0.0094 - Validation Loss: 0.0087\n",
      "Epoch 196/500 - Training Loss: 0.0095 - Validation Loss: 0.0085\n",
      "Epoch 197/500 - Training Loss: 0.0078 - Validation Loss: 0.0084\n",
      "Epoch 198/500 - Training Loss: 0.0091 - Validation Loss: 0.0086\n",
      "Epoch 199/500 - Training Loss: 0.0090 - Validation Loss: 0.0081\n",
      "Epoch 200/500 - Training Loss: 0.0079 - Validation Loss: 0.0075\n",
      "Epoch 201/500 - Training Loss: 0.0094 - Validation Loss: 0.0074\n",
      "Epoch 202/500 - Training Loss: 0.0090 - Validation Loss: 0.0079\n",
      "Epoch 203/500 - Training Loss: 0.0094 - Validation Loss: 0.0088\n",
      "Epoch 204/500 - Training Loss: 0.0095 - Validation Loss: 0.0093\n",
      "Epoch 205/500 - Training Loss: 0.0089 - Validation Loss: 0.0085\n",
      "Epoch 206/500 - Training Loss: 0.0080 - Validation Loss: 0.0080\n",
      "Epoch 207/500 - Training Loss: 0.0085 - Validation Loss: 0.0082\n",
      "Epoch 208/500 - Training Loss: 0.0088 - Validation Loss: 0.0086\n",
      "Epoch 209/500 - Training Loss: 0.0089 - Validation Loss: 0.0077\n",
      "Epoch 210/500 - Training Loss: 0.0087 - Validation Loss: 0.0074\n",
      "Epoch 211/500 - Training Loss: 0.0083 - Validation Loss: 0.0078\n",
      "Epoch 212/500 - Training Loss: 0.0089 - Validation Loss: 0.0083\n",
      "Epoch 213/500 - Training Loss: 0.0090 - Validation Loss: 0.0086\n",
      "Epoch 214/500 - Training Loss: 0.0098 - Validation Loss: 0.0079\n",
      "Epoch 215/500 - Training Loss: 0.0081 - Validation Loss: 0.0078\n",
      "Epoch 216/500 - Training Loss: 0.0090 - Validation Loss: 0.0081\n",
      "Epoch 217/500 - Training Loss: 0.0093 - Validation Loss: 0.0082\n",
      "Epoch 218/500 - Training Loss: 0.0089 - Validation Loss: 0.0080\n",
      "Epoch 219/500 - Training Loss: 0.0092 - Validation Loss: 0.0078\n",
      "Epoch 220/500 - Training Loss: 0.0088 - Validation Loss: 0.0076\n",
      "Epoch 221/500 - Training Loss: 0.0089 - Validation Loss: 0.0080\n",
      "Epoch 222/500 - Training Loss: 0.0086 - Validation Loss: 0.0085\n",
      "Epoch 223/500 - Training Loss: 0.0083 - Validation Loss: 0.0083\n",
      "Epoch 224/500 - Training Loss: 0.0085 - Validation Loss: 0.0077\n",
      "Epoch 225/500 - Training Loss: 0.0091 - Validation Loss: 0.0075\n",
      "Epoch 226/500 - Training Loss: 0.0095 - Validation Loss: 0.0082\n",
      "Epoch 227/500 - Training Loss: 0.0088 - Validation Loss: 0.0081\n",
      "Epoch 228/500 - Training Loss: 0.0075 - Validation Loss: 0.0074\n",
      "Epoch 229/500 - Training Loss: 0.0083 - Validation Loss: 0.0068\n",
      "Epoch 230/500 - Training Loss: 0.0085 - Validation Loss: 0.0070\n",
      "Epoch 231/500 - Training Loss: 0.0081 - Validation Loss: 0.0076\n",
      "Epoch 232/500 - Training Loss: 0.0088 - Validation Loss: 0.0084\n",
      "Epoch 233/500 - Training Loss: 0.0085 - Validation Loss: 0.0077\n",
      "Epoch 234/500 - Training Loss: 0.0072 - Validation Loss: 0.0069\n",
      "Epoch 235/500 - Training Loss: 0.0079 - Validation Loss: 0.0069\n",
      "Epoch 236/500 - Training Loss: 0.0084 - Validation Loss: 0.0076\n",
      "Epoch 237/500 - Training Loss: 0.0079 - Validation Loss: 0.0077\n",
      "Epoch 238/500 - Training Loss: 0.0095 - Validation Loss: 0.0077\n",
      "Epoch 239/500 - Training Loss: 0.0081 - Validation Loss: 0.0072\n",
      "Epoch 240/500 - Training Loss: 0.0081 - Validation Loss: 0.0073\n",
      "Epoch 241/500 - Training Loss: 0.0088 - Validation Loss: 0.0078\n",
      "Epoch 242/500 - Training Loss: 0.0082 - Validation Loss: 0.0076\n",
      "Epoch 243/500 - Training Loss: 0.0079 - Validation Loss: 0.0070\n",
      "Epoch 244/500 - Training Loss: 0.0081 - Validation Loss: 0.0067\n",
      "Epoch 245/500 - Training Loss: 0.0085 - Validation Loss: 0.0065\n",
      "Epoch 246/500 - Training Loss: 0.0086 - Validation Loss: 0.0071\n",
      "Epoch 247/500 - Training Loss: 0.0084 - Validation Loss: 0.0073\n",
      "Epoch 248/500 - Training Loss: 0.0077 - Validation Loss: 0.0067\n",
      "Epoch 249/500 - Training Loss: 0.0073 - Validation Loss: 0.0067\n",
      "Epoch 250/500 - Training Loss: 0.0077 - Validation Loss: 0.0075\n",
      "Epoch 251/500 - Training Loss: 0.0077 - Validation Loss: 0.0078\n",
      "Epoch 252/500 - Training Loss: 0.0080 - Validation Loss: 0.0074\n",
      "Epoch 253/500 - Training Loss: 0.0080 - Validation Loss: 0.0070\n",
      "Epoch 254/500 - Training Loss: 0.0081 - Validation Loss: 0.0070\n",
      "Epoch 255/500 - Training Loss: 0.0081 - Validation Loss: 0.0079\n",
      "Epoch 256/500 - Training Loss: 0.0075 - Validation Loss: 0.0085\n",
      "Epoch 257/500 - Training Loss: 0.0084 - Validation Loss: 0.0076\n",
      "Epoch 258/500 - Training Loss: 0.0078 - Validation Loss: 0.0068\n",
      "Epoch 259/500 - Training Loss: 0.0072 - Validation Loss: 0.0067\n",
      "Epoch 260/500 - Training Loss: 0.0074 - Validation Loss: 0.0072\n",
      "Epoch 261/500 - Training Loss: 0.0080 - Validation Loss: 0.0076\n",
      "Epoch 262/500 - Training Loss: 0.0079 - Validation Loss: 0.0072\n",
      "Epoch 263/500 - Training Loss: 0.0088 - Validation Loss: 0.0069\n",
      "Epoch 264/500 - Training Loss: 0.0069 - Validation Loss: 0.0067\n",
      "Epoch 265/500 - Training Loss: 0.0080 - Validation Loss: 0.0062\n",
      "Epoch 266/500 - Training Loss: 0.0077 - Validation Loss: 0.0069\n",
      "Epoch 267/500 - Training Loss: 0.0079 - Validation Loss: 0.0074\n",
      "Epoch 268/500 - Training Loss: 0.0077 - Validation Loss: 0.0075\n",
      "Epoch 269/500 - Training Loss: 0.0085 - Validation Loss: 0.0073\n",
      "Epoch 270/500 - Training Loss: 0.0074 - Validation Loss: 0.0072\n",
      "Epoch 271/500 - Training Loss: 0.0073 - Validation Loss: 0.0072\n",
      "Epoch 272/500 - Training Loss: 0.0087 - Validation Loss: 0.0066\n",
      "Epoch 273/500 - Training Loss: 0.0078 - Validation Loss: 0.0066\n",
      "Epoch 274/500 - Training Loss: 0.0071 - Validation Loss: 0.0071\n",
      "Epoch 275/500 - Training Loss: 0.0072 - Validation Loss: 0.0071\n",
      "Epoch 276/500 - Training Loss: 0.0075 - Validation Loss: 0.0067\n",
      "Epoch 277/500 - Training Loss: 0.0078 - Validation Loss: 0.0071\n",
      "Epoch 278/500 - Training Loss: 0.0068 - Validation Loss: 0.0071\n",
      "Epoch 279/500 - Training Loss: 0.0068 - Validation Loss: 0.0069\n",
      "Epoch 280/500 - Training Loss: 0.0073 - Validation Loss: 0.0067\n",
      "Epoch 281/500 - Training Loss: 0.0079 - Validation Loss: 0.0070\n",
      "Epoch 282/500 - Training Loss: 0.0075 - Validation Loss: 0.0070\n",
      "Epoch 283/500 - Training Loss: 0.0078 - Validation Loss: 0.0066\n",
      "Epoch 284/500 - Training Loss: 0.0079 - Validation Loss: 0.0068\n",
      "Epoch 285/500 - Training Loss: 0.0078 - Validation Loss: 0.0069\n",
      "Epoch 286/500 - Training Loss: 0.0078 - Validation Loss: 0.0069\n",
      "Epoch 287/500 - Training Loss: 0.0071 - Validation Loss: 0.0066\n",
      "Epoch 288/500 - Training Loss: 0.0067 - Validation Loss: 0.0065\n",
      "Epoch 289/500 - Training Loss: 0.0066 - Validation Loss: 0.0063\n",
      "Epoch 290/500 - Training Loss: 0.0076 - Validation Loss: 0.0065\n",
      "Epoch 291/500 - Training Loss: 0.0068 - Validation Loss: 0.0066\n",
      "Epoch 292/500 - Training Loss: 0.0077 - Validation Loss: 0.0066\n",
      "Epoch 293/500 - Training Loss: 0.0074 - Validation Loss: 0.0068\n",
      "Epoch 294/500 - Training Loss: 0.0071 - Validation Loss: 0.0071\n",
      "Epoch 295/500 - Training Loss: 0.0064 - Validation Loss: 0.0068\n",
      "Epoch 296/500 - Training Loss: 0.0084 - Validation Loss: 0.0065\n",
      "Epoch 297/500 - Training Loss: 0.0074 - Validation Loss: 0.0061\n",
      "Epoch 298/500 - Training Loss: 0.0072 - Validation Loss: 0.0061\n",
      "Epoch 299/500 - Training Loss: 0.0063 - Validation Loss: 0.0066\n",
      "Epoch 300/500 - Training Loss: 0.0068 - Validation Loss: 0.0062\n",
      "Epoch 301/500 - Training Loss: 0.0076 - Validation Loss: 0.0064\n",
      "Epoch 302/500 - Training Loss: 0.0065 - Validation Loss: 0.0074\n",
      "Epoch 303/500 - Training Loss: 0.0078 - Validation Loss: 0.0072\n",
      "Epoch 304/500 - Training Loss: 0.0072 - Validation Loss: 0.0066\n",
      "Epoch 305/500 - Training Loss: 0.0072 - Validation Loss: 0.0065\n",
      "Epoch 306/500 - Training Loss: 0.0071 - Validation Loss: 0.0059\n",
      "Epoch 307/500 - Training Loss: 0.0064 - Validation Loss: 0.0062\n",
      "Epoch 308/500 - Training Loss: 0.0075 - Validation Loss: 0.0068\n",
      "Epoch 309/500 - Training Loss: 0.0066 - Validation Loss: 0.0064\n",
      "Epoch 310/500 - Training Loss: 0.0073 - Validation Loss: 0.0064\n",
      "Epoch 311/500 - Training Loss: 0.0067 - Validation Loss: 0.0067\n",
      "Epoch 312/500 - Training Loss: 0.0072 - Validation Loss: 0.0061\n",
      "Epoch 313/500 - Training Loss: 0.0067 - Validation Loss: 0.0059\n",
      "Epoch 314/500 - Training Loss: 0.0066 - Validation Loss: 0.0064\n",
      "Epoch 315/500 - Training Loss: 0.0065 - Validation Loss: 0.0069\n",
      "Epoch 316/500 - Training Loss: 0.0065 - Validation Loss: 0.0069\n",
      "Epoch 317/500 - Training Loss: 0.0061 - Validation Loss: 0.0064\n",
      "Epoch 318/500 - Training Loss: 0.0068 - Validation Loss: 0.0064\n",
      "Epoch 319/500 - Training Loss: 0.0072 - Validation Loss: 0.0063\n",
      "Epoch 320/500 - Training Loss: 0.0071 - Validation Loss: 0.0064\n",
      "Epoch 321/500 - Training Loss: 0.0073 - Validation Loss: 0.0063\n",
      "Epoch 322/500 - Training Loss: 0.0067 - Validation Loss: 0.0063\n",
      "Epoch 323/500 - Training Loss: 0.0076 - Validation Loss: 0.0066\n",
      "Epoch 324/500 - Training Loss: 0.0071 - Validation Loss: 0.0068\n",
      "Epoch 325/500 - Training Loss: 0.0069 - Validation Loss: 0.0068\n",
      "Epoch 326/500 - Training Loss: 0.0066 - Validation Loss: 0.0066\n",
      "Epoch 327/500 - Training Loss: 0.0067 - Validation Loss: 0.0069\n",
      "Epoch 328/500 - Training Loss: 0.0066 - Validation Loss: 0.0068\n",
      "Epoch 329/500 - Training Loss: 0.0067 - Validation Loss: 0.0066\n",
      "Epoch 330/500 - Training Loss: 0.0066 - Validation Loss: 0.0064\n",
      "Epoch 331/500 - Training Loss: 0.0078 - Validation Loss: 0.0064\n",
      "Epoch 332/500 - Training Loss: 0.0071 - Validation Loss: 0.0060\n",
      "Epoch 333/500 - Training Loss: 0.0072 - Validation Loss: 0.0060\n",
      "Epoch 334/500 - Training Loss: 0.0064 - Validation Loss: 0.0061\n",
      "Epoch 335/500 - Training Loss: 0.0070 - Validation Loss: 0.0065\n",
      "Epoch 336/500 - Training Loss: 0.0067 - Validation Loss: 0.0066\n",
      "Epoch 337/500 - Training Loss: 0.0078 - Validation Loss: 0.0063\n",
      "Epoch 338/500 - Training Loss: 0.0067 - Validation Loss: 0.0061\n",
      "Epoch 339/500 - Training Loss: 0.0070 - Validation Loss: 0.0062\n",
      "Epoch 340/500 - Training Loss: 0.0070 - Validation Loss: 0.0067\n",
      "Epoch 341/500 - Training Loss: 0.0062 - Validation Loss: 0.0065\n",
      "Epoch 342/500 - Training Loss: 0.0069 - Validation Loss: 0.0060\n",
      "Epoch 343/500 - Training Loss: 0.0068 - Validation Loss: 0.0065\n",
      "Epoch 344/500 - Training Loss: 0.0061 - Validation Loss: 0.0068\n",
      "Epoch 345/500 - Training Loss: 0.0060 - Validation Loss: 0.0066\n",
      "Epoch 346/500 - Training Loss: 0.0059 - Validation Loss: 0.0067\n",
      "Epoch 347/500 - Training Loss: 0.0063 - Validation Loss: 0.0065\n",
      "Epoch 348/500 - Training Loss: 0.0065 - Validation Loss: 0.0060\n",
      "Epoch 349/500 - Training Loss: 0.0063 - Validation Loss: 0.0058\n",
      "Epoch 350/500 - Training Loss: 0.0068 - Validation Loss: 0.0063\n",
      "Epoch 351/500 - Training Loss: 0.0072 - Validation Loss: 0.0063\n",
      "Epoch 352/500 - Training Loss: 0.0073 - Validation Loss: 0.0060\n",
      "Epoch 353/500 - Training Loss: 0.0064 - Validation Loss: 0.0059\n",
      "Epoch 354/500 - Training Loss: 0.0066 - Validation Loss: 0.0055\n",
      "Epoch 355/500 - Training Loss: 0.0076 - Validation Loss: 0.0056\n",
      "Epoch 356/500 - Training Loss: 0.0064 - Validation Loss: 0.0062\n",
      "Epoch 357/500 - Training Loss: 0.0058 - Validation Loss: 0.0064\n",
      "Epoch 358/500 - Training Loss: 0.0071 - Validation Loss: 0.0065\n",
      "Epoch 359/500 - Training Loss: 0.0064 - Validation Loss: 0.0059\n",
      "Epoch 360/500 - Training Loss: 0.0067 - Validation Loss: 0.0057\n",
      "Epoch 361/500 - Training Loss: 0.0061 - Validation Loss: 0.0057\n",
      "Epoch 362/500 - Training Loss: 0.0064 - Validation Loss: 0.0058\n",
      "Epoch 363/500 - Training Loss: 0.0070 - Validation Loss: 0.0059\n",
      "Epoch 364/500 - Training Loss: 0.0068 - Validation Loss: 0.0061\n",
      "Epoch 365/500 - Training Loss: 0.0060 - Validation Loss: 0.0060\n",
      "Epoch 366/500 - Training Loss: 0.0063 - Validation Loss: 0.0062\n",
      "Epoch 367/500 - Training Loss: 0.0062 - Validation Loss: 0.0065\n",
      "Epoch 368/500 - Training Loss: 0.0061 - Validation Loss: 0.0063\n",
      "Epoch 369/500 - Training Loss: 0.0063 - Validation Loss: 0.0063\n",
      "Epoch 370/500 - Training Loss: 0.0060 - Validation Loss: 0.0058\n",
      "Epoch 371/500 - Training Loss: 0.0066 - Validation Loss: 0.0053\n",
      "Epoch 372/500 - Training Loss: 0.0064 - Validation Loss: 0.0056\n",
      "Epoch 373/500 - Training Loss: 0.0067 - Validation Loss: 0.0066\n",
      "Epoch 374/500 - Training Loss: 0.0061 - Validation Loss: 0.0064\n",
      "Epoch 375/500 - Training Loss: 0.0063 - Validation Loss: 0.0057\n",
      "Epoch 376/500 - Training Loss: 0.0057 - Validation Loss: 0.0055\n",
      "Epoch 377/500 - Training Loss: 0.0061 - Validation Loss: 0.0060\n",
      "Epoch 378/500 - Training Loss: 0.0064 - Validation Loss: 0.0069\n",
      "Epoch 379/500 - Training Loss: 0.0064 - Validation Loss: 0.0065\n",
      "Epoch 380/500 - Training Loss: 0.0058 - Validation Loss: 0.0055\n",
      "Epoch 381/500 - Training Loss: 0.0061 - Validation Loss: 0.0054\n",
      "Epoch 382/500 - Training Loss: 0.0065 - Validation Loss: 0.0058\n",
      "Epoch 383/500 - Training Loss: 0.0059 - Validation Loss: 0.0061\n",
      "Epoch 384/500 - Training Loss: 0.0063 - Validation Loss: 0.0061\n",
      "Epoch 385/500 - Training Loss: 0.0061 - Validation Loss: 0.0057\n",
      "Epoch 386/500 - Training Loss: 0.0057 - Validation Loss: 0.0055\n",
      "Epoch 387/500 - Training Loss: 0.0058 - Validation Loss: 0.0059\n",
      "Epoch 388/500 - Training Loss: 0.0066 - Validation Loss: 0.0060\n",
      "Epoch 389/500 - Training Loss: 0.0067 - Validation Loss: 0.0064\n",
      "Epoch 390/500 - Training Loss: 0.0066 - Validation Loss: 0.0060\n",
      "Epoch 391/500 - Training Loss: 0.0056 - Validation Loss: 0.0057\n",
      "Epoch 392/500 - Training Loss: 0.0061 - Validation Loss: 0.0058\n",
      "Epoch 393/500 - Training Loss: 0.0061 - Validation Loss: 0.0059\n",
      "Epoch 394/500 - Training Loss: 0.0069 - Validation Loss: 0.0060\n",
      "Epoch 395/500 - Training Loss: 0.0064 - Validation Loss: 0.0060\n",
      "Epoch 396/500 - Training Loss: 0.0065 - Validation Loss: 0.0063\n",
      "Epoch 397/500 - Training Loss: 0.0064 - Validation Loss: 0.0062\n",
      "Epoch 398/500 - Training Loss: 0.0068 - Validation Loss: 0.0056\n",
      "Epoch 399/500 - Training Loss: 0.0064 - Validation Loss: 0.0057\n",
      "Epoch 400/500 - Training Loss: 0.0056 - Validation Loss: 0.0060\n",
      "Epoch 401/500 - Training Loss: 0.0068 - Validation Loss: 0.0059\n",
      "Epoch 402/500 - Training Loss: 0.0053 - Validation Loss: 0.0057\n",
      "Epoch 403/500 - Training Loss: 0.0060 - Validation Loss: 0.0058\n",
      "Epoch 404/500 - Training Loss: 0.0064 - Validation Loss: 0.0057\n",
      "Epoch 405/500 - Training Loss: 0.0065 - Validation Loss: 0.0055\n",
      "Epoch 406/500 - Training Loss: 0.0067 - Validation Loss: 0.0059\n",
      "Epoch 407/500 - Training Loss: 0.0062 - Validation Loss: 0.0063\n",
      "Epoch 408/500 - Training Loss: 0.0062 - Validation Loss: 0.0057\n",
      "Epoch 409/500 - Training Loss: 0.0058 - Validation Loss: 0.0051\n",
      "Epoch 410/500 - Training Loss: 0.0061 - Validation Loss: 0.0056\n",
      "Epoch 411/500 - Training Loss: 0.0063 - Validation Loss: 0.0061\n",
      "Epoch 412/500 - Training Loss: 0.0058 - Validation Loss: 0.0059\n",
      "Epoch 413/500 - Training Loss: 0.0068 - Validation Loss: 0.0056\n",
      "Epoch 414/500 - Training Loss: 0.0062 - Validation Loss: 0.0061\n",
      "Epoch 415/500 - Training Loss: 0.0057 - Validation Loss: 0.0061\n",
      "Epoch 416/500 - Training Loss: 0.0062 - Validation Loss: 0.0058\n",
      "Epoch 417/500 - Training Loss: 0.0063 - Validation Loss: 0.0056\n",
      "Epoch 418/500 - Training Loss: 0.0068 - Validation Loss: 0.0054\n",
      "Epoch 419/500 - Training Loss: 0.0061 - Validation Loss: 0.0055\n",
      "Epoch 420/500 - Training Loss: 0.0063 - Validation Loss: 0.0053\n",
      "Epoch 421/500 - Training Loss: 0.0056 - Validation Loss: 0.0054\n",
      "Epoch 422/500 - Training Loss: 0.0059 - Validation Loss: 0.0063\n",
      "Epoch 423/500 - Training Loss: 0.0054 - Validation Loss: 0.0065\n",
      "Epoch 424/500 - Training Loss: 0.0058 - Validation Loss: 0.0059\n",
      "Epoch 425/500 - Training Loss: 0.0058 - Validation Loss: 0.0054\n",
      "Epoch 426/500 - Training Loss: 0.0055 - Validation Loss: 0.0053\n",
      "Epoch 427/500 - Training Loss: 0.0060 - Validation Loss: 0.0055\n",
      "Epoch 428/500 - Training Loss: 0.0054 - Validation Loss: 0.0060\n",
      "Epoch 429/500 - Training Loss: 0.0064 - Validation Loss: 0.0060\n",
      "Epoch 430/500 - Training Loss: 0.0057 - Validation Loss: 0.0057\n",
      "Epoch 431/500 - Training Loss: 0.0061 - Validation Loss: 0.0055\n",
      "Epoch 432/500 - Training Loss: 0.0053 - Validation Loss: 0.0056\n",
      "Epoch 433/500 - Training Loss: 0.0063 - Validation Loss: 0.0054\n",
      "Epoch 434/500 - Training Loss: 0.0064 - Validation Loss: 0.0054\n",
      "Epoch 435/500 - Training Loss: 0.0050 - Validation Loss: 0.0056\n",
      "Epoch 436/500 - Training Loss: 0.0052 - Validation Loss: 0.0053\n",
      "Epoch 437/500 - Training Loss: 0.0062 - Validation Loss: 0.0051\n",
      "Epoch 438/500 - Training Loss: 0.0051 - Validation Loss: 0.0054\n",
      "Epoch 439/500 - Training Loss: 0.0060 - Validation Loss: 0.0057\n",
      "Epoch 440/500 - Training Loss: 0.0052 - Validation Loss: 0.0057\n",
      "Epoch 441/500 - Training Loss: 0.0055 - Validation Loss: 0.0055\n",
      "Epoch 442/500 - Training Loss: 0.0060 - Validation Loss: 0.0054\n",
      "Epoch 443/500 - Training Loss: 0.0061 - Validation Loss: 0.0052\n",
      "Epoch 444/500 - Training Loss: 0.0063 - Validation Loss: 0.0053\n",
      "Epoch 445/500 - Training Loss: 0.0061 - Validation Loss: 0.0058\n",
      "Epoch 446/500 - Training Loss: 0.0063 - Validation Loss: 0.0062\n",
      "Epoch 447/500 - Training Loss: 0.0060 - Validation Loss: 0.0058\n",
      "Epoch 448/500 - Training Loss: 0.0065 - Validation Loss: 0.0052\n",
      "Epoch 449/500 - Training Loss: 0.0058 - Validation Loss: 0.0055\n",
      "Epoch 450/500 - Training Loss: 0.0051 - Validation Loss: 0.0060\n",
      "Epoch 451/500 - Training Loss: 0.0059 - Validation Loss: 0.0064\n",
      "Epoch 452/500 - Training Loss: 0.0057 - Validation Loss: 0.0057\n",
      "Epoch 453/500 - Training Loss: 0.0062 - Validation Loss: 0.0057\n",
      "Epoch 454/500 - Training Loss: 0.0061 - Validation Loss: 0.0061\n",
      "Epoch 455/500 - Training Loss: 0.0054 - Validation Loss: 0.0057\n",
      "Epoch 456/500 - Training Loss: 0.0057 - Validation Loss: 0.0054\n",
      "Epoch 457/500 - Training Loss: 0.0054 - Validation Loss: 0.0057\n",
      "Epoch 458/500 - Training Loss: 0.0056 - Validation Loss: 0.0060\n",
      "Epoch 459/500 - Training Loss: 0.0048 - Validation Loss: 0.0058\n",
      "Epoch 460/500 - Training Loss: 0.0061 - Validation Loss: 0.0054\n",
      "Epoch 461/500 - Training Loss: 0.0050 - Validation Loss: 0.0051\n",
      "Epoch 462/500 - Training Loss: 0.0052 - Validation Loss: 0.0052\n",
      "Epoch 463/500 - Training Loss: 0.0055 - Validation Loss: 0.0056\n",
      "Epoch 464/500 - Training Loss: 0.0053 - Validation Loss: 0.0060\n",
      "Epoch 465/500 - Training Loss: 0.0062 - Validation Loss: 0.0057\n",
      "Epoch 466/500 - Training Loss: 0.0053 - Validation Loss: 0.0051\n",
      "Epoch 467/500 - Training Loss: 0.0055 - Validation Loss: 0.0052\n",
      "Epoch 468/500 - Training Loss: 0.0061 - Validation Loss: 0.0055\n",
      "Epoch 469/500 - Training Loss: 0.0057 - Validation Loss: 0.0056\n",
      "Epoch 470/500 - Training Loss: 0.0049 - Validation Loss: 0.0057\n",
      "Epoch 471/500 - Training Loss: 0.0048 - Validation Loss: 0.0053\n",
      "Epoch 472/500 - Training Loss: 0.0066 - Validation Loss: 0.0053\n",
      "Epoch 473/500 - Training Loss: 0.0051 - Validation Loss: 0.0058\n",
      "Epoch 474/500 - Training Loss: 0.0055 - Validation Loss: 0.0057\n",
      "Epoch 475/500 - Training Loss: 0.0056 - Validation Loss: 0.0053\n",
      "Epoch 476/500 - Training Loss: 0.0045 - Validation Loss: 0.0048\n",
      "Epoch 477/500 - Training Loss: 0.0059 - Validation Loss: 0.0049\n",
      "Epoch 478/500 - Training Loss: 0.0055 - Validation Loss: 0.0055\n",
      "Epoch 479/500 - Training Loss: 0.0057 - Validation Loss: 0.0055\n",
      "Epoch 480/500 - Training Loss: 0.0056 - Validation Loss: 0.0050\n",
      "Epoch 481/500 - Training Loss: 0.0052 - Validation Loss: 0.0049\n",
      "Epoch 482/500 - Training Loss: 0.0050 - Validation Loss: 0.0051\n",
      "Epoch 483/500 - Training Loss: 0.0055 - Validation Loss: 0.0057\n",
      "Epoch 484/500 - Training Loss: 0.0058 - Validation Loss: 0.0054\n",
      "Epoch 485/500 - Training Loss: 0.0050 - Validation Loss: 0.0050\n",
      "Epoch 486/500 - Training Loss: 0.0051 - Validation Loss: 0.0052\n",
      "Epoch 487/500 - Training Loss: 0.0051 - Validation Loss: 0.0055\n",
      "Epoch 488/500 - Training Loss: 0.0050 - Validation Loss: 0.0058\n",
      "Epoch 489/500 - Training Loss: 0.0054 - Validation Loss: 0.0058\n",
      "Epoch 490/500 - Training Loss: 0.0063 - Validation Loss: 0.0055\n",
      "Epoch 491/500 - Training Loss: 0.0057 - Validation Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:38:33,374] A new study created in memory with name: no-name-307e55d6-0221-47bb-a3c2-0f65b0c40cfc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/500 - Training Loss: 0.0049 - Validation Loss: 0.0054\n",
      "Epoch 493/500 - Training Loss: 0.0062 - Validation Loss: 0.0052\n",
      "Epoch 494/500 - Training Loss: 0.0061 - Validation Loss: 0.0051\n",
      "Epoch 495/500 - Training Loss: 0.0051 - Validation Loss: 0.0054\n",
      "Epoch 496/500 - Training Loss: 0.0056 - Validation Loss: 0.0055\n",
      "Epoch 497/500 - Training Loss: 0.0051 - Validation Loss: 0.0054\n",
      "Epoch 498/500 - Training Loss: 0.0056 - Validation Loss: 0.0052\n",
      "Epoch 499/500 - Training Loss: 0.0054 - Validation Loss: 0.0058\n",
      "Epoch 500/500 - Training Loss: 0.0052 - Validation Loss: 0.0058\n",
      "Training of the best model completed.\n",
      "\n",
      "===== Model Performance =====\n",
      "Training Set:\n",
      "  R2 Score: 0.7336\n",
      "  MSE: 0.0027\n",
      "  MAE: 0.0360\n",
      "\n",
      "Testing Set:\n",
      "  R2 Score: 0.6113\n",
      "  MSE: 0.0046\n",
      "  MAE: 0.0421\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:38:34,102] Trial 0 finished with value: -238056960.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 326, 'n_units_l1': 971, 'dropout_rate': 0.3985490039117544, 'lr': 0.00012080916546233573, 'optimizer': 'AdamW'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:34,595] Trial 1 finished with value: -7877400064.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 281, 'n_units_l1': 411, 'dropout_rate': 0.3027302531145866, 'lr': 0.00031823940113229746, 'optimizer': 'SGD'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:35,109] Trial 2 finished with value: -9587012608.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 753, 'n_units_l1': 713, 'dropout_rate': 0.5236533621445085, 'lr': 0.0032264932713695184, 'optimizer': 'SGD'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:35,760] Trial 3 finished with value: -350215072.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 960, 'n_units_l1': 631, 'dropout_rate': 0.4359115577644705, 'lr': 2.3623925202853702e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:36,439] Trial 4 finished with value: -361175392.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 810, 'n_units_l1': 258, 'n_units_l2': 861, 'dropout_rate': 0.44307109640032033, 'lr': 0.00010265095661824739, 'optimizer': 'RMSprop'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:37,111] Trial 5 finished with value: -669741312.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 237, 'n_units_l1': 961, 'dropout_rate': 0.45061028745323, 'lr': 0.003437461570170951, 'optimizer': 'AdamW'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:37,833] Trial 6 finished with value: -3811522304.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 516, 'n_units_l1': 782, 'n_units_l2': 280, 'n_units_l3': 450, 'n_units_l4': 1002, 'dropout_rate': 0.3781083597404455, 'lr': 2.7610547794159227e-05, 'optimizer': 'SGD'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:38,498] Trial 7 finished with value: -309726976.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 411, 'n_units_l1': 727, 'dropout_rate': 0.5531614013536148, 'lr': 9.842734769631531e-05, 'optimizer': 'AdamW'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:39,323] Trial 8 finished with value: -261778048.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 658, 'n_units_l1': 435, 'n_units_l2': 205, 'n_units_l3': 375, 'dropout_rate': 0.3815125622634057, 'lr': 0.00016849836824714306, 'optimizer': 'Adam'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:39,889] Trial 9 finished with value: -5238694912.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 189, 'n_units_l1': 385, 'n_units_l2': 723, 'dropout_rate': 0.35907769285548596, 'lr': 0.004727282118813666, 'optimizer': 'SGD'}. Best is trial 0 with value: -238056960.0.\n",
      "[I 2024-09-25 14:38:40,939] Trial 10 finished with value: -153194512.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 449, 'n_units_l1': 1015, 'n_units_l2': 1002, 'n_units_l3': 906, 'dropout_rate': 0.2057637202736924, 'lr': 0.0007864936411751455, 'optimizer': 'AdamW'}. Best is trial 10 with value: -153194512.0.\n",
      "[I 2024-09-25 14:38:41,973] Trial 11 finished with value: -146017264.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 421, 'n_units_l1': 1024, 'n_units_l2': 988, 'n_units_l3': 985, 'dropout_rate': 0.21731348803917178, 'lr': 0.0004760714363550005, 'optimizer': 'AdamW'}. Best is trial 11 with value: -146017264.0.\n",
      "[I 2024-09-25 14:38:43,073] Trial 12 finished with value: -225982272.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 506, 'n_units_l1': 1015, 'n_units_l2': 1016, 'n_units_l3': 996, 'dropout_rate': 0.20262175136860824, 'lr': 0.0009628503080352669, 'optimizer': 'AdamW'}. Best is trial 11 with value: -146017264.0.\n",
      "[I 2024-09-25 14:38:44,148] Trial 13 finished with value: -188839680.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 415, 'n_units_l1': 850, 'n_units_l2': 1023, 'n_units_l3': 991, 'n_units_l4': 161, 'dropout_rate': 0.22263821235496234, 'lr': 0.0007516294028781212, 'optimizer': 'AdamW'}. Best is trial 11 with value: -146017264.0.\n",
      "[I 2024-09-25 14:38:44,997] Trial 14 finished with value: -164533648.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 608, 'n_units_l1': 878, 'n_units_l2': 553, 'n_units_l3': 769, 'dropout_rate': 0.2839067423552639, 'lr': 0.0011287988280951134, 'optimizer': 'RMSprop'}. Best is trial 11 with value: -146017264.0.\n",
      "[I 2024-09-25 14:38:45,949] Trial 15 finished with value: -172183648.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 417, 'n_units_l1': 551, 'n_units_l2': 792, 'n_units_l3': 729, 'dropout_rate': 0.27328154019849227, 'lr': 0.00046054680919097534, 'optimizer': 'AdamW'}. Best is trial 11 with value: -146017264.0.\n",
      "[I 2024-09-25 14:38:47,063] Trial 16 finished with value: -131991192.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 661, 'n_units_l1': 859, 'n_units_l2': 510, 'n_units_l3': 796, 'n_units_l4': 632, 'dropout_rate': 0.2514317271394648, 'lr': 0.0017954696753980108, 'optimizer': 'AdamW'}. Best is trial 16 with value: -131991192.0.\n",
      "[I 2024-09-25 14:38:48,099] Trial 17 finished with value: -118046912.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 717, 'n_units_l1': 873, 'n_units_l2': 476, 'n_units_l3': 151, 'n_units_l4': 617, 'dropout_rate': 0.32185857284209896, 'lr': 0.009737862623986064, 'optimizer': 'AdamW'}. Best is trial 17 with value: -118046912.0.\n",
      "[I 2024-09-25 14:38:49,061] Trial 18 finished with value: -5847257.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 888, 'n_units_l1': 867, 'n_units_l2': 446, 'n_units_l3': 219, 'n_units_l4': 627, 'dropout_rate': 0.32290755737740034, 'lr': 0.008690106655211005, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:49,999] Trial 19 finished with value: -884510848.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1020, 'n_units_l1': 569, 'n_units_l2': 392, 'n_units_l3': 131, 'n_units_l4': 601, 'dropout_rate': 0.3291582541561212, 'lr': 0.008451345653000911, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:50,929] Trial 20 finished with value: -8094668.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 888, 'n_units_l1': 654, 'n_units_l2': 410, 'n_units_l3': 137, 'n_units_l4': 707, 'dropout_rate': 0.3342172875106767, 'lr': 0.008768894485921396, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:51,848] Trial 21 finished with value: -56524432.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 867, 'n_units_l1': 662, 'n_units_l2': 446, 'n_units_l3': 136, 'n_units_l4': 713, 'dropout_rate': 0.3434998197810243, 'lr': 0.008926984959147028, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:52,785] Trial 22 finished with value: -5890002.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 874, 'n_units_l1': 629, 'n_units_l2': 371, 'n_units_l3': 282, 'n_units_l4': 825, 'dropout_rate': 0.3297745632239132, 'lr': 0.005150730763749643, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:53,727] Trial 23 finished with value: -125568128.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 899, 'n_units_l1': 569, 'n_units_l2': 340, 'n_units_l3': 299, 'n_units_l4': 931, 'dropout_rate': 0.25872641052838735, 'lr': 0.001961337554120021, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:54,616] Trial 24 finished with value: -46687992.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 861, 'n_units_l1': 473, 'n_units_l2': 141, 'n_units_l3': 279, 'n_units_l4': 802, 'dropout_rate': 0.31011345400509926, 'lr': 0.0049934785887186355, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:55,531] Trial 25 finished with value: -41398388.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1024, 'n_units_l1': 155, 'n_units_l2': 304, 'n_units_l3': 523, 'n_units_l4': 807, 'dropout_rate': 0.414132754750011, 'lr': 0.002191758209329618, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:56,285] Trial 26 finished with value: -131520792.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 930, 'n_units_l1': 757, 'n_units_l2': 623, 'dropout_rate': 0.35917863578531817, 'lr': 0.0051121307803357816, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:57,199] Trial 27 finished with value: -71650256.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 806, 'n_units_l1': 640, 'n_units_l2': 644, 'n_units_l3': 250, 'n_units_l4': 446, 'dropout_rate': 0.47771727277122306, 'lr': 0.005562560698871953, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:57,974] Trial 28 finished with value: -543656256.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 800, 'n_units_l1': 307, 'n_units_l2': 369, 'n_units_l3': 218, 'dropout_rate': 0.2982608045962762, 'lr': 0.0028644936470404968, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:58,922] Trial 29 finished with value: -227711040.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 947, 'n_units_l1': 521, 'n_units_l2': 236, 'n_units_l3': 421, 'n_units_l4': 443, 'dropout_rate': 0.4059923837022496, 'lr': 0.006841100368010109, 'optimizer': 'Adam'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:38:59,749] Trial 30 finished with value: -69559304.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 723, 'n_units_l1': 798, 'n_units_l2': 438, 'n_units_l3': 346, 'dropout_rate': 0.24476213428107088, 'lr': 0.0014076430602827797, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:00,647] Trial 31 finished with value: -61901184.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1022, 'n_units_l1': 186, 'n_units_l2': 307, 'n_units_l3': 584, 'n_units_l4': 796, 'dropout_rate': 0.40133782244068716, 'lr': 0.0024721621052480967, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:01,594] Trial 32 finished with value: -40751052.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 985, 'n_units_l1': 694, 'n_units_l2': 391, 'n_units_l3': 527, 'n_units_l4': 844, 'dropout_rate': 0.33951836434196125, 'lr': 0.003694488858249032, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:02,519] Trial 33 finished with value: -81272656.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 844, 'n_units_l1': 687, 'n_units_l2': 402, 'n_units_l3': 212, 'n_units_l4': 878, 'dropout_rate': 0.3427757653704206, 'lr': 0.0036748182533469726, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:03,471] Trial 34 finished with value: -37462780.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 973, 'n_units_l1': 618, 'n_units_l2': 517, 'n_units_l3': 566, 'n_units_l4': 745, 'dropout_rate': 0.2878145983416729, 'lr': 0.006639788800752539, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:04,395] Trial 35 finished with value: -417898496.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 922, 'n_units_l1': 616, 'n_units_l2': 523, 'n_units_l3': 649, 'n_units_l4': 682, 'dropout_rate': 0.3005589473838177, 'lr': 1.1527096470006594e-05, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:05,371] Trial 36 finished with value: -217828400.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 754, 'n_units_l1': 498, 'n_units_l2': 599, 'n_units_l3': 361, 'n_units_l4': 478, 'dropout_rate': 0.2832340775487719, 'lr': 0.009801141751458048, 'optimizer': 'Adam'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:06,074] Trial 37 finished with value: -5207092224.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 883, 'n_units_l1': 947, 'n_units_l2': 482, 'n_units_l3': 211, 'dropout_rate': 0.3737080704522795, 'lr': 0.006318028761157631, 'optimizer': 'SGD'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:07,014] Trial 38 finished with value: -156657712.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 968, 'n_units_l1': 725, 'n_units_l2': 672, 'n_units_l3': 307, 'n_units_l4': 727, 'dropout_rate': 0.3145678984699608, 'lr': 0.00018714485650460558, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:07,719] Trial 39 finished with value: -519991584.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 771, 'n_units_l1': 604, 'n_units_l2': 566, 'dropout_rate': 0.4710110397636205, 'lr': 5.2291992956949836e-05, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:08,586] Trial 40 finished with value: -116204080.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 305, 'n_units_l1': 808, 'n_units_l2': 244, 'n_units_l3': 415, 'n_units_l4': 536, 'dropout_rate': 0.4278663346049777, 'lr': 0.003983772564472255, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:09,584] Trial 41 finished with value: -26037968.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 969, 'n_units_l1': 696, 'n_units_l2': 426, 'n_units_l3': 523, 'n_units_l4': 878, 'dropout_rate': 0.33519164849583183, 'lr': 0.0036321210938100993, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:10,599] Trial 42 finished with value: -260531104.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 831, 'n_units_l1': 671, 'n_units_l2': 433, 'n_units_l3': 471, 'n_units_l4': 731, 'dropout_rate': 0.3734935636920629, 'lr': 0.006702883524489284, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:11,468] Trial 43 finished with value: -6152988160.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 974, 'n_units_l1': 744, 'n_units_l2': 354, 'n_units_l3': 636, 'n_units_l4': 928, 'dropout_rate': 0.2712348720147219, 'lr': 0.0029905931158749686, 'optimizer': 'SGD'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:12,521] Trial 44 finished with value: -67937152.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 898, 'n_units_l1': 609, 'n_units_l2': 507, 'n_units_l3': 480, 'n_units_l4': 755, 'dropout_rate': 0.361001846138825, 'lr': 0.006674211364003148, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:13,497] Trial 45 finished with value: -102712384.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 921, 'n_units_l1': 905, 'n_units_l2': 282, 'n_units_l3': 187, 'dropout_rate': 0.29414600029954313, 'lr': 0.0014450289699673416, 'optimizer': 'Adam'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:14,173] Trial 46 finished with value: -419770304.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 134, 'n_units_l1': 451, 'dropout_rate': 0.23520483464085506, 'lr': 0.004505037310403791, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:15,136] Trial 47 finished with value: -140308752.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 964, 'n_units_l1': 369, 'n_units_l2': 450, 'n_units_l3': 604, 'n_units_l4': 917, 'dropout_rate': 0.33385229287478485, 'lr': 0.007967627038810923, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:15,849] Trial 48 finished with value: -4426421760.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 668, 'n_units_l1': 539, 'n_units_l2': 554, 'n_units_l3': 681, 'dropout_rate': 0.38859854365405255, 'lr': 0.0032528960098771073, 'optimizer': 'SGD'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:16,918] Trial 49 finished with value: -95318336.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 782, 'n_units_l1': 774, 'n_units_l2': 704, 'n_units_l3': 893, 'n_units_l4': 677, 'dropout_rate': 0.3185240511599769, 'lr': 0.005236829054292159, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:17,885] Trial 50 finished with value: -19168074.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 850, 'n_units_l1': 828, 'n_units_l2': 182, 'n_units_l3': 539, 'n_units_l4': 869, 'dropout_rate': 0.5726242950321403, 'lr': 0.002506511081477316, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:18,911] Trial 51 finished with value: -597702464.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 843, 'n_units_l1': 922, 'n_units_l2': 141, 'n_units_l3': 536, 'n_units_l4': 873, 'dropout_rate': 0.4875469764080358, 'lr': 0.009906967881972175, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:19,965] Trial 52 finished with value: -52109800.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 906, 'n_units_l1': 708, 'n_units_l2': 182, 'n_units_l3': 179, 'n_units_l4': 779, 'dropout_rate': 0.5325644909062967, 'lr': 0.0022851273764051766, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:21,018] Trial 53 finished with value: -17633574.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 720, 'n_units_l1': 827, 'n_units_l2': 332, 'n_units_l3': 417, 'n_units_l4': 1017, 'dropout_rate': 0.5936909943301886, 'lr': 0.006816054872590546, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:21,993] Trial 54 finished with value: -336680704.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 702, 'n_units_l1': 828, 'n_units_l2': 240, 'n_units_l3': 392, 'n_units_l4': 965, 'dropout_rate': 0.5892084347085924, 'lr': 0.004146199805150577, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:23,048] Trial 55 finished with value: -75410416.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 822, 'n_units_l1': 906, 'n_units_l2': 323, 'n_units_l3': 253, 'n_units_l4': 1023, 'dropout_rate': 0.5998480413145694, 'lr': 0.0014501866222385125, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:23,973] Trial 56 finished with value: -204423008.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 539, 'n_units_l1': 838, 'n_units_l2': 418, 'n_units_l3': 454, 'dropout_rate': 0.565568165846754, 'lr': 0.002880516144356577, 'optimizer': 'Adam'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:24,901] Trial 57 finished with value: -44733600.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 599, 'n_units_l1': 762, 'n_units_l2': 187, 'n_units_l3': 354, 'n_units_l4': 988, 'dropout_rate': 0.5596085659136735, 'lr': 0.0003927779663506282, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:25,849] Trial 58 finished with value: -7964160.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 874, 'n_units_l1': 952, 'n_units_l2': 360, 'n_units_l3': 313, 'n_units_l4': 847, 'dropout_rate': 0.5748105777311654, 'lr': 0.0005938743753605154, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:26,668] Trial 59 finished with value: -4034372864.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 873, 'n_units_l1': 955, 'n_units_l2': 363, 'n_units_l3': 331, 'n_units_l4': 556, 'dropout_rate': 0.5469917399640064, 'lr': 0.00028584475480834814, 'optimizer': 'SGD'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:27,473] Trial 60 finished with value: -1680465152.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 735, 'n_units_l1': 1011, 'n_units_l2': 263, 'n_units_l3': 262, 'dropout_rate': 0.5802223840226916, 'lr': 5.9588497754760883e-05, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:28,422] Trial 61 finished with value: -149076912.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 867, 'n_units_l1': 978, 'n_units_l2': 332, 'n_units_l3': 162, 'n_units_l4': 862, 'dropout_rate': 0.505117006788209, 'lr': 0.0005924515263347206, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:29,384] Trial 62 finished with value: -55833208.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 803, 'n_units_l1': 881, 'n_units_l2': 863, 'n_units_l3': 303, 'n_units_l4': 147, 'dropout_rate': 0.5763968278698722, 'lr': 0.0009724761916548358, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:30,372] Trial 63 finished with value: -162657280.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 994, 'n_units_l1': 805, 'n_units_l2': 461, 'n_units_l3': 494, 'n_units_l4': 654, 'dropout_rate': 0.5714827335083392, 'lr': 0.00780489013009488, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:31,537] Trial 64 finished with value: -165367136.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 922, 'n_units_l1': 649, 'n_units_l2': 287, 'n_units_l3': 426, 'n_units_l4': 825, 'dropout_rate': 0.5389146340653628, 'lr': 0.0017262164662801209, 'optimizer': 'AdamW'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:32,666] Trial 65 finished with value: -24389518.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 781, 'n_units_l1': 986, 'n_units_l2': 383, 'n_units_l3': 389, 'n_units_l4': 913, 'dropout_rate': 0.5262648466627404, 'lr': 0.005809503638651376, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:33,825] Trial 66 finished with value: -21630140.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 777, 'n_units_l1': 973, 'n_units_l2': 389, 'n_units_l3': 388, 'n_units_l4': 959, 'dropout_rate': 0.5252282681134042, 'lr': 0.005443208047234948, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:34,960] Trial 67 finished with value: -108509912.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 688, 'n_units_l1': 934, 'n_units_l2': 338, 'n_units_l3': 269, 'n_units_l4': 969, 'dropout_rate': 0.5105103113975344, 'lr': 0.004728278613013482, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:36,043] Trial 68 finished with value: -563995072.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 748, 'n_units_l1': 893, 'n_units_l2': 206, 'n_units_l3': 234, 'n_units_l4': 341, 'dropout_rate': 0.5934487522662413, 'lr': 0.008234797160986566, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:37,174] Trial 69 finished with value: -23153550.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 836, 'n_units_l1': 860, 'n_units_l2': 398, 'n_units_l3': 320, 'n_units_l4': 618, 'dropout_rate': 0.5553832948316009, 'lr': 0.002557123431322873, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:38,073] Trial 70 finished with value: -185272496.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 369, 'n_units_l1': 992, 'n_units_l2': 305, 'dropout_rate': 0.5810230630389417, 'lr': 0.005405969037502179, 'optimizer': 'Adam'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:39,204] Trial 71 finished with value: -38950580.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 838, 'n_units_l1': 851, 'n_units_l2': 407, 'n_units_l3': 324, 'n_units_l4': 614, 'dropout_rate': 0.5443910779848937, 'lr': 0.002573029825507333, 'optimizer': 'RMSprop'}. Best is trial 18 with value: -5847257.0.\n",
      "[I 2024-09-25 14:39:40,349] Trial 72 finished with value: -1582258.25 and parameters: {'hidden_layers': 5, 'n_units_l0': 890, 'n_units_l1': 872, 'n_units_l2': 480, 'n_units_l3': 283, 'n_units_l4': 579, 'dropout_rate': 0.5537233340574725, 'lr': 0.007292141285585957, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:41,516] Trial 73 finished with value: -1929432.375 and parameters: {'hidden_layers': 5, 'n_units_l0': 940, 'n_units_l1': 917, 'n_units_l2': 497, 'n_units_l3': 191, 'n_units_l4': 956, 'dropout_rate': 0.5114285373676217, 'lr': 0.008056697715790052, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:42,648] Trial 74 finished with value: -50063008.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 949, 'n_units_l1': 817, 'n_units_l2': 499, 'n_units_l3': 133, 'n_units_l4': 524, 'dropout_rate': 0.561680638630793, 'lr': 0.007883144851213228, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:43,822] Trial 75 finished with value: -7472936.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 882, 'n_units_l1': 929, 'n_units_l2': 535, 'n_units_l3': 202, 'n_units_l4': 1019, 'dropout_rate': 0.3534950101604836, 'lr': 0.007336315994754375, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:45,140] Trial 76 finished with value: -66773328.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 879, 'n_units_l1': 940, 'n_units_l2': 592, 'n_units_l3': 203, 'n_units_l4': 1010, 'dropout_rate': 0.36154638121593824, 'lr': 0.007346478460053231, 'optimizer': 'AdamW'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:46,163] Trial 77 finished with value: -4728740.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 998, 'n_units_l1': 910, 'n_units_l2': 528, 'n_units_l3': 163, 'n_units_l4': 1024, 'dropout_rate': 0.35103977576102313, 'lr': 0.009488964970540492, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:47,306] Trial 78 finished with value: -1071561600.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 998, 'n_units_l1': 919, 'n_units_l2': 480, 'n_units_l3': 156, 'n_units_l4': 581, 'dropout_rate': 0.34687586023200007, 'lr': 0.009450031523725547, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:48,047] Trial 79 finished with value: -352614176.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 943, 'n_units_l1': 872, 'dropout_rate': 0.3502202421268701, 'lr': 0.004276117071879997, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:49,012] Trial 80 finished with value: -2816210688.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 922, 'n_units_l1': 895, 'n_units_l2': 537, 'n_units_l3': 224, 'n_units_l4': 950, 'dropout_rate': 0.3225095723890294, 'lr': 0.006059312906539482, 'optimizer': 'SGD'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:50,174] Trial 81 finished with value: -59829508.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 900, 'n_units_l1': 925, 'n_units_l2': 465, 'n_units_l3': 283, 'n_units_l4': 992, 'dropout_rate': 0.31076238102894094, 'lr': 0.008758906155310154, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:51,349] Trial 82 finished with value: -20924040.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1002, 'n_units_l1': 959, 'n_units_l2': 529, 'n_units_l3': 158, 'n_units_l4': 1024, 'dropout_rate': 0.39198964060920993, 'lr': 0.006996974054067874, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:52,474] Trial 83 finished with value: -11971956.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 890, 'n_units_l1': 786, 'n_units_l2': 608, 'n_units_l3': 187, 'n_units_l4': 908, 'dropout_rate': 0.32872520373414216, 'lr': 0.009547794898737706, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:53,611] Trial 84 finished with value: -103522928.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 893, 'n_units_l1': 870, 'n_units_l2': 633, 'n_units_l3': 184, 'n_units_l4': 901, 'dropout_rate': 0.3261052869131702, 'lr': 0.009677314379896877, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:54,749] Trial 85 finished with value: -90000936.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 946, 'n_units_l1': 787, 'n_units_l2': 590, 'n_units_l3': 238, 'n_units_l4': 843, 'dropout_rate': 0.36986471139133936, 'lr': 0.0047251310720627165, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:55,849] Trial 86 finished with value: -111927640.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 865, 'n_units_l1': 591, 'n_units_l2': 494, 'n_units_l3': 130, 'n_units_l4': 783, 'dropout_rate': 0.3533669166586428, 'lr': 0.00020159448473683765, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:56,949] Trial 87 finished with value: -336117792.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 812, 'n_units_l1': 738, 'n_units_l2': 555, 'n_units_l3': 200, 'n_units_l4': 704, 'dropout_rate': 0.30543865918153856, 'lr': 0.009986276605233766, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:58,106] Trial 88 finished with value: -58731712.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 941, 'n_units_l1': 1006, 'n_units_l2': 441, 'n_units_l3': 179, 'n_units_l4': 943, 'dropout_rate': 0.45710688279304806, 'lr': 0.00013332705340599087, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:39:59,388] Trial 89 finished with value: -107874368.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 909, 'n_units_l1': 909, 'n_units_l2': 615, 'n_units_l3': 291, 'n_units_l4': 825, 'dropout_rate': 0.4172110550101657, 'lr': 0.006001811070272902, 'optimizer': 'AdamW'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:00,473] Trial 90 finished with value: -63415700.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 888, 'n_units_l1': 845, 'n_units_l2': 657, 'n_units_l3': 231, 'dropout_rate': 0.335119860897518, 'lr': 0.007797020749460609, 'optimizer': 'Adam'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:01,659] Trial 91 finished with value: -63461900.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 862, 'n_units_l1': 940, 'n_units_l2': 573, 'n_units_l3': 261, 'n_units_l4': 983, 'dropout_rate': 0.3820116159723037, 'lr': 0.006772027216458916, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:02,788] Trial 92 finished with value: -312569568.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 961, 'n_units_l1': 889, 'n_units_l2': 357, 'n_units_l3': 208, 'n_units_l4': 652, 'dropout_rate': 0.36661867887321886, 'lr': 0.008391041281706983, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:03,946] Trial 93 finished with value: -171127888.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1008, 'n_units_l1': 779, 'n_units_l2': 541, 'n_units_l3': 165, 'n_units_l4': 906, 'dropout_rate': 0.2930696377727536, 'lr': 0.0037679890655300864, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:05,093] Trial 94 finished with value: -171001440.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 459, 'n_units_l1': 962, 'n_units_l2': 427, 'n_units_l3': 243, 'n_units_l4': 934, 'dropout_rate': 0.2710511145575623, 'lr': 0.006416025001958875, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:06,247] Trial 95 finished with value: -17135030.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 929, 'n_units_l1': 824, 'n_units_l2': 511, 'n_units_l3': 188, 'n_units_l4': 1004, 'dropout_rate': 0.3277103962097582, 'lr': 0.004988703687265792, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:07,133] Trial 96 finished with value: -191331888.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 980, 'n_units_l1': 666, 'n_units_l2': 515, 'dropout_rate': 0.3277071088417519, 'lr': 2.3295927501686314e-05, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:08,265] Trial 97 finished with value: -110892296.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 926, 'n_units_l1': 631, 'n_units_l2': 466, 'n_units_l3': 187, 'n_units_l4': 990, 'dropout_rate': 0.31451273334021823, 'lr': 0.0034090098865418124, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:09,385] Trial 98 finished with value: -125481400.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 909, 'n_units_l1': 857, 'n_units_l2': 491, 'n_units_l3': 148, 'n_units_l4': 492, 'dropout_rate': 0.3415901461359361, 'lr': 0.005000009358505853, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:10,511] Trial 99 finished with value: -305837312.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 852, 'n_units_l1': 556, 'n_units_l2': 448, 'n_units_l3': 212, 'n_units_l4': 960, 'dropout_rate': 0.30354553114697563, 'lr': 0.008476741699804282, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:11,473] Trial 100 finished with value: -5601901056.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 817, 'n_units_l1': 758, 'n_units_l2': 518, 'n_units_l3': 276, 'n_units_l4': 578, 'dropout_rate': 0.3549240206980382, 'lr': 0.004210159241771593, 'optimizer': 'SGD'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:12,599] Trial 101 finished with value: -82369752.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 883, 'n_units_l1': 825, 'n_units_l2': 373, 'n_units_l3': 172, 'n_units_l4': 1004, 'dropout_rate': 0.5892899886891617, 'lr': 0.007185276176650444, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:13,735] Trial 102 finished with value: -86872344.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 938, 'n_units_l1': 801, 'n_units_l2': 419, 'n_units_l3': 198, 'n_units_l4': 894, 'dropout_rate': 0.3357208945044007, 'lr': 0.0059899647447594654, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:14,874] Trial 103 finished with value: -22605940.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 910, 'n_units_l1': 721, 'n_units_l2': 567, 'n_units_l3': 246, 'n_units_l4': 1020, 'dropout_rate': 0.5117458320385943, 'lr': 0.005360463259848647, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:16,016] Trial 104 finished with value: -52532896.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 885, 'n_units_l1': 879, 'n_units_l2': 478, 'n_units_l3': 145, 'n_units_l4': 986, 'dropout_rate': 0.31793887119939196, 'lr': 0.007277853653098704, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:17,195] Trial 105 finished with value: -1966165.625 and parameters: {'hidden_layers': 5, 'n_units_l0': 618, 'n_units_l1': 906, 'n_units_l2': 944, 'n_units_l3': 298, 'n_units_l4': 755, 'dropout_rate': 0.598530307464692, 'lr': 0.004634611145619266, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:18,320] Trial 106 finished with value: -41130960.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 271, 'n_units_l1': 897, 'n_units_l2': 855, 'n_units_l3': 342, 'n_units_l4': 751, 'dropout_rate': 0.28017537275004123, 'lr': 0.008929400425751136, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:19,470] Trial 107 finished with value: -168849248.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 621, 'n_units_l1': 921, 'n_units_l2': 740, 'n_units_l3': 306, 'n_units_l4': 760, 'dropout_rate': 0.34588503960513617, 'lr': 0.0031807006299039223, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:20,671] Trial 108 finished with value: -23549238.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 795, 'n_units_l1': 949, 'n_units_l2': 983, 'n_units_l3': 221, 'n_units_l4': 696, 'dropout_rate': 0.3275511343172661, 'lr': 0.004545185556878053, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:21,835] Trial 109 finished with value: -118153040.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 542, 'n_units_l1': 997, 'n_units_l2': 928, 'n_units_l3': 281, 'n_units_l4': 654, 'dropout_rate': 0.37817468168313817, 'lr': 0.005191282372651973, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:22,993] Trial 110 finished with value: -976241088.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 1022, 'n_units_l1': 838, 'n_units_l2': 410, 'n_units_l3': 370, 'n_units_l4': 849, 'dropout_rate': 0.3613866837611057, 'lr': 0.009952737023773667, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:24,117] Trial 111 finished with value: -721417472.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 626, 'n_units_l1': 908, 'n_units_l2': 313, 'n_units_l3': 194, 'n_units_l4': 939, 'dropout_rate': 0.5681209188120121, 'lr': 0.006140947551747693, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:25,298] Trial 112 finished with value: -5039348.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 587, 'n_units_l1': 866, 'n_units_l2': 796, 'n_units_l3': 226, 'n_units_l4': 801, 'dropout_rate': 0.595043109522956, 'lr': 0.007279943257924919, 'optimizer': 'RMSprop'}. Best is trial 72 with value: -1582258.25.\n",
      "[I 2024-09-25 14:40:26,442] Trial 113 finished with value: -262851.9375 and parameters: {'hidden_layers': 5, 'n_units_l0': 475, 'n_units_l1': 864, 'n_units_l2': 780, 'n_units_l3': 252, 'n_units_l4': 728, 'dropout_rate': 0.5858346745568316, 'lr': 0.0077084598523987965, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:27,610] Trial 114 finished with value: -116229488.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 480, 'n_units_l1': 975, 'n_units_l2': 784, 'n_units_l3': 249, 'n_units_l4': 817, 'dropout_rate': 0.5991399383775089, 'lr': 0.008146998651090734, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:28,903] Trial 115 finished with value: -132839400.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 573, 'n_units_l1': 857, 'n_units_l2': 834, 'n_units_l3': 852, 'n_units_l4': 722, 'dropout_rate': 0.5816846096636717, 'lr': 0.0007203502383537943, 'optimizer': 'Adam'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:30,125] Trial 116 finished with value: -18588890.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 502, 'n_units_l1': 929, 'n_units_l2': 890, 'n_units_l3': 971, 'n_units_l4': 797, 'dropout_rate': 0.5508994436720774, 'lr': 0.007113620387237416, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:31,365] Trial 117 finished with value: -926743360.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 579, 'n_units_l1': 882, 'n_units_l2': 756, 'n_units_l3': 260, 'n_units_l4': 737, 'dropout_rate': 0.5852797685113135, 'lr': 6.890497749183764e-05, 'optimizer': 'AdamW'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:32,517] Trial 118 finished with value: -14042294.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 393, 'n_units_l1': 869, 'n_units_l2': 961, 'n_units_l3': 313, 'n_units_l4': 765, 'dropout_rate': 0.5729470999883738, 'lr': 0.008503026617813754, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:33,724] Trial 119 finished with value: -74594520.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 960, 'n_units_l1': 949, 'n_units_l2': 924, 'n_units_l3': 217, 'n_units_l4': 788, 'dropout_rate': 0.5576785209052249, 'lr': 0.005724699916515017, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:34,695] Trial 120 finished with value: -213172272.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 644, 'n_units_l1': 914, 'n_units_l2': 718, 'n_units_l3': 290, 'n_units_l4': 683, 'dropout_rate': 0.5906370561457751, 'lr': 0.007533932421044862, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:35,642] Trial 121 finished with value: -131105080.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 391, 'n_units_l1': 862, 'n_units_l2': 812, 'n_units_l3': 328, 'n_units_l4': 762, 'dropout_rate': 0.5733178534410845, 'lr': 0.008690608942731022, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:36,798] Trial 122 finished with value: -6216731.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 431, 'n_units_l1': 897, 'n_units_l2': 965, 'n_units_l3': 317, 'n_units_l4': 846, 'dropout_rate': 0.5755182091041904, 'lr': 0.006315234777746927, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:37,915] Trial 123 finished with value: -38063472.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 533, 'n_units_l1': 586, 'n_units_l2': 949, 'n_units_l3': 267, 'n_units_l4': 841, 'dropout_rate': 0.5407599921919896, 'lr': 0.006271446456293891, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:39,068] Trial 124 finished with value: -99537856.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 350, 'n_units_l1': 888, 'n_units_l2': 678, 'n_units_l3': 349, 'n_units_l4': 890, 'dropout_rate': 0.5807502361977113, 'lr': 0.003996565166846429, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:40,221] Trial 125 finished with value: -1153586.125 and parameters: {'hidden_layers': 5, 'n_units_l0': 439, 'n_units_l1': 900, 'n_units_l2': 896, 'n_units_l3': 231, 'n_units_l4': 812, 'dropout_rate': 0.5997307833111389, 'lr': 0.009800675103408185, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:41,169] Trial 126 finished with value: -3858004736.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 456, 'n_units_l1': 356, 'n_units_l2': 992, 'n_units_l3': 228, 'n_units_l4': 805, 'dropout_rate': 0.5982885603867213, 'lr': 0.00746986774641328, 'optimizer': 'SGD'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:42,293] Trial 127 finished with value: -53768216.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 468, 'n_units_l1': 937, 'n_units_l2': 894, 'n_units_l3': 292, 'n_units_l4': 601, 'dropout_rate': 0.5639441248479797, 'lr': 0.006585273566059528, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:43,440] Trial 128 finished with value: -27659474.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 409, 'n_units_l1': 965, 'n_units_l2': 1011, 'n_units_l3': 163, 'n_units_l4': 826, 'dropout_rate': 0.5872881908450858, 'lr': 0.0003090004385688339, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:44,574] Trial 129 finished with value: -42455636.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 513, 'n_units_l1': 903, 'n_units_l2': 897, 'n_units_l3': 131, 'n_units_l4': 873, 'dropout_rate': 0.5945399358815351, 'lr': 0.009976059216300286, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:45,732] Trial 130 finished with value: -31402578.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 422, 'n_units_l1': 930, 'n_units_l2': 973, 'n_units_l3': 240, 'n_units_l4': 773, 'dropout_rate': 0.5767157694197794, 'lr': 0.005425292798399579, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:46,885] Trial 131 finished with value: -602580992.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 432, 'n_units_l1': 893, 'n_units_l2': 940, 'n_units_l3': 177, 'n_units_l4': 711, 'dropout_rate': 0.5678730561761164, 'lr': 0.008341334559900734, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:48,011] Trial 132 finished with value: -46892552.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 436, 'n_units_l1': 850, 'n_units_l2': 801, 'n_units_l3': 214, 'n_units_l4': 854, 'dropout_rate': 0.5829335413036196, 'lr': 0.009205269311335423, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:49,191] Trial 133 finished with value: -13019492.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 486, 'n_units_l1': 1024, 'n_units_l2': 872, 'n_units_l3': 271, 'n_units_l4': 923, 'dropout_rate': 0.549787644865655, 'lr': 0.007218686559193722, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:50,330] Trial 134 finished with value: -16094269.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 852, 'n_units_l1': 683, 'n_units_l2': 347, 'n_units_l3': 147, 'n_units_l4': 230, 'dropout_rate': 0.5996750734585851, 'lr': 0.005918439856535674, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:51,371] Trial 135 finished with value: -11728958.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 354, 'n_units_l1': 874, 'n_units_l2': 917, 'n_units_l3': 232, 'n_units_l4': 558, 'dropout_rate': 0.5879577599015492, 'lr': 0.0045634535014057965, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:52,499] Trial 136 finished with value: -31410162.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 314, 'n_units_l1': 916, 'n_units_l2': 926, 'n_units_l3': 298, 'n_units_l4': 550, 'dropout_rate': 0.5891415825462828, 'lr': 0.004324184520817405, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:53,649] Trial 137 finished with value: -19769816.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 360, 'n_units_l1': 873, 'n_units_l2': 836, 'n_units_l3': 249, 'n_units_l4': 527, 'dropout_rate': 0.5621590716776147, 'lr': 0.0035014905009401742, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:54,769] Trial 138 finished with value: -54304456.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 261, 'n_units_l1': 517, 'n_units_l2': 458, 'n_units_l3': 231, 'n_units_l4': 628, 'dropout_rate': 0.5777814710950055, 'lr': 0.0011546165437649803, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:55,822] Trial 139 finished with value: -161521152.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 383, 'n_units_l1': 262, 'n_units_l2': 963, 'n_units_l3': 205, 'n_units_l4': 600, 'dropout_rate': 0.5335909964446983, 'lr': 0.004766663786378175, 'optimizer': 'Adam'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:56,596] Trial 140 finished with value: -185594480.0 and parameters: {'hidden_layers': 2, 'n_units_l0': 584, 'n_units_l1': 840, 'dropout_rate': 0.556377078218458, 'lr': 0.006633398020380142, 'optimizer': 'AdamW'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:57,736] Trial 141 finished with value: -53448280.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 555, 'n_units_l1': 808, 'n_units_l2': 905, 'n_units_l3': 173, 'n_units_l4': 494, 'dropout_rate': 0.3399154051258932, 'lr': 0.007822421146250218, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:40:58,919] Trial 142 finished with value: -37366084.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 899, 'n_units_l1': 896, 'n_units_l2': 1019, 'n_units_l3': 196, 'n_units_l4': 815, 'dropout_rate': 0.5908738669207544, 'lr': 0.008923126774272018, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:00,016] Trial 143 finished with value: -15518158.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 872, 'n_units_l1': 651, 'n_units_l2': 382, 'n_units_l3': 260, 'n_units_l4': 833, 'dropout_rate': 0.49165861170534075, 'lr': 0.009939445903900074, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:01,139] Trial 144 finished with value: -462310976.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 301, 'n_units_l1': 876, 'n_units_l2': 551, 'n_units_l3': 700, 'n_units_l4': 863, 'dropout_rate': 0.5678121222272315, 'lr': 0.005663083025552531, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:02,174] Trial 145 finished with value: -51612508.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 838, 'n_units_l1': 950, 'n_units_l2': 495, 'n_units_l3': 222, 'n_units_l4': 743, 'dropout_rate': 0.5839280652581316, 'lr': 0.0002410293254146307, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:03,244] Trial 146 finished with value: -307486976.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 326, 'n_units_l1': 792, 'n_units_l2': 877, 'n_units_l3': 314, 'n_units_l4': 775, 'dropout_rate': 0.30705079609742886, 'lr': 0.00770057762421011, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:04,334] Trial 147 finished with value: -60506448.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 988, 'n_units_l1': 914, 'n_units_l2': 998, 'n_units_l3': 279, 'n_units_l4': 665, 'dropout_rate': 0.31931889418149434, 'lr': 0.0067763424424570826, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:05,364] Trial 148 finished with value: -25233106.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 338, 'n_units_l1': 984, 'n_units_l2': 911, 'n_units_l3': 150, 'n_units_l4': 552, 'dropout_rate': 0.5993170891506664, 'lr': 0.0004073264084147235, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:06,361] Trial 149 finished with value: -121226128.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 441, 'n_units_l1': 932, 'n_units_l2': 610, 'n_units_l3': 332, 'n_units_l4': 802, 'dropout_rate': 0.35124217114286294, 'lr': 0.008459332013269125, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:07,346] Trial 150 finished with value: -127073952.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 155, 'n_units_l1': 850, 'n_units_l2': 431, 'n_units_l3': 176, 'n_units_l4': 882, 'dropout_rate': 0.5743653139866697, 'lr': 0.004909061093353189, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:08,374] Trial 151 finished with value: -30977308.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 498, 'n_units_l1': 963, 'n_units_l2': 869, 'n_units_l3': 273, 'n_units_l4': 917, 'dropout_rate': 0.5495968545161003, 'lr': 0.007451257228729079, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:09,549] Trial 152 finished with value: -23277342.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 497, 'n_units_l1': 1020, 'n_units_l2': 842, 'n_units_l3': 242, 'n_units_l4': 968, 'dropout_rate': 0.5887418844524243, 'lr': 0.006276880884325248, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:10,670] Trial 153 finished with value: -21786194.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 410, 'n_units_l1': 900, 'n_units_l2': 764, 'n_units_l3': 268, 'n_units_l4': 917, 'dropout_rate': 0.446496447605886, 'lr': 0.007102127616905746, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:11,717] Trial 154 finished with value: -9369610.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 223, 'n_units_l1': 884, 'n_units_l2': 950, 'n_units_l3': 193, 'n_units_l4': 568, 'dropout_rate': 0.5536245929859831, 'lr': 0.008877442278646903, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:12,714] Trial 155 finished with value: -2979822080.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 175, 'n_units_l1': 870, 'n_units_l2': 948, 'n_units_l3': 197, 'n_units_l4': 589, 'dropout_rate': 0.5802832804071698, 'lr': 0.009986284712949276, 'optimizer': 'SGD'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:13,669] Trial 156 finished with value: -194573344.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 218, 'n_units_l1': 630, 'n_units_l2': 476, 'n_units_l3': 162, 'n_units_l4': 562, 'dropout_rate': 0.33223643770300315, 'lr': 1.083250669300135e-05, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:14,813] Trial 157 finished with value: -78441128.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 883, 'n_units_l1': 837, 'n_units_l2': 531, 'n_units_l3': 225, 'n_units_l4': 570, 'dropout_rate': 0.565460706147975, 'lr': 0.008386041979838263, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:15,725] Trial 158 finished with value: -22247348.0 and parameters: {'hidden_layers': 4, 'n_units_l0': 914, 'n_units_l1': 883, 'n_units_l2': 975, 'n_units_l3': 208, 'dropout_rate': 0.5161501893818401, 'lr': 0.005858172716819088, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:16,868] Trial 159 finished with value: -50469300.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 224, 'n_units_l1': 915, 'n_units_l2': 403, 'n_units_l3': 187, 'n_units_l4': 511, 'dropout_rate': 0.5359073787819382, 'lr': 0.008866750522967337, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:18,074] Trial 160 finished with value: -80952960.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 854, 'n_units_l1': 815, 'n_units_l2': 933, 'n_units_l3': 247, 'n_units_l4': 788, 'dropout_rate': 0.34038807552428973, 'lr': 0.005271772410769796, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:19,273] Trial 161 finished with value: -102409264.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 479, 'n_units_l1': 863, 'n_units_l2': 920, 'n_units_l3': 286, 'n_units_l4': 980, 'dropout_rate': 0.5730641261374695, 'lr': 0.007083587141343538, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:20,473] Trial 162 finished with value: -1108124.75 and parameters: {'hidden_layers': 5, 'n_units_l0': 527, 'n_units_l1': 940, 'n_units_l2': 956, 'n_units_l3': 262, 'n_units_l4': 733, 'dropout_rate': 0.548427419950707, 'lr': 0.008076431775175669, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:21,674] Trial 163 finished with value: -23458582.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 602, 'n_units_l1': 942, 'n_units_l2': 964, 'n_units_l3': 232, 'n_units_l4': 712, 'dropout_rate': 0.5569394507239497, 'lr': 0.008184432748863804, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:22,873] Trial 164 finished with value: -3000649.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 533, 'n_units_l1': 896, 'n_units_l2': 947, 'n_units_l3': 305, 'n_units_l4': 726, 'dropout_rate': 0.5423292834397161, 'lr': 0.009948508308723923, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:24,073] Trial 165 finished with value: -33765932.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 469, 'n_units_l1': 900, 'n_units_l2': 945, 'n_units_l3': 316, 'n_units_l4': 740, 'dropout_rate': 0.5428329982269263, 'lr': 0.006283908722643193, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:25,295] Trial 166 finished with value: -303516992.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 534, 'n_units_l1': 927, 'n_units_l2': 911, 'n_units_l3': 360, 'n_units_l4': 691, 'dropout_rate': 0.5302868680196323, 'lr': 0.008701911414961454, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:26,488] Trial 167 finished with value: -40500764.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 518, 'n_units_l1': 883, 'n_units_l2': 979, 'n_units_l3': 299, 'n_units_l4': 665, 'dropout_rate': 0.5494791228850334, 'lr': 0.0005763776007550672, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:27,670] Trial 168 finished with value: -1067053440.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 556, 'n_units_l1': 969, 'n_units_l2': 953, 'n_units_l3': 338, 'n_units_l4': 639, 'dropout_rate': 0.5590753831546031, 'lr': 2.5547407551114426e-05, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:28,690] Trial 169 finished with value: -263762960.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 568, 'n_units_l1': 918, 'n_units_l2': 367, 'n_units_l3': 256, 'n_units_l4': 732, 'dropout_rate': 0.5198784724857675, 'lr': 0.009914805704771414, 'optimizer': 'Adam'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:29,839] Trial 170 finished with value: -35703232.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 514, 'n_units_l1': 948, 'n_units_l2': 449, 'n_units_l3': 300, 'n_units_l4': 749, 'dropout_rate': 0.4317465172441126, 'lr': 0.007716261939449162, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:30,874] Trial 171 finished with value: -19853542.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 448, 'n_units_l1': 885, 'n_units_l2': 1003, 'n_units_l3': 212, 'n_units_l4': 538, 'dropout_rate': 0.5937548036346407, 'lr': 0.009142216115866313, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:32,049] Trial 172 finished with value: -8689019.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 895, 'n_units_l1': 907, 'n_units_l2': 938, 'n_units_l3': 188, 'n_units_l4': 713, 'dropout_rate': 0.32661768896404203, 'lr': 0.006635383169325728, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:33,131] Trial 173 finished with value: -160741632.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 825, 'n_units_l1': 899, 'n_units_l2': 936, 'n_units_l3': 162, 'n_units_l4': 722, 'dropout_rate': 0.3126616016552342, 'lr': 0.00649712788163944, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:34,148] Trial 174 finished with value: -34922600.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 872, 'n_units_l1': 932, 'n_units_l2': 991, 'n_units_l3': 240, 'n_units_l4': 445, 'dropout_rate': 0.32239246946311684, 'lr': 0.005694664905002062, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:35,162] Trial 175 finished with value: -11300072.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 931, 'n_units_l1': 862, 'n_units_l2': 891, 'n_units_l3': 258, 'n_units_l4': 706, 'dropout_rate': 0.5804993021226501, 'lr': 0.007615549379827803, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:36,273] Trial 176 finished with value: -28801762.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 932, 'n_units_l1': 846, 'n_units_l2': 507, 'n_units_l3': 136, 'n_units_l4': 698, 'dropout_rate': 0.5708441695106246, 'lr': 0.007480527644781547, 'optimizer': 'AdamW'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:37,427] Trial 177 finished with value: -11445590.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 947, 'n_units_l1': 433, 'n_units_l2': 884, 'n_units_l3': 283, 'n_units_l4': 721, 'dropout_rate': 0.5422355916123563, 'lr': 0.008019459397183923, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:38,499] Trial 178 finished with value: -10807669.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 900, 'n_units_l1': 905, 'n_units_l2': 966, 'n_units_l3': 257, 'n_units_l4': 768, 'dropout_rate': 0.3468649611714143, 'lr': 0.00668008046482718, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:39,301] Trial 179 finished with value: -404379328.0 and parameters: {'hidden_layers': 3, 'n_units_l0': 685, 'n_units_l1': 906, 'n_units_l2': 963, 'dropout_rate': 0.34662950813967824, 'lr': 0.006551676821828751, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:40,439] Trial 180 finished with value: -73402912.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 644, 'n_units_l1': 924, 'n_units_l2': 987, 'n_units_l3': 196, 'n_units_l4': 765, 'dropout_rate': 0.33347925068764456, 'lr': 0.005607485419071142, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:41,609] Trial 181 finished with value: -39465892.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 914, 'n_units_l1': 859, 'n_units_l2': 946, 'n_units_l3': 775, 'n_units_l4': 677, 'dropout_rate': 0.3456804555301708, 'lr': 0.007093382010524304, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:42,627] Trial 182 finished with value: -183268288.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 891, 'n_units_l1': 890, 'n_units_l2': 905, 'n_units_l3': 260, 'n_units_l4': 789, 'dropout_rate': 0.3698282949906267, 'lr': 0.008785420086431524, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:43,760] Trial 183 finished with value: -74355984.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 969, 'n_units_l1': 910, 'n_units_l2': 938, 'n_units_l3': 257, 'n_units_l4': 808, 'dropout_rate': 0.35711284796341897, 'lr': 0.007868861837699071, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:44,895] Trial 184 finished with value: -54880696.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 862, 'n_units_l1': 947, 'n_units_l2': 969, 'n_units_l3': 315, 'n_units_l4': 708, 'dropout_rate': 0.5641907766369526, 'lr': 0.009848788830079313, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:45,865] Trial 185 finished with value: -7600014.5 and parameters: {'hidden_layers': 5, 'n_units_l0': 902, 'n_units_l1': 860, 'n_units_l2': 481, 'n_units_l3': 216, 'n_units_l4': 756, 'dropout_rate': 0.5821083189223826, 'lr': 0.006854633801989708, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:46,841] Trial 186 finished with value: -252143712.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 910, 'n_units_l1': 884, 'n_units_l2': 468, 'n_units_l3': 218, 'n_units_l4': 754, 'dropout_rate': 0.2988233383864484, 'lr': 3.984523208599928e-05, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:47,993] Trial 187 finished with value: -32162572.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 590, 'n_units_l1': 934, 'n_units_l2': 491, 'n_units_l3': 173, 'n_units_l4': 781, 'dropout_rate': 0.49972765494440907, 'lr': 0.006473398101063015, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:49,024] Trial 188 finished with value: -5566616576.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 899, 'n_units_l1': 908, 'n_units_l2': 1024, 'n_units_l3': 203, 'n_units_l4': 730, 'dropout_rate': 0.46625951796138065, 'lr': 9.60478250336841e-05, 'optimizer': 'SGD'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:50,032] Trial 189 finished with value: -10491015.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 885, 'n_units_l1': 958, 'n_units_l2': 439, 'n_units_l3': 286, 'n_units_l4': 822, 'dropout_rate': 0.5935441949603124, 'lr': 0.005965841860536937, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:51,027] Trial 190 finished with value: -127548752.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 874, 'n_units_l1': 1000, 'n_units_l2': 418, 'n_units_l3': 303, 'n_units_l4': 816, 'dropout_rate': 0.5926270185300211, 'lr': 0.005362847738384114, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:52,175] Trial 191 finished with value: -74959168.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 893, 'n_units_l1': 982, 'n_units_l2': 447, 'n_units_l3': 278, 'n_units_l4': 838, 'dropout_rate': 0.5987571612974778, 'lr': 0.006489686937151084, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:53,292] Trial 192 finished with value: -476799104.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 865, 'n_units_l1': 957, 'n_units_l2': 393, 'n_units_l3': 236, 'n_units_l4': 771, 'dropout_rate': 0.5847344605641244, 'lr': 0.008541033699467595, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:54,474] Trial 193 finished with value: -4985749.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 845, 'n_units_l1': 922, 'n_units_l2': 540, 'n_units_l3': 292, 'n_units_l4': 854, 'dropout_rate': 0.5790725124242785, 'lr': 0.0048581890696532525, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:55,587] Trial 194 finished with value: -28575260.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 848, 'n_units_l1': 968, 'n_units_l2': 533, 'n_units_l3': 328, 'n_units_l4': 860, 'dropout_rate': 0.5823681352849766, 'lr': 0.005850823216850214, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:56,638] Trial 195 finished with value: -10138034.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 558, 'n_units_l1': 934, 'n_units_l2': 486, 'n_units_l3': 297, 'n_units_l4': 827, 'dropout_rate': 0.5736803338029363, 'lr': 0.004208040448377871, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:57,748] Trial 196 finished with value: -97202688.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 567, 'n_units_l1': 916, 'n_units_l2': 548, 'n_units_l3': 344, 'n_units_l4': 835, 'dropout_rate': 0.5745674637718575, 'lr': 0.003939686158374977, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:58,762] Trial 197 finished with value: -121608720.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 542, 'n_units_l1': 939, 'n_units_l2': 508, 'n_units_l3': 185, 'n_units_l4': 1023, 'dropout_rate': 0.5609928418676857, 'lr': 0.004578998547725292, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:41:59,888] Trial 198 finished with value: -92259016.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 543, 'n_units_l1': 874, 'n_units_l2': 520, 'n_units_l3': 307, 'n_units_l4': 794, 'dropout_rate': 0.5547571605088734, 'lr': 0.004890135789845719, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n",
      "[I 2024-09-25 14:42:00,939] Trial 199 finished with value: -63930888.0 and parameters: {'hidden_layers': 5, 'n_units_l0': 623, 'n_units_l1': 895, 'n_units_l2': 580, 'n_units_l3': 208, 'n_units_l4': 1004, 'dropout_rate': 0.5683649011107811, 'lr': 0.003915745347981352, 'optimizer': 'RMSprop'}. Best is trial 113 with value: -262851.9375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization completed.\n",
      "Best Trial:\n",
      "  Value (MSE): -262851.9375\n",
      "  Params:\n",
      "    hidden_layers: 5\n",
      "    n_units_l0: 475\n",
      "    n_units_l1: 864\n",
      "    n_units_l2: 780\n",
      "    n_units_l3: 252\n",
      "    n_units_l4: 728\n",
      "    dropout_rate: 0.5858346745568316\n",
      "    lr: 0.0077084598523987965\n",
      "    optimizer: RMSprop\n",
      "Starting training of the best model...\n",
      "Epoch 1/500 - Training Loss: 14252395.6434 - Validation Loss: 1.2448\n",
      "Epoch 2/500 - Training Loss: 0.9647 - Validation Loss: 0.1941\n",
      "Epoch 3/500 - Training Loss: 0.1349 - Validation Loss: 0.0406\n",
      "Epoch 4/500 - Training Loss: 0.0838 - Validation Loss: 0.0376\n",
      "Epoch 5/500 - Training Loss: 0.0605 - Validation Loss: 0.0262\n",
      "Epoch 6/500 - Training Loss: 0.0566 - Validation Loss: 0.0455\n",
      "Epoch 7/500 - Training Loss: 0.0630 - Validation Loss: 0.0274\n",
      "Epoch 8/500 - Training Loss: 0.0429 - Validation Loss: 0.0289\n",
      "Epoch 9/500 - Training Loss: 0.0473 - Validation Loss: 0.0275\n",
      "Epoch 10/500 - Training Loss: 0.0458 - Validation Loss: 0.0312\n",
      "Epoch 11/500 - Training Loss: 0.0412 - Validation Loss: 0.0258\n",
      "Epoch 12/500 - Training Loss: 0.0425 - Validation Loss: 0.0261\n",
      "Epoch 13/500 - Training Loss: 0.0397 - Validation Loss: 0.0295\n",
      "Epoch 14/500 - Training Loss: 0.0471 - Validation Loss: 0.0262\n",
      "Epoch 15/500 - Training Loss: 0.0369 - Validation Loss: 0.0262\n",
      "Epoch 16/500 - Training Loss: 0.0363 - Validation Loss: 0.0259\n",
      "Epoch 17/500 - Training Loss: 0.0363 - Validation Loss: 0.0258\n",
      "Epoch 18/500 - Training Loss: 0.0382 - Validation Loss: 0.0259\n",
      "Epoch 19/500 - Training Loss: 0.0444 - Validation Loss: 0.0261\n",
      "Epoch 20/500 - Training Loss: 0.0351 - Validation Loss: 0.0268\n",
      "Epoch 21/500 - Training Loss: 0.0368 - Validation Loss: 0.0300\n",
      "Epoch 22/500 - Training Loss: 0.0369 - Validation Loss: 0.0257\n",
      "Epoch 23/500 - Training Loss: 0.0362 - Validation Loss: 0.0264\n",
      "Epoch 24/500 - Training Loss: 0.0382 - Validation Loss: 0.0264\n",
      "Epoch 25/500 - Training Loss: 0.0374 - Validation Loss: 0.0258\n",
      "Epoch 26/500 - Training Loss: 0.0313 - Validation Loss: 0.0269\n",
      "Epoch 27/500 - Training Loss: 0.0346 - Validation Loss: 0.0259\n",
      "Epoch 28/500 - Training Loss: 0.0354 - Validation Loss: 0.0263\n",
      "Epoch 29/500 - Training Loss: 0.0330 - Validation Loss: 0.0259\n",
      "Epoch 30/500 - Training Loss: 0.0390 - Validation Loss: 0.0267\n",
      "Epoch 31/500 - Training Loss: 0.0362 - Validation Loss: 0.0258\n",
      "Epoch 32/500 - Training Loss: 0.0343 - Validation Loss: 0.0279\n",
      "Epoch 33/500 - Training Loss: 0.0336 - Validation Loss: 0.0287\n",
      "Epoch 34/500 - Training Loss: 0.0310 - Validation Loss: 0.0258\n",
      "Epoch 35/500 - Training Loss: 0.0383 - Validation Loss: 0.0260\n",
      "Epoch 36/500 - Training Loss: 0.0330 - Validation Loss: 0.0266\n",
      "Epoch 37/500 - Training Loss: 0.0328 - Validation Loss: 0.0272\n",
      "Epoch 38/500 - Training Loss: 0.0345 - Validation Loss: 0.0299\n",
      "Epoch 39/500 - Training Loss: 0.0309 - Validation Loss: 0.0283\n",
      "Epoch 40/500 - Training Loss: 0.0335 - Validation Loss: 0.0261\n",
      "Epoch 41/500 - Training Loss: 0.0331 - Validation Loss: 0.0286\n",
      "Epoch 42/500 - Training Loss: 0.0345 - Validation Loss: 0.0266\n",
      "Epoch 43/500 - Training Loss: 0.0372 - Validation Loss: 0.0267\n",
      "Epoch 44/500 - Training Loss: 0.0333 - Validation Loss: 0.0293\n",
      "Epoch 45/500 - Training Loss: 0.0295 - Validation Loss: 0.0266\n",
      "Epoch 46/500 - Training Loss: 0.0324 - Validation Loss: 0.0259\n",
      "Epoch 47/500 - Training Loss: 0.0329 - Validation Loss: 0.0291\n",
      "Epoch 48/500 - Training Loss: 0.0312 - Validation Loss: 0.0267\n",
      "Epoch 49/500 - Training Loss: 0.0379 - Validation Loss: 0.0283\n",
      "Epoch 50/500 - Training Loss: 0.0359 - Validation Loss: 0.0272\n",
      "Epoch 51/500 - Training Loss: 0.0357 - Validation Loss: 0.0260\n",
      "Epoch 52/500 - Training Loss: 0.0363 - Validation Loss: 0.0264\n",
      "Epoch 53/500 - Training Loss: 0.0311 - Validation Loss: 0.0263\n",
      "Epoch 54/500 - Training Loss: 0.0321 - Validation Loss: 0.0259\n",
      "Epoch 55/500 - Training Loss: 0.0321 - Validation Loss: 0.0259\n",
      "Epoch 56/500 - Training Loss: 0.0329 - Validation Loss: 0.0259\n",
      "Epoch 57/500 - Training Loss: 0.0332 - Validation Loss: 0.0259\n",
      "Epoch 58/500 - Training Loss: 0.0341 - Validation Loss: 0.0277\n",
      "Epoch 59/500 - Training Loss: 0.0338 - Validation Loss: 0.0260\n",
      "Epoch 60/500 - Training Loss: 0.0328 - Validation Loss: 0.0258\n",
      "Epoch 61/500 - Training Loss: 0.0297 - Validation Loss: 0.0260\n",
      "Epoch 62/500 - Training Loss: 0.0323 - Validation Loss: 0.0263\n",
      "Epoch 63/500 - Training Loss: 0.0320 - Validation Loss: 0.0268\n",
      "Epoch 64/500 - Training Loss: 0.0314 - Validation Loss: 0.0309\n",
      "Epoch 65/500 - Training Loss: 0.0295 - Validation Loss: 0.0258\n",
      "Epoch 66/500 - Training Loss: 0.0295 - Validation Loss: 0.0260\n",
      "Epoch 67/500 - Training Loss: 0.0351 - Validation Loss: 0.0259\n",
      "Epoch 68/500 - Training Loss: 0.0323 - Validation Loss: 0.0285\n",
      "Epoch 69/500 - Training Loss: 0.0340 - Validation Loss: 0.0262\n",
      "Epoch 70/500 - Training Loss: 0.0326 - Validation Loss: 0.0280\n",
      "Epoch 71/500 - Training Loss: 0.0321 - Validation Loss: 0.0267\n",
      "Epoch 72/500 - Training Loss: 0.0323 - Validation Loss: 0.0261\n",
      "Epoch 73/500 - Training Loss: 0.0321 - Validation Loss: 0.0260\n",
      "Epoch 74/500 - Training Loss: 0.0324 - Validation Loss: 0.0261\n",
      "Epoch 75/500 - Training Loss: 0.0321 - Validation Loss: 0.0259\n",
      "Epoch 76/500 - Training Loss: 0.0334 - Validation Loss: 0.0261\n",
      "Epoch 77/500 - Training Loss: 0.0288 - Validation Loss: 0.0259\n",
      "Epoch 78/500 - Training Loss: 0.0329 - Validation Loss: 0.0264\n",
      "Epoch 79/500 - Training Loss: 0.0328 - Validation Loss: 0.0261\n",
      "Epoch 80/500 - Training Loss: 0.0345 - Validation Loss: 0.0259\n",
      "Epoch 81/500 - Training Loss: 0.0343 - Validation Loss: 0.0273\n",
      "Epoch 82/500 - Training Loss: 0.0324 - Validation Loss: 0.0258\n",
      "Epoch 83/500 - Training Loss: 0.0339 - Validation Loss: 0.0265\n",
      "Epoch 84/500 - Training Loss: 0.0323 - Validation Loss: 0.0262\n",
      "Epoch 85/500 - Training Loss: 0.0309 - Validation Loss: 0.0261\n",
      "Epoch 86/500 - Training Loss: 0.0304 - Validation Loss: 0.0259\n",
      "Epoch 87/500 - Training Loss: 0.0348 - Validation Loss: 0.0261\n",
      "Epoch 88/500 - Training Loss: 0.0306 - Validation Loss: 0.0282\n",
      "Epoch 89/500 - Training Loss: 0.0333 - Validation Loss: 0.0269\n",
      "Epoch 90/500 - Training Loss: 0.0335 - Validation Loss: 0.0270\n",
      "Epoch 91/500 - Training Loss: 0.0322 - Validation Loss: 0.0260\n",
      "Epoch 92/500 - Training Loss: 0.0327 - Validation Loss: 0.0290\n",
      "Epoch 93/500 - Training Loss: 0.0308 - Validation Loss: 0.0285\n",
      "Epoch 94/500 - Training Loss: 0.0325 - Validation Loss: 0.0260\n",
      "Epoch 95/500 - Training Loss: 0.0322 - Validation Loss: 0.0261\n",
      "Epoch 96/500 - Training Loss: 0.0325 - Validation Loss: 0.0259\n",
      "Epoch 97/500 - Training Loss: 0.0323 - Validation Loss: 0.0263\n",
      "Epoch 98/500 - Training Loss: 0.0309 - Validation Loss: 0.0281\n",
      "Epoch 99/500 - Training Loss: 0.0305 - Validation Loss: 0.0391\n",
      "Epoch 100/500 - Training Loss: 0.0415 - Validation Loss: 0.0304\n",
      "Epoch 101/500 - Training Loss: 0.0307 - Validation Loss: 0.0259\n",
      "Epoch 102/500 - Training Loss: 0.0335 - Validation Loss: 0.0261\n",
      "Epoch 103/500 - Training Loss: 0.0369 - Validation Loss: 0.0289\n",
      "Epoch 104/500 - Training Loss: 0.0340 - Validation Loss: 0.0265\n",
      "Epoch 105/500 - Training Loss: 0.0339 - Validation Loss: 0.0302\n",
      "Epoch 106/500 - Training Loss: 0.0342 - Validation Loss: 0.0260\n",
      "Epoch 107/500 - Training Loss: 0.0340 - Validation Loss: 0.0258\n",
      "Epoch 108/500 - Training Loss: 0.0310 - Validation Loss: 0.0259\n",
      "Epoch 109/500 - Training Loss: 0.0317 - Validation Loss: 0.0259\n",
      "Epoch 110/500 - Training Loss: 0.0319 - Validation Loss: 0.0259\n",
      "Epoch 111/500 - Training Loss: 0.0325 - Validation Loss: 0.0260\n",
      "Epoch 112/500 - Training Loss: 0.0323 - Validation Loss: 0.0260\n",
      "Epoch 113/500 - Training Loss: 0.0341 - Validation Loss: 0.0265\n",
      "Epoch 114/500 - Training Loss: 0.0324 - Validation Loss: 0.0260\n",
      "Epoch 115/500 - Training Loss: 0.0311 - Validation Loss: 0.0274\n",
      "Epoch 116/500 - Training Loss: 0.0314 - Validation Loss: 0.0259\n",
      "Epoch 117/500 - Training Loss: 0.0323 - Validation Loss: 0.0262\n",
      "Epoch 118/500 - Training Loss: 0.0342 - Validation Loss: 0.0266\n",
      "Epoch 119/500 - Training Loss: 0.0293 - Validation Loss: 0.0260\n",
      "Epoch 120/500 - Training Loss: 0.0349 - Validation Loss: 0.0261\n",
      "Epoch 121/500 - Training Loss: 0.0327 - Validation Loss: 0.0260\n",
      "Epoch 122/500 - Training Loss: 0.0321 - Validation Loss: 0.0299\n",
      "Epoch 123/500 - Training Loss: 0.0363 - Validation Loss: 0.0263\n",
      "Epoch 124/500 - Training Loss: 0.0297 - Validation Loss: 0.0263\n",
      "Epoch 125/500 - Training Loss: 0.0320 - Validation Loss: 0.0258\n",
      "Epoch 126/500 - Training Loss: 0.0305 - Validation Loss: 0.0276\n",
      "Epoch 127/500 - Training Loss: 0.0352 - Validation Loss: 0.0265\n",
      "Epoch 128/500 - Training Loss: 0.0317 - Validation Loss: 0.0264\n",
      "Epoch 129/500 - Training Loss: 0.0317 - Validation Loss: 0.0261\n",
      "Epoch 130/500 - Training Loss: 0.0321 - Validation Loss: 0.0261\n",
      "Epoch 131/500 - Training Loss: 0.0291 - Validation Loss: 0.0300\n",
      "Epoch 132/500 - Training Loss: 0.0338 - Validation Loss: 0.0270\n",
      "Epoch 133/500 - Training Loss: 0.0284 - Validation Loss: 0.0262\n",
      "Epoch 134/500 - Training Loss: 0.0297 - Validation Loss: 0.0260\n",
      "Epoch 135/500 - Training Loss: 0.0315 - Validation Loss: 0.0260\n",
      "Epoch 136/500 - Training Loss: 0.0341 - Validation Loss: 0.0268\n",
      "Epoch 137/500 - Training Loss: 0.0345 - Validation Loss: 0.0260\n",
      "Epoch 138/500 - Training Loss: 0.0313 - Validation Loss: 0.0287\n",
      "Epoch 139/500 - Training Loss: 0.0322 - Validation Loss: 0.0264\n",
      "Epoch 140/500 - Training Loss: 0.0349 - Validation Loss: 0.0302\n",
      "Epoch 141/500 - Training Loss: 0.0303 - Validation Loss: 0.0263\n",
      "Epoch 142/500 - Training Loss: 0.0337 - Validation Loss: 0.0261\n",
      "Epoch 143/500 - Training Loss: 0.0333 - Validation Loss: 0.0260\n",
      "Epoch 144/500 - Training Loss: 0.0301 - Validation Loss: 0.0263\n",
      "Epoch 145/500 - Training Loss: 0.0318 - Validation Loss: 0.0266\n",
      "Epoch 146/500 - Training Loss: 0.0317 - Validation Loss: 0.0291\n",
      "Epoch 147/500 - Training Loss: 0.0328 - Validation Loss: 0.0260\n",
      "Epoch 148/500 - Training Loss: 0.0284 - Validation Loss: 0.0273\n",
      "Epoch 149/500 - Training Loss: 0.0305 - Validation Loss: 0.0260\n",
      "Epoch 150/500 - Training Loss: 0.0301 - Validation Loss: 0.0272\n",
      "Epoch 151/500 - Training Loss: 0.0326 - Validation Loss: 0.0264\n",
      "Epoch 152/500 - Training Loss: 0.0318 - Validation Loss: 0.0260\n",
      "Epoch 153/500 - Training Loss: 0.0301 - Validation Loss: 0.0259\n",
      "Epoch 154/500 - Training Loss: 0.0327 - Validation Loss: 0.0292\n",
      "Epoch 155/500 - Training Loss: 0.0338 - Validation Loss: 0.0266\n",
      "Epoch 156/500 - Training Loss: 0.0389 - Validation Loss: 0.0265\n",
      "Epoch 157/500 - Training Loss: 0.0290 - Validation Loss: 0.0261\n",
      "Epoch 158/500 - Training Loss: 0.0312 - Validation Loss: 0.0258\n",
      "Epoch 159/500 - Training Loss: 0.0320 - Validation Loss: 0.0281\n",
      "Epoch 160/500 - Training Loss: 0.0342 - Validation Loss: 0.0261\n",
      "Epoch 161/500 - Training Loss: 0.0330 - Validation Loss: 0.0259\n",
      "Epoch 162/500 - Training Loss: 0.0280 - Validation Loss: 0.0277\n",
      "Epoch 163/500 - Training Loss: 0.0295 - Validation Loss: 0.0264\n",
      "Epoch 164/500 - Training Loss: 0.0372 - Validation Loss: 0.0260\n",
      "Epoch 165/500 - Training Loss: 0.0298 - Validation Loss: 0.0259\n",
      "Epoch 166/500 - Training Loss: 0.0282 - Validation Loss: 0.0265\n",
      "Epoch 167/500 - Training Loss: 0.0301 - Validation Loss: 0.0260\n",
      "Epoch 168/500 - Training Loss: 0.0320 - Validation Loss: 0.0260\n",
      "Epoch 169/500 - Training Loss: 0.0306 - Validation Loss: 0.0265\n",
      "Epoch 170/500 - Training Loss: 0.0306 - Validation Loss: 0.0270\n",
      "Epoch 171/500 - Training Loss: 0.0297 - Validation Loss: 0.0260\n",
      "Epoch 172/500 - Training Loss: 0.0288 - Validation Loss: 0.0267\n",
      "Epoch 173/500 - Training Loss: 0.0290 - Validation Loss: 0.0269\n",
      "Epoch 174/500 - Training Loss: 0.0310 - Validation Loss: 0.0272\n",
      "Epoch 175/500 - Training Loss: 0.0293 - Validation Loss: 0.0270\n",
      "Epoch 176/500 - Training Loss: 0.0288 - Validation Loss: 0.0268\n",
      "Epoch 177/500 - Training Loss: 0.0355 - Validation Loss: 0.0266\n",
      "Epoch 178/500 - Training Loss: 0.0314 - Validation Loss: 0.0260\n",
      "Epoch 179/500 - Training Loss: 0.0343 - Validation Loss: 0.0264\n",
      "Epoch 180/500 - Training Loss: 0.0311 - Validation Loss: 0.0260\n",
      "Epoch 181/500 - Training Loss: 0.0307 - Validation Loss: 0.0258\n",
      "Epoch 182/500 - Training Loss: 0.0295 - Validation Loss: 0.0262\n",
      "Epoch 183/500 - Training Loss: 0.0307 - Validation Loss: 0.0259\n",
      "Epoch 184/500 - Training Loss: 0.0295 - Validation Loss: 0.0260\n",
      "Epoch 185/500 - Training Loss: 0.0316 - Validation Loss: 0.0272\n",
      "Epoch 186/500 - Training Loss: 0.0290 - Validation Loss: 0.0272\n",
      "Epoch 187/500 - Training Loss: 0.0297 - Validation Loss: 0.0267\n",
      "Epoch 188/500 - Training Loss: 0.0320 - Validation Loss: 0.0262\n",
      "Epoch 189/500 - Training Loss: 0.0323 - Validation Loss: 0.0288\n",
      "Epoch 190/500 - Training Loss: 0.0317 - Validation Loss: 0.0260\n",
      "Epoch 191/500 - Training Loss: 0.0303 - Validation Loss: 0.0300\n",
      "Epoch 192/500 - Training Loss: 0.0333 - Validation Loss: 0.0259\n",
      "Epoch 193/500 - Training Loss: 0.0309 - Validation Loss: 0.0267\n",
      "Epoch 194/500 - Training Loss: 0.0309 - Validation Loss: 0.0259\n",
      "Epoch 195/500 - Training Loss: 0.0335 - Validation Loss: 0.0261\n",
      "Epoch 196/500 - Training Loss: 0.0382 - Validation Loss: 0.0260\n",
      "Epoch 197/500 - Training Loss: 0.0285 - Validation Loss: 0.0268\n",
      "Epoch 198/500 - Training Loss: 0.0298 - Validation Loss: 0.0261\n",
      "Epoch 199/500 - Training Loss: 0.0347 - Validation Loss: 0.0262\n",
      "Epoch 200/500 - Training Loss: 0.0342 - Validation Loss: 0.0259\n",
      "Epoch 201/500 - Training Loss: 0.0352 - Validation Loss: 0.0264\n",
      "Epoch 202/500 - Training Loss: 0.0309 - Validation Loss: 0.0262\n",
      "Epoch 203/500 - Training Loss: 0.0338 - Validation Loss: 0.0260\n",
      "Epoch 204/500 - Training Loss: 0.0318 - Validation Loss: 0.0261\n",
      "Epoch 205/500 - Training Loss: 0.0301 - Validation Loss: 0.0259\n",
      "Epoch 206/500 - Training Loss: 0.0329 - Validation Loss: 0.0260\n",
      "Epoch 207/500 - Training Loss: 0.0312 - Validation Loss: 0.0259\n",
      "Epoch 208/500 - Training Loss: 0.0325 - Validation Loss: 0.0259\n",
      "Epoch 209/500 - Training Loss: 0.0327 - Validation Loss: 0.0264\n",
      "Epoch 210/500 - Training Loss: 0.0277 - Validation Loss: 0.0259\n",
      "Epoch 211/500 - Training Loss: 0.0293 - Validation Loss: 0.0272\n",
      "Epoch 212/500 - Training Loss: 0.0316 - Validation Loss: 0.0266\n",
      "Epoch 213/500 - Training Loss: 0.0341 - Validation Loss: 0.0270\n",
      "Epoch 214/500 - Training Loss: 0.0298 - Validation Loss: 0.0261\n",
      "Epoch 215/500 - Training Loss: 0.0310 - Validation Loss: 0.0265\n",
      "Epoch 216/500 - Training Loss: 0.0322 - Validation Loss: 0.0259\n",
      "Epoch 217/500 - Training Loss: 0.0329 - Validation Loss: 0.0262\n",
      "Epoch 218/500 - Training Loss: 0.0325 - Validation Loss: 0.0259\n",
      "Epoch 219/500 - Training Loss: 0.0336 - Validation Loss: 0.0266\n",
      "Epoch 220/500 - Training Loss: 0.0308 - Validation Loss: 0.0266\n",
      "Epoch 221/500 - Training Loss: 0.0313 - Validation Loss: 0.0261\n",
      "Epoch 222/500 - Training Loss: 0.0319 - Validation Loss: 0.0259\n",
      "Epoch 223/500 - Training Loss: 0.0365 - Validation Loss: 0.0262\n",
      "Epoch 224/500 - Training Loss: 0.0331 - Validation Loss: 0.0259\n",
      "Epoch 225/500 - Training Loss: 0.0365 - Validation Loss: 0.0274\n",
      "Epoch 226/500 - Training Loss: 0.0322 - Validation Loss: 0.0262\n",
      "Epoch 227/500 - Training Loss: 0.0291 - Validation Loss: 0.0264\n",
      "Epoch 228/500 - Training Loss: 0.0618 - Validation Loss: 0.0276\n",
      "Epoch 229/500 - Training Loss: 0.0348 - Validation Loss: 0.0314\n",
      "Epoch 230/500 - Training Loss: 0.0309 - Validation Loss: 0.0283\n",
      "Epoch 231/500 - Training Loss: 0.0329 - Validation Loss: 0.0263\n",
      "Epoch 232/500 - Training Loss: 0.0279 - Validation Loss: 0.0283\n",
      "Epoch 233/500 - Training Loss: 0.0313 - Validation Loss: 0.0260\n",
      "Epoch 234/500 - Training Loss: 0.0303 - Validation Loss: 0.0259\n",
      "Epoch 235/500 - Training Loss: 0.0320 - Validation Loss: 0.0260\n",
      "Epoch 236/500 - Training Loss: 0.0329 - Validation Loss: 0.0262\n",
      "Epoch 237/500 - Training Loss: 0.0340 - Validation Loss: 0.0285\n",
      "Epoch 238/500 - Training Loss: 0.0312 - Validation Loss: 0.0262\n",
      "Epoch 239/500 - Training Loss: 0.0301 - Validation Loss: 0.0280\n",
      "Epoch 240/500 - Training Loss: 0.0318 - Validation Loss: 0.0268\n",
      "Epoch 241/500 - Training Loss: 0.0295 - Validation Loss: 0.0260\n",
      "Epoch 242/500 - Training Loss: 0.0332 - Validation Loss: 0.0262\n",
      "Epoch 243/500 - Training Loss: 0.0331 - Validation Loss: 0.0259\n",
      "Epoch 244/500 - Training Loss: 0.0282 - Validation Loss: 0.0259\n",
      "Epoch 245/500 - Training Loss: 0.0293 - Validation Loss: 0.0275\n",
      "Epoch 246/500 - Training Loss: 0.0303 - Validation Loss: 0.0267\n",
      "Epoch 247/500 - Training Loss: 0.0308 - Validation Loss: 0.0263\n",
      "Epoch 248/500 - Training Loss: 0.0335 - Validation Loss: 0.0261\n",
      "Epoch 249/500 - Training Loss: 0.0305 - Validation Loss: 0.0264\n",
      "Epoch 250/500 - Training Loss: 0.0333 - Validation Loss: 0.0259\n",
      "Epoch 251/500 - Training Loss: 0.0331 - Validation Loss: 0.0258\n",
      "Epoch 252/500 - Training Loss: 0.0331 - Validation Loss: 0.0259\n",
      "Epoch 253/500 - Training Loss: 0.0310 - Validation Loss: 0.0259\n",
      "Epoch 254/500 - Training Loss: 0.0331 - Validation Loss: 0.0260\n",
      "Epoch 255/500 - Training Loss: 0.0286 - Validation Loss: 0.0259\n",
      "Epoch 256/500 - Training Loss: 0.0310 - Validation Loss: 0.0266\n",
      "Epoch 257/500 - Training Loss: 0.0280 - Validation Loss: 0.0297\n",
      "Epoch 258/500 - Training Loss: 0.0321 - Validation Loss: 0.0258\n",
      "Epoch 259/500 - Training Loss: 0.0308 - Validation Loss: 0.0289\n",
      "Epoch 260/500 - Training Loss: 0.0313 - Validation Loss: 0.0260\n",
      "Epoch 261/500 - Training Loss: 0.0285 - Validation Loss: 0.0270\n",
      "Epoch 262/500 - Training Loss: 0.0294 - Validation Loss: 0.0272\n",
      "Epoch 263/500 - Training Loss: 0.0288 - Validation Loss: 0.0260\n",
      "Epoch 264/500 - Training Loss: 0.0297 - Validation Loss: 0.0260\n",
      "Epoch 265/500 - Training Loss: 0.0312 - Validation Loss: 0.0260\n",
      "Epoch 266/500 - Training Loss: 0.0338 - Validation Loss: 0.0261\n",
      "Epoch 267/500 - Training Loss: 0.0301 - Validation Loss: 0.0289\n",
      "Epoch 268/500 - Training Loss: 0.0284 - Validation Loss: 0.0260\n",
      "Epoch 269/500 - Training Loss: 0.0319 - Validation Loss: 0.0262\n",
      "Epoch 270/500 - Training Loss: 0.0270 - Validation Loss: 0.0260\n",
      "Epoch 271/500 - Training Loss: 0.0297 - Validation Loss: 0.0259\n",
      "Epoch 272/500 - Training Loss: 0.0320 - Validation Loss: 0.0267\n",
      "Epoch 273/500 - Training Loss: 0.0317 - Validation Loss: 0.0259\n",
      "Epoch 274/500 - Training Loss: 0.0285 - Validation Loss: 0.0264\n",
      "Epoch 275/500 - Training Loss: 0.0331 - Validation Loss: 0.0294\n",
      "Epoch 276/500 - Training Loss: 0.0336 - Validation Loss: 0.0259\n",
      "Epoch 277/500 - Training Loss: 0.0300 - Validation Loss: 0.0261\n",
      "Epoch 278/500 - Training Loss: 0.0312 - Validation Loss: 0.0270\n",
      "Epoch 279/500 - Training Loss: 0.0300 - Validation Loss: 0.0259\n",
      "Epoch 280/500 - Training Loss: 0.0266 - Validation Loss: 0.0261\n",
      "Epoch 281/500 - Training Loss: 0.0305 - Validation Loss: 0.0260\n",
      "Epoch 282/500 - Training Loss: 0.0322 - Validation Loss: 0.0259\n",
      "Epoch 283/500 - Training Loss: 0.0284 - Validation Loss: 0.0260\n",
      "Epoch 284/500 - Training Loss: 0.0308 - Validation Loss: 0.0299\n",
      "Epoch 285/500 - Training Loss: 0.0310 - Validation Loss: 0.0259\n",
      "Epoch 286/500 - Training Loss: 0.0314 - Validation Loss: 0.0261\n",
      "Epoch 287/500 - Training Loss: 0.0296 - Validation Loss: 0.0264\n",
      "Epoch 288/500 - Training Loss: 0.0280 - Validation Loss: 0.0266\n",
      "Epoch 289/500 - Training Loss: 0.0325 - Validation Loss: 0.0259\n",
      "Epoch 290/500 - Training Loss: 0.0310 - Validation Loss: 0.0280\n",
      "Epoch 291/500 - Training Loss: 0.0273 - Validation Loss: 0.0259\n",
      "Epoch 292/500 - Training Loss: 0.0270 - Validation Loss: 0.0259\n",
      "Epoch 293/500 - Training Loss: 0.0301 - Validation Loss: 0.0266\n",
      "Epoch 294/500 - Training Loss: 0.0332 - Validation Loss: 0.0266\n",
      "Epoch 295/500 - Training Loss: 0.0304 - Validation Loss: 0.0261\n",
      "Epoch 296/500 - Training Loss: 0.0320 - Validation Loss: 0.0260\n",
      "Epoch 297/500 - Training Loss: 0.0331 - Validation Loss: 0.0261\n",
      "Epoch 298/500 - Training Loss: 0.0263 - Validation Loss: 0.0259\n",
      "Epoch 299/500 - Training Loss: 0.0293 - Validation Loss: 0.0259\n",
      "Epoch 300/500 - Training Loss: 0.0290 - Validation Loss: 0.0261\n",
      "Epoch 301/500 - Training Loss: 0.0271 - Validation Loss: 0.0261\n",
      "Epoch 302/500 - Training Loss: 0.0306 - Validation Loss: 0.0259\n",
      "Epoch 303/500 - Training Loss: 0.0327 - Validation Loss: 0.0316\n",
      "Epoch 304/500 - Training Loss: 0.0263 - Validation Loss: 0.0264\n",
      "Epoch 305/500 - Training Loss: 0.0310 - Validation Loss: 0.0261\n",
      "Epoch 306/500 - Training Loss: 0.0321 - Validation Loss: 0.0268\n",
      "Epoch 307/500 - Training Loss: 0.0304 - Validation Loss: 0.0258\n",
      "Epoch 308/500 - Training Loss: 0.0276 - Validation Loss: 0.0260\n",
      "Epoch 309/500 - Training Loss: 0.0300 - Validation Loss: 0.0258\n",
      "Epoch 310/500 - Training Loss: 0.0290 - Validation Loss: 0.0272\n",
      "Epoch 311/500 - Training Loss: 0.0293 - Validation Loss: 0.0263\n",
      "Epoch 312/500 - Training Loss: 0.0308 - Validation Loss: 0.0283\n",
      "Epoch 313/500 - Training Loss: 0.0290 - Validation Loss: 0.0261\n",
      "Epoch 314/500 - Training Loss: 0.0321 - Validation Loss: 0.0259\n",
      "Epoch 315/500 - Training Loss: 0.0288 - Validation Loss: 0.0264\n",
      "Epoch 316/500 - Training Loss: 0.0312 - Validation Loss: 0.0261\n",
      "Epoch 317/500 - Training Loss: 0.0285 - Validation Loss: 0.0261\n",
      "Epoch 318/500 - Training Loss: 0.0319 - Validation Loss: 0.0259\n",
      "Epoch 319/500 - Training Loss: 0.0294 - Validation Loss: 0.0259\n",
      "Epoch 320/500 - Training Loss: 0.0267 - Validation Loss: 0.0260\n",
      "Epoch 321/500 - Training Loss: 0.0298 - Validation Loss: 0.0281\n",
      "Epoch 322/500 - Training Loss: 0.0282 - Validation Loss: 0.0265\n",
      "Epoch 323/500 - Training Loss: 0.0284 - Validation Loss: 0.0259\n",
      "Epoch 324/500 - Training Loss: 0.0275 - Validation Loss: 0.0258\n",
      "Epoch 325/500 - Training Loss: 0.0291 - Validation Loss: 0.0259\n",
      "Epoch 326/500 - Training Loss: 0.0276 - Validation Loss: 0.0258\n",
      "Epoch 327/500 - Training Loss: 0.0269 - Validation Loss: 0.0270\n",
      "Epoch 328/500 - Training Loss: 0.0285 - Validation Loss: 0.0259\n",
      "Epoch 329/500 - Training Loss: 0.0280 - Validation Loss: 0.0260\n",
      "Epoch 330/500 - Training Loss: 0.0296 - Validation Loss: 0.0258\n",
      "Epoch 331/500 - Training Loss: 0.0303 - Validation Loss: 0.0290\n",
      "Epoch 332/500 - Training Loss: 0.0264 - Validation Loss: 0.0264\n",
      "Epoch 333/500 - Training Loss: 0.0292 - Validation Loss: 0.0263\n",
      "Epoch 334/500 - Training Loss: 0.0259 - Validation Loss: 0.0261\n",
      "Epoch 335/500 - Training Loss: 0.0505 - Validation Loss: 0.0271\n",
      "Epoch 336/500 - Training Loss: 0.0292 - Validation Loss: 0.0277\n",
      "Epoch 337/500 - Training Loss: 0.0294 - Validation Loss: 0.0259\n",
      "Epoch 338/500 - Training Loss: 0.0288 - Validation Loss: 0.0259\n",
      "Epoch 339/500 - Training Loss: 0.0274 - Validation Loss: 0.0267\n",
      "Epoch 340/500 - Training Loss: 0.0303 - Validation Loss: 0.0259\n",
      "Epoch 341/500 - Training Loss: 0.0280 - Validation Loss: 0.0259\n",
      "Epoch 342/500 - Training Loss: 0.0265 - Validation Loss: 0.0270\n",
      "Epoch 343/500 - Training Loss: 0.0282 - Validation Loss: 0.0262\n",
      "Epoch 344/500 - Training Loss: 0.0265 - Validation Loss: 0.0260\n",
      "Epoch 345/500 - Training Loss: 0.0268 - Validation Loss: 0.0260\n",
      "Epoch 346/500 - Training Loss: 0.0286 - Validation Loss: 0.0258\n",
      "Epoch 347/500 - Training Loss: 0.0264 - Validation Loss: 0.0260\n",
      "Epoch 348/500 - Training Loss: 0.0269 - Validation Loss: 0.0259\n",
      "Epoch 349/500 - Training Loss: 0.0272 - Validation Loss: 0.0264\n",
      "Epoch 350/500 - Training Loss: 0.0250 - Validation Loss: 0.0258\n",
      "Epoch 351/500 - Training Loss: 0.0254 - Validation Loss: 0.0260\n",
      "Epoch 352/500 - Training Loss: 0.0278 - Validation Loss: 0.0259\n",
      "Epoch 353/500 - Training Loss: 0.0260 - Validation Loss: 0.0258\n",
      "Epoch 354/500 - Training Loss: 0.0246 - Validation Loss: 0.0259\n",
      "Epoch 355/500 - Training Loss: 0.0264 - Validation Loss: 0.0258\n",
      "Epoch 356/500 - Training Loss: 0.0275 - Validation Loss: 0.0259\n",
      "Epoch 357/500 - Training Loss: 0.0272 - Validation Loss: 0.0259\n",
      "Epoch 358/500 - Training Loss: 0.0256 - Validation Loss: 0.0265\n",
      "Epoch 359/500 - Training Loss: 0.0261 - Validation Loss: 0.0260\n",
      "Epoch 360/500 - Training Loss: 0.0248 - Validation Loss: 0.0258\n",
      "Epoch 361/500 - Training Loss: 0.0249 - Validation Loss: 0.0259\n",
      "Epoch 362/500 - Training Loss: 0.0248 - Validation Loss: 0.0259\n",
      "Epoch 363/500 - Training Loss: 0.0247 - Validation Loss: 0.0259\n",
      "Epoch 364/500 - Training Loss: 0.0252 - Validation Loss: 0.0262\n",
      "Epoch 365/500 - Training Loss: 0.0243 - Validation Loss: 0.0259\n",
      "Epoch 366/500 - Training Loss: 0.0255 - Validation Loss: 0.0263\n",
      "Epoch 367/500 - Training Loss: 0.0262 - Validation Loss: 0.0258\n",
      "Epoch 368/500 - Training Loss: 0.0234 - Validation Loss: 0.0260\n",
      "Epoch 369/500 - Training Loss: 0.0274 - Validation Loss: 0.0258\n",
      "Epoch 370/500 - Training Loss: 0.0251 - Validation Loss: 0.0258\n",
      "Epoch 371/500 - Training Loss: 0.0252 - Validation Loss: 0.0258\n",
      "Epoch 372/500 - Training Loss: 0.0230 - Validation Loss: 0.0258\n",
      "Epoch 373/500 - Training Loss: 0.0238 - Validation Loss: 0.0258\n",
      "Epoch 374/500 - Training Loss: 0.0247 - Validation Loss: 0.0260\n",
      "Epoch 375/500 - Training Loss: 0.0247 - Validation Loss: 0.0260\n",
      "Epoch 376/500 - Training Loss: 0.0242 - Validation Loss: 0.0258\n",
      "Epoch 377/500 - Training Loss: 0.0233 - Validation Loss: 0.0258\n",
      "Epoch 378/500 - Training Loss: 0.0241 - Validation Loss: 0.0259\n",
      "Epoch 379/500 - Training Loss: 0.0253 - Validation Loss: 0.0259\n",
      "Epoch 380/500 - Training Loss: 0.0242 - Validation Loss: 0.0258\n",
      "Epoch 381/500 - Training Loss: 0.0228 - Validation Loss: 0.0258\n",
      "Epoch 382/500 - Training Loss: 0.0234 - Validation Loss: 0.0258\n",
      "Epoch 383/500 - Training Loss: 0.0248 - Validation Loss: 0.0258\n",
      "Epoch 384/500 - Training Loss: 0.0242 - Validation Loss: 0.0259\n",
      "Epoch 385/500 - Training Loss: 0.0245 - Validation Loss: 0.0259\n",
      "Epoch 386/500 - Training Loss: 0.0247 - Validation Loss: 0.0258\n",
      "Epoch 387/500 - Training Loss: 0.0238 - Validation Loss: 0.0258\n",
      "Epoch 388/500 - Training Loss: 0.0244 - Validation Loss: 0.0260\n",
      "Epoch 389/500 - Training Loss: 0.0228 - Validation Loss: 0.0259\n",
      "Epoch 390/500 - Training Loss: 0.0234 - Validation Loss: 0.0260\n",
      "Epoch 391/500 - Training Loss: 0.0238 - Validation Loss: 0.0259\n",
      "Epoch 392/500 - Training Loss: 0.0239 - Validation Loss: 0.0258\n",
      "Epoch 393/500 - Training Loss: 0.0239 - Validation Loss: 0.0258\n",
      "Epoch 394/500 - Training Loss: 0.0232 - Validation Loss: 0.0258\n",
      "Epoch 395/500 - Training Loss: 0.0236 - Validation Loss: 0.0258\n",
      "Epoch 396/500 - Training Loss: 0.0224 - Validation Loss: 0.0258\n",
      "Epoch 397/500 - Training Loss: 0.0230 - Validation Loss: 0.0260\n",
      "Epoch 398/500 - Training Loss: 0.0234 - Validation Loss: 0.0258\n",
      "Epoch 399/500 - Training Loss: 0.0235 - Validation Loss: 0.0258\n",
      "Epoch 400/500 - Training Loss: 0.0231 - Validation Loss: 0.0259\n",
      "Epoch 401/500 - Training Loss: 0.0237 - Validation Loss: 0.0258\n",
      "Epoch 402/500 - Training Loss: 0.0240 - Validation Loss: 0.0260\n",
      "Epoch 403/500 - Training Loss: 0.0234 - Validation Loss: 0.0259\n",
      "Epoch 404/500 - Training Loss: 0.0219 - Validation Loss: 0.0258\n",
      "Epoch 405/500 - Training Loss: 0.0224 - Validation Loss: 0.0258\n",
      "Epoch 406/500 - Training Loss: 0.0230 - Validation Loss: 0.0259\n",
      "Epoch 407/500 - Training Loss: 0.0221 - Validation Loss: 0.0259\n",
      "Epoch 408/500 - Training Loss: 0.0225 - Validation Loss: 0.0259\n",
      "Epoch 409/500 - Training Loss: 0.0230 - Validation Loss: 0.0259\n",
      "Epoch 410/500 - Training Loss: 0.0238 - Validation Loss: 0.0259\n",
      "Epoch 411/500 - Training Loss: 0.0223 - Validation Loss: 0.0258\n",
      "Epoch 412/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 413/500 - Training Loss: 0.0226 - Validation Loss: 0.0258\n",
      "Epoch 414/500 - Training Loss: 0.0232 - Validation Loss: 0.0258\n",
      "Epoch 415/500 - Training Loss: 0.0227 - Validation Loss: 0.0258\n",
      "Epoch 416/500 - Training Loss: 0.0226 - Validation Loss: 0.0258\n",
      "Epoch 417/500 - Training Loss: 0.0219 - Validation Loss: 0.0258\n",
      "Epoch 418/500 - Training Loss: 0.0221 - Validation Loss: 0.0258\n",
      "Epoch 419/500 - Training Loss: 0.0220 - Validation Loss: 0.0259\n",
      "Epoch 420/500 - Training Loss: 0.0223 - Validation Loss: 0.0258\n",
      "Epoch 421/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 422/500 - Training Loss: 0.0223 - Validation Loss: 0.0258\n",
      "Epoch 423/500 - Training Loss: 0.0224 - Validation Loss: 0.0258\n",
      "Epoch 424/500 - Training Loss: 0.0224 - Validation Loss: 0.0258\n",
      "Epoch 425/500 - Training Loss: 0.0226 - Validation Loss: 0.0259\n",
      "Epoch 426/500 - Training Loss: 0.0228 - Validation Loss: 0.0259\n",
      "Epoch 427/500 - Training Loss: 0.0224 - Validation Loss: 0.0259\n",
      "Epoch 428/500 - Training Loss: 0.0223 - Validation Loss: 0.0258\n",
      "Epoch 429/500 - Training Loss: 0.0224 - Validation Loss: 0.0258\n",
      "Epoch 430/500 - Training Loss: 0.0222 - Validation Loss: 0.0258\n",
      "Epoch 431/500 - Training Loss: 0.0223 - Validation Loss: 0.0258\n",
      "Epoch 432/500 - Training Loss: 0.0219 - Validation Loss: 0.0259\n",
      "Epoch 433/500 - Training Loss: 0.0221 - Validation Loss: 0.0258\n",
      "Epoch 434/500 - Training Loss: 0.0222 - Validation Loss: 0.0258\n",
      "Epoch 435/500 - Training Loss: 0.0224 - Validation Loss: 0.0259\n",
      "Epoch 436/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 437/500 - Training Loss: 0.0221 - Validation Loss: 0.0258\n",
      "Epoch 438/500 - Training Loss: 0.0216 - Validation Loss: 0.0258\n",
      "Epoch 439/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 440/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 441/500 - Training Loss: 0.0221 - Validation Loss: 0.0258\n",
      "Epoch 442/500 - Training Loss: 0.0221 - Validation Loss: 0.0259\n",
      "Epoch 443/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 444/500 - Training Loss: 0.0248 - Validation Loss: 0.0258\n",
      "Epoch 445/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 446/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 447/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 448/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 449/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 450/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 451/500 - Training Loss: 0.0219 - Validation Loss: 0.0258\n",
      "Epoch 452/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 453/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 454/500 - Training Loss: 0.0219 - Validation Loss: 0.0258\n",
      "Epoch 455/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 456/500 - Training Loss: 0.0215 - Validation Loss: 0.0258\n",
      "Epoch 457/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 458/500 - Training Loss: 0.0220 - Validation Loss: 0.0258\n",
      "Epoch 459/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 460/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 461/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 462/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 463/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 464/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 465/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 466/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 467/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 468/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 469/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 470/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 471/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 472/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 473/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 474/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 475/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 476/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 477/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 478/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 479/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 480/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 481/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 482/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 483/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 484/500 - Training Loss: 0.0219 - Validation Loss: 0.0260\n",
      "Epoch 485/500 - Training Loss: 0.0218 - Validation Loss: 0.0258\n",
      "Epoch 486/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 487/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 488/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 489/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:42:08,091] A new study created in memory with name: no-name-0518230b-c73e-41f5-b3fd-ca7a74c66c83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 491/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 492/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 493/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 494/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 495/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 496/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 497/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 498/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 499/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Epoch 500/500 - Training Loss: 0.0217 - Validation Loss: 0.0258\n",
      "Training of the best model completed.\n",
      "\n",
      "===== Model Performance =====\n",
      "Training Set:\n",
      "  R2 Score: -0.2745\n",
      "  MSE: 0.0172\n",
      "  MAE: 0.0951\n",
      "\n",
      "Testing Set:\n",
      "  R2 Score: -6608829440.0000\n",
      "  MSE: 0.0205\n",
      "  MAE: 0.1010\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:42:09,159] Trial 0 finished with value: -0.8677326440811157 and parameters: {'hidden_layers': 5, 'n_units_l0': 600, 'n_units_l1': 269, 'n_units_l2': 298, 'n_units_l3': 650, 'n_units_l4': 626, 'dropout_rate': 0.4952988436076984, 'lr': 0.0005985687798153047, 'optimizer': 'Adam'}. Best is trial 0 with value: -0.8677326440811157.\n",
      "[I 2024-09-25 14:42:09,721] Trial 1 finished with value: -98.9993896484375 and parameters: {'hidden_layers': 2, 'n_units_l0': 388, 'n_units_l1': 544, 'dropout_rate': 0.2149027571072224, 'lr': 0.00021272667742181363, 'optimizer': 'SGD'}. Best is trial 0 with value: -0.8677326440811157.\n",
      "[I 2024-09-25 14:42:10,422] Trial 2 finished with value: 0.3867229223251343 and parameters: {'hidden_layers': 2, 'n_units_l0': 229, 'n_units_l1': 296, 'dropout_rate': 0.3100600033896034, 'lr': 0.0019108963200206838, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:11,373] Trial 3 finished with value: 0.20448186993598938 and parameters: {'hidden_layers': 5, 'n_units_l0': 765, 'n_units_l1': 659, 'n_units_l2': 162, 'n_units_l3': 262, 'n_units_l4': 152, 'dropout_rate': 0.3968881447242069, 'lr': 0.0007062090768435262, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:12,327] Trial 4 finished with value: -2.7364511489868164 and parameters: {'hidden_layers': 3, 'n_units_l0': 594, 'n_units_l1': 826, 'n_units_l2': 836, 'dropout_rate': 0.25934075085914393, 'lr': 9.018379370414488e-05, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:13,173] Trial 5 finished with value: -33.46036911010742 and parameters: {'hidden_layers': 4, 'n_units_l0': 218, 'n_units_l1': 881, 'n_units_l2': 1005, 'n_units_l3': 890, 'dropout_rate': 0.2749344373196465, 'lr': 0.0028202359720206236, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:13,973] Trial 6 finished with value: -2.511883020401001 and parameters: {'hidden_layers': 3, 'n_units_l0': 435, 'n_units_l1': 361, 'n_units_l2': 743, 'dropout_rate': 0.25830840527381926, 'lr': 7.439206706021379e-05, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:14,784] Trial 7 finished with value: -21.864652633666992 and parameters: {'hidden_layers': 4, 'n_units_l0': 688, 'n_units_l1': 658, 'n_units_l2': 919, 'n_units_l3': 866, 'dropout_rate': 0.5203233274459446, 'lr': 3.476731221173007e-05, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:15,288] Trial 8 finished with value: -116.73056030273438 and parameters: {'hidden_layers': 2, 'n_units_l0': 418, 'n_units_l1': 914, 'dropout_rate': 0.30759333024958035, 'lr': 0.0063197130257433105, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:16,074] Trial 9 finished with value: -42.73164749145508 and parameters: {'hidden_layers': 5, 'n_units_l0': 821, 'n_units_l1': 901, 'n_units_l2': 645, 'n_units_l3': 293, 'n_units_l4': 296, 'dropout_rate': 0.4111955795762334, 'lr': 0.000502852973810685, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:16,727] Trial 10 finished with value: -232.46949768066406 and parameters: {'hidden_layers': 2, 'n_units_l0': 145, 'n_units_l1': 147, 'dropout_rate': 0.35540063773394404, 'lr': 1.1003241672175049e-05, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:17,641] Trial 11 finished with value: 0.35907602310180664 and parameters: {'hidden_layers': 4, 'n_units_l0': 922, 'n_units_l1': 544, 'n_units_l2': 133, 'n_units_l3': 145, 'dropout_rate': 0.4059925914725506, 'lr': 0.0016572854182699231, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:18,437] Trial 12 finished with value: 0.22070325911045074 and parameters: {'hidden_layers': 3, 'n_units_l0': 1017, 'n_units_l1': 453, 'n_units_l2': 431, 'dropout_rate': 0.4534352197788164, 'lr': 0.0021610808088063553, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:19,261] Trial 13 finished with value: -1.762922763824463 and parameters: {'hidden_layers': 4, 'n_units_l0': 986, 'n_units_l1': 136, 'n_units_l2': 448, 'n_units_l3': 181, 'dropout_rate': 0.34700692490569296, 'lr': 0.001947105129445378, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.3867229223251343.\n",
      "[I 2024-09-25 14:42:20,014] Trial 14 finished with value: 0.46871745586395264 and parameters: {'hidden_layers': 3, 'n_units_l0': 306, 'n_units_l1': 326, 'n_units_l2': 146, 'dropout_rate': 0.589035864761455, 'lr': 0.004881001945007912, 'optimizer': 'AdamW'}. Best is trial 14 with value: 0.46871745586395264.\n",
      "[I 2024-09-25 14:42:20,717] Trial 15 finished with value: 0.6201358437538147 and parameters: {'hidden_layers': 2, 'n_units_l0': 322, 'n_units_l1': 304, 'dropout_rate': 0.5917753568765097, 'lr': 0.008886425500693124, 'optimizer': 'AdamW'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:21,542] Trial 16 finished with value: -20.582317352294922 and parameters: {'hidden_layers': 3, 'n_units_l0': 300, 'n_units_l1': 407, 'n_units_l2': 507, 'dropout_rate': 0.5890094752204241, 'lr': 0.008734777466869126, 'optimizer': 'RMSprop'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:22,248] Trial 17 finished with value: 0.1907467395067215 and parameters: {'hidden_layers': 2, 'n_units_l0': 493, 'n_units_l1': 245, 'dropout_rate': 0.5946120241482549, 'lr': 0.004900247822133796, 'optimizer': 'AdamW'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:23,071] Trial 18 finished with value: 0.4188728630542755 and parameters: {'hidden_layers': 3, 'n_units_l0': 314, 'n_units_l1': 455, 'n_units_l2': 288, 'dropout_rate': 0.5327104133507948, 'lr': 0.004439237876882775, 'optimizer': 'AdamW'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:23,715] Trial 19 finished with value: 0.47419261932373047 and parameters: {'hidden_layers': 2, 'n_units_l0': 134, 'n_units_l1': 216, 'dropout_rate': 0.566607840948145, 'lr': 0.009706832384732841, 'optimizer': 'AdamW'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:24,387] Trial 20 finished with value: -4.747469902038574 and parameters: {'hidden_layers': 2, 'n_units_l0': 160, 'n_units_l1': 200, 'dropout_rate': 0.5351506604766021, 'lr': 0.000983134980391385, 'optimizer': 'RMSprop'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:25,246] Trial 21 finished with value: 0.5806789398193359 and parameters: {'hidden_layers': 2, 'n_units_l0': 298, 'n_units_l1': 318, 'dropout_rate': 0.5670572202206617, 'lr': 0.009763710612233219, 'optimizer': 'AdamW'}. Best is trial 15 with value: 0.6201358437538147.\n",
      "[I 2024-09-25 14:42:25,949] Trial 22 finished with value: 0.6222487688064575 and parameters: {'hidden_layers': 2, 'n_units_l0': 234, 'n_units_l1': 214, 'dropout_rate': 0.47996224627324985, 'lr': 0.008607402631923274, 'optimizer': 'AdamW'}. Best is trial 22 with value: 0.6222487688064575.\n",
      "[I 2024-09-25 14:42:26,828] Trial 23 finished with value: 0.6257026791572571 and parameters: {'hidden_layers': 2, 'n_units_l0': 521, 'n_units_l1': 397, 'dropout_rate': 0.4851342986228304, 'lr': 0.009466966624000818, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:27,519] Trial 24 finished with value: 0.5015473365783691 and parameters: {'hidden_layers': 2, 'n_units_l0': 519, 'n_units_l1': 393, 'dropout_rate': 0.4833831095429627, 'lr': 0.0036174561779735415, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:28,193] Trial 25 finished with value: -0.2802734673023224 and parameters: {'hidden_layers': 2, 'n_units_l0': 395, 'n_units_l1': 471, 'dropout_rate': 0.46107353664976386, 'lr': 0.0010236550864853268, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:28,974] Trial 26 finished with value: -2.046297550201416 and parameters: {'hidden_layers': 3, 'n_units_l0': 670, 'n_units_l1': 204, 'n_units_l2': 636, 'dropout_rate': 0.4436370063060644, 'lr': 0.0001995980799671002, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:29,715] Trial 27 finished with value: 0.5354801416397095 and parameters: {'hidden_layers': 2, 'n_units_l0': 492, 'n_units_l1': 1012, 'dropout_rate': 0.5045823907230664, 'lr': 0.006257499875965825, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:30,534] Trial 28 finished with value: 0.25606393814086914 and parameters: {'hidden_layers': 3, 'n_units_l0': 215, 'n_units_l1': 658, 'n_units_l2': 775, 'dropout_rate': 0.5452284369234657, 'lr': 0.0030934784635603505, 'optimizer': 'Adam'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:31,187] Trial 29 finished with value: 0.057504478842020035 and parameters: {'hidden_layers': 2, 'n_units_l0': 651, 'n_units_l1': 269, 'dropout_rate': 0.4839805246536305, 'lr': 0.00041267725676437763, 'optimizer': 'RMSprop'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:31,861] Trial 30 finished with value: 0.5620904564857483 and parameters: {'hidden_layers': 2, 'n_units_l0': 554, 'n_units_l1': 350, 'dropout_rate': 0.4409926444763725, 'lr': 0.006697179969686351, 'optimizer': 'Adam'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:32,558] Trial 31 finished with value: 0.6097941994667053 and parameters: {'hidden_layers': 2, 'n_units_l0': 328, 'n_units_l1': 312, 'dropout_rate': 0.56271681689007, 'lr': 0.008824816925834705, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:33,249] Trial 32 finished with value: 0.6164396405220032 and parameters: {'hidden_layers': 2, 'n_units_l0': 364, 'n_units_l1': 274, 'dropout_rate': 0.5615425349370572, 'lr': 0.00977765083728966, 'optimizer': 'AdamW'}. Best is trial 23 with value: 0.6257026791572571.\n",
      "[I 2024-09-25 14:42:33,941] Trial 33 finished with value: 0.6329677104949951 and parameters: {'hidden_layers': 2, 'n_units_l0': 375, 'n_units_l1': 184, 'dropout_rate': 0.5060783509624958, 'lr': 0.0033394770881992617, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:34,624] Trial 34 finished with value: 0.606996476650238 and parameters: {'hidden_layers': 2, 'n_units_l0': 238, 'n_units_l1': 179, 'dropout_rate': 0.5051202586656233, 'lr': 0.003155092452465493, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:35,468] Trial 35 finished with value: 0.1406150460243225 and parameters: {'hidden_layers': 2, 'n_units_l0': 454, 'n_units_l1': 543, 'dropout_rate': 0.48288657259803797, 'lr': 0.0011003384149390369, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:36,248] Trial 36 finished with value: 0.46932926774024963 and parameters: {'hidden_layers': 3, 'n_units_l0': 363, 'n_units_l1': 236, 'n_units_l2': 316, 'dropout_rate': 0.41629397815176383, 'lr': 0.0056581496741890435, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:36,967] Trial 37 finished with value: 0.13998079299926758 and parameters: {'hidden_layers': 2, 'n_units_l0': 238, 'n_units_l1': 389, 'dropout_rate': 0.38280752083941266, 'lr': 0.00137846632997202, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:38,022] Trial 38 finished with value: -0.4676472842693329 and parameters: {'hidden_layers': 5, 'n_units_l0': 562, 'n_units_l1': 131, 'n_units_l2': 1012, 'n_units_l3': 517, 'n_units_l4': 1011, 'dropout_rate': 0.4637613602151057, 'lr': 0.0002281021979543268, 'optimizer': 'Adam'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:38,674] Trial 39 finished with value: -61.64186477661133 and parameters: {'hidden_layers': 3, 'n_units_l0': 446, 'n_units_l1': 494, 'n_units_l2': 558, 'dropout_rate': 0.21521447460731538, 'lr': 0.0023872398783748475, 'optimizer': 'SGD'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:39,409] Trial 40 finished with value: 0.5564448237419128 and parameters: {'hidden_layers': 2, 'n_units_l0': 261, 'n_units_l1': 283, 'dropout_rate': 0.5134858148764961, 'lr': 0.003992672644192654, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:40,097] Trial 41 finished with value: 0.33757728338241577 and parameters: {'hidden_layers': 2, 'n_units_l0': 366, 'n_units_l1': 270, 'dropout_rate': 0.5458420783217083, 'lr': 0.0069730172867151965, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:40,787] Trial 42 finished with value: 0.6292725801467896 and parameters: {'hidden_layers': 2, 'n_units_l0': 357, 'n_units_l1': 187, 'dropout_rate': 0.5242024612653727, 'lr': 0.009785356119216276, 'optimizer': 'AdamW'}. Best is trial 33 with value: 0.6329677104949951.\n",
      "[I 2024-09-25 14:42:41,467] Trial 43 finished with value: 0.6366471648216248 and parameters: {'hidden_layers': 2, 'n_units_l0': 187, 'n_units_l1': 176, 'dropout_rate': 0.49128503518357364, 'lr': 0.00701342823531647, 'optimizer': 'AdamW'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:42,173] Trial 44 finished with value: 0.6112455725669861 and parameters: {'hidden_layers': 2, 'n_units_l0': 196, 'n_units_l1': 198, 'dropout_rate': 0.4344202614682995, 'lr': 0.004005626856751786, 'optimizer': 'AdamW'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:42,692] Trial 45 finished with value: -322.4741516113281 and parameters: {'hidden_layers': 2, 'n_units_l0': 170, 'n_units_l1': 165, 'dropout_rate': 0.473438068563302, 'lr': 0.0062862261772901286, 'optimizer': 'SGD'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:43,496] Trial 46 finished with value: 0.3420979678630829 and parameters: {'hidden_layers': 2, 'n_units_l0': 756, 'n_units_l1': 784, 'dropout_rate': 0.4964930348827402, 'lr': 0.0024855272471680507, 'optimizer': 'AdamW'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:44,274] Trial 47 finished with value: -1.1120883226394653 and parameters: {'hidden_layers': 3, 'n_units_l0': 616, 'n_units_l1': 168, 'n_units_l2': 869, 'dropout_rate': 0.521930397880173, 'lr': 0.005219879076026454, 'optimizer': 'AdamW'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:45,195] Trial 48 finished with value: -4.605123043060303 and parameters: {'hidden_layers': 4, 'n_units_l0': 268, 'n_units_l1': 366, 'n_units_l2': 714, 'n_units_l3': 1018, 'dropout_rate': 0.3890833461404161, 'lr': 6.254356039985208e-05, 'optimizer': 'Adam'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:45,825] Trial 49 finished with value: -6.813472270965576 and parameters: {'hidden_layers': 2, 'n_units_l0': 403, 'n_units_l1': 241, 'dropout_rate': 0.4320719247572866, 'lr': 0.00681634935354966, 'optimizer': 'RMSprop'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:46,494] Trial 50 finished with value: -115.72245025634766 and parameters: {'hidden_layers': 3, 'n_units_l0': 188, 'n_units_l1': 592, 'n_units_l2': 407, 'dropout_rate': 0.5212524130972804, 'lr': 0.0015373269849632233, 'optimizer': 'SGD'}. Best is trial 43 with value: 0.6366471648216248.\n",
      "[I 2024-09-25 14:42:47,174] Trial 51 finished with value: 0.6616662740707397 and parameters: {'hidden_layers': 2, 'n_units_l0': 344, 'n_units_l1': 229, 'dropout_rate': 0.4973352674986709, 'lr': 0.007331673107464598, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:47,894] Trial 52 finished with value: 0.5928400158882141 and parameters: {'hidden_layers': 2, 'n_units_l0': 352, 'n_units_l1': 231, 'dropout_rate': 0.49285189839518145, 'lr': 0.007511782448031875, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:48,532] Trial 53 finished with value: 0.5910085439682007 and parameters: {'hidden_layers': 2, 'n_units_l0': 267, 'n_units_l1': 136, 'dropout_rate': 0.46391284117251186, 'lr': 0.004966850568514826, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:49,270] Trial 54 finished with value: -133.91270446777344 and parameters: {'hidden_layers': 2, 'n_units_l0': 458, 'n_units_l1': 165, 'dropout_rate': 0.533853914417724, 'lr': 1.0574288964067891e-05, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:50,049] Trial 55 finished with value: 0.5689021944999695 and parameters: {'hidden_layers': 2, 'n_units_l0': 414, 'n_units_l1': 424, 'dropout_rate': 0.5120580240029592, 'lr': 0.003287537318563135, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:50,705] Trial 56 finished with value: 0.5369439125061035 and parameters: {'hidden_layers': 2, 'n_units_l0': 274, 'n_units_l1': 350, 'dropout_rate': 0.4204795729383914, 'lr': 0.007956283963406174, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:51,365] Trial 57 finished with value: 0.633366584777832 and parameters: {'hidden_layers': 2, 'n_units_l0': 498, 'n_units_l1': 204, 'dropout_rate': 0.4771967957210353, 'lr': 0.0043418037596259906, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:52,042] Trial 58 finished with value: -0.9753169417381287 and parameters: {'hidden_layers': 2, 'n_units_l0': 521, 'n_units_l1': 253, 'dropout_rate': 0.4964247846057401, 'lr': 0.0019347539637588442, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:53,174] Trial 59 finished with value: -7.404814720153809 and parameters: {'hidden_layers': 5, 'n_units_l0': 621, 'n_units_l1': 192, 'n_units_l2': 225, 'n_units_l3': 484, 'n_units_l4': 1003, 'dropout_rate': 0.4509166288422675, 'lr': 2.5690714291297595e-05, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:53,958] Trial 60 finished with value: -0.8716229796409607 and parameters: {'hidden_layers': 3, 'n_units_l0': 521, 'n_units_l1': 297, 'n_units_l2': 649, 'dropout_rate': 0.3721767655884146, 'lr': 0.000617228485223339, 'optimizer': 'RMSprop'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:54,615] Trial 61 finished with value: 0.42719927430152893 and parameters: {'hidden_layers': 2, 'n_units_l0': 474, 'n_units_l1': 212, 'dropout_rate': 0.473626271801131, 'lr': 0.004517461827235222, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:55,275] Trial 62 finished with value: 0.6206772923469543 and parameters: {'hidden_layers': 2, 'n_units_l0': 427, 'n_units_l1': 131, 'dropout_rate': 0.5428251576427798, 'lr': 0.005568838947373776, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:55,967] Trial 63 finished with value: 0.5905681252479553 and parameters: {'hidden_layers': 2, 'n_units_l0': 341, 'n_units_l1': 219, 'dropout_rate': 0.4737638385091698, 'lr': 0.007777935045829052, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:56,672] Trial 64 finished with value: 0.5350117683410645 and parameters: {'hidden_layers': 2, 'n_units_l0': 387, 'n_units_l1': 163, 'dropout_rate': 0.5239205125669164, 'lr': 0.002746784373755038, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:57,373] Trial 65 finished with value: 0.5740001797676086 and parameters: {'hidden_layers': 2, 'n_units_l0': 303, 'n_units_l1': 328, 'dropout_rate': 0.49547478326551714, 'lr': 0.0036572545500786592, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:58,086] Trial 66 finished with value: 0.619024932384491 and parameters: {'hidden_layers': 2, 'n_units_l0': 128, 'n_units_l1': 257, 'dropout_rate': 0.5544971016324804, 'lr': 0.009800646763081467, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:58,867] Trial 67 finished with value: 0.5845806002616882 and parameters: {'hidden_layers': 2, 'n_units_l0': 586, 'n_units_l1': 189, 'dropout_rate': 0.5771872856492206, 'lr': 0.005844097634685988, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:42:59,584] Trial 68 finished with value: 0.6493374109268188 and parameters: {'hidden_layers': 2, 'n_units_l0': 217, 'n_units_l1': 228, 'dropout_rate': 0.31389875914945287, 'lr': 0.004560864497792397, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:00,284] Trial 69 finished with value: 0.5280495882034302 and parameters: {'hidden_layers': 2, 'n_units_l0': 493, 'n_units_l1': 295, 'dropout_rate': 0.32390059448705627, 'lr': 0.0045479675461023985, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:00,910] Trial 70 finished with value: 0.536990225315094 and parameters: {'hidden_layers': 2, 'n_units_l0': 209, 'n_units_l1': 155, 'dropout_rate': 0.2747914446740318, 'lr': 0.003568994816711013, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:01,543] Trial 71 finished with value: 0.5862531065940857 and parameters: {'hidden_layers': 2, 'n_units_l0': 184, 'n_units_l1': 218, 'dropout_rate': 0.482548840535053, 'lr': 0.0077670087073251235, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:02,274] Trial 72 finished with value: 0.5199151635169983 and parameters: {'hidden_layers': 2, 'n_units_l0': 239, 'n_units_l1': 189, 'dropout_rate': 0.23223719983411684, 'lr': 0.0060212145234053655, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:03,049] Trial 73 finished with value: 0.5973767042160034 and parameters: {'hidden_layers': 2, 'n_units_l0': 288, 'n_units_l1': 237, 'dropout_rate': 0.3016463087696491, 'lr': 0.009773780651911859, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:03,620] Trial 74 finished with value: -431.022705078125 and parameters: {'hidden_layers': 2, 'n_units_l0': 164, 'n_units_l1': 269, 'dropout_rate': 0.5110274399324924, 'lr': 0.008126444584830831, 'optimizer': 'SGD'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:04,371] Trial 75 finished with value: 0.4974897503852844 and parameters: {'hidden_layers': 2, 'n_units_l0': 224, 'n_units_l1': 332, 'dropout_rate': 0.4635043313906219, 'lr': 0.004393084683845343, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:05,067] Trial 76 finished with value: 0.6195898056030273 and parameters: {'hidden_layers': 2, 'n_units_l0': 321, 'n_units_l1': 184, 'dropout_rate': 0.4518937552140455, 'lr': 0.0030545854524421123, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:05,860] Trial 77 finished with value: -12.697590827941895 and parameters: {'hidden_layers': 3, 'n_units_l0': 248, 'n_units_l1': 153, 'n_units_l2': 385, 'dropout_rate': 0.3544122999033198, 'lr': 0.006705548980310292, 'optimizer': 'RMSprop'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:06,873] Trial 78 finished with value: -0.0036951452493667603 and parameters: {'hidden_layers': 4, 'n_units_l0': 381, 'n_units_l1': 618, 'n_units_l2': 501, 'n_units_l3': 650, 'dropout_rate': 0.48834520596332764, 'lr': 0.0023571307989275637, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:07,546] Trial 79 finished with value: 0.33907070755958557 and parameters: {'hidden_layers': 2, 'n_units_l0': 546, 'n_units_l1': 499, 'dropout_rate': 0.5048423223276906, 'lr': 0.005294999896198978, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:08,248] Trial 80 finished with value: 0.4372027814388275 and parameters: {'hidden_layers': 2, 'n_units_l0': 885, 'n_units_l1': 214, 'dropout_rate': 0.40263203713499846, 'lr': 0.008297061430915117, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:08,904] Trial 81 finished with value: 0.5543854832649231 and parameters: {'hidden_layers': 2, 'n_units_l0': 445, 'n_units_l1': 128, 'dropout_rate': 0.5498167754801715, 'lr': 0.005296067963662724, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:09,573] Trial 82 finished with value: 0.6361299157142639 and parameters: {'hidden_layers': 2, 'n_units_l0': 423, 'n_units_l1': 147, 'dropout_rate': 0.5343917585314815, 'lr': 0.006117541840713034, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:10,274] Trial 83 finished with value: -9.739033699035645 and parameters: {'hidden_layers': 2, 'n_units_l0': 475, 'n_units_l1': 245, 'dropout_rate': 0.5310218251374628, 'lr': 0.00012535591293448773, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:10,993] Trial 84 finished with value: 0.5869876742362976 and parameters: {'hidden_layers': 2, 'n_units_l0': 334, 'n_units_l1': 173, 'dropout_rate': 0.5795738828257134, 'lr': 0.006827315470487341, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:11,716] Trial 85 finished with value: 0.5901914834976196 and parameters: {'hidden_layers': 2, 'n_units_l0': 418, 'n_units_l1': 201, 'dropout_rate': 0.5282759747682154, 'lr': 0.004004289184335391, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:12,493] Trial 86 finished with value: 0.2522298991680145 and parameters: {'hidden_layers': 2, 'n_units_l0': 381, 'n_units_l1': 791, 'dropout_rate': 0.5138019899197938, 'lr': 0.0027161132924605215, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:13,142] Trial 87 finished with value: 0.25719502568244934 and parameters: {'hidden_layers': 2, 'n_units_l0': 149, 'n_units_l1': 692, 'dropout_rate': 0.537846777525849, 'lr': 0.009910845942456566, 'optimizer': 'Adam'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:13,843] Trial 88 finished with value: 0.5653808116912842 and parameters: {'hidden_layers': 2, 'n_units_l0': 207, 'n_units_l1': 281, 'dropout_rate': 0.502610004569743, 'lr': 0.006878562831315774, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:14,374] Trial 89 finished with value: -404.00634765625 and parameters: {'hidden_layers': 2, 'n_units_l0': 507, 'n_units_l1': 147, 'dropout_rate': 0.47675001464527245, 'lr': 0.0003858706930722296, 'optimizer': 'SGD'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:15,086] Trial 90 finished with value: 0.6349443197250366 and parameters: {'hidden_layers': 2, 'n_units_l0': 286, 'n_units_l1': 229, 'dropout_rate': 0.5558644692680339, 'lr': 0.004809279684889221, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:15,813] Trial 91 finished with value: 0.5740755200386047 and parameters: {'hidden_layers': 2, 'n_units_l0': 286, 'n_units_l1': 227, 'dropout_rate': 0.5591002924937866, 'lr': 0.004425301093741037, 'optimizer': 'AdamW'}. Best is trial 51 with value: 0.6616662740707397.\n",
      "[I 2024-09-25 14:43:16,572] Trial 92 finished with value: 0.6652600765228271 and parameters: {'hidden_layers': 2, 'n_units_l0': 354, 'n_units_l1': 178, 'dropout_rate': 0.5721057498205864, 'lr': 0.008087180725235689, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:17,292] Trial 93 finished with value: 0.5470295548439026 and parameters: {'hidden_layers': 2, 'n_units_l0': 351, 'n_units_l1': 176, 'dropout_rate': 0.5721167929447272, 'lr': 0.005967371050190423, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:18,048] Trial 94 finished with value: 0.6427825689315796 and parameters: {'hidden_layers': 2, 'n_units_l0': 404, 'n_units_l1': 201, 'dropout_rate': 0.5431388188985733, 'lr': 0.008306266823920282, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:18,774] Trial 95 finished with value: 0.48891106247901917 and parameters: {'hidden_layers': 2, 'n_units_l0': 399, 'n_units_l1': 153, 'dropout_rate': 0.5959789626897286, 'lr': 0.004928141889840299, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:19,511] Trial 96 finished with value: 0.614819347858429 and parameters: {'hidden_layers': 2, 'n_units_l0': 368, 'n_units_l1': 202, 'dropout_rate': 0.5513626077318444, 'lr': 0.0036785696586053088, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:20,201] Trial 97 finished with value: -3.4440865516662598 and parameters: {'hidden_layers': 2, 'n_units_l0': 311, 'n_units_l1': 253, 'dropout_rate': 0.5848371486840896, 'lr': 0.00829009417789862, 'optimizer': 'RMSprop'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:21,085] Trial 98 finished with value: -2.39858341217041 and parameters: {'hidden_layers': 2, 'n_units_l0': 425, 'n_units_l1': 997, 'dropout_rate': 0.5673892885472881, 'lr': 0.0008223686001975704, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:21,799] Trial 99 finished with value: 0.46955713629722595 and parameters: {'hidden_layers': 2, 'n_units_l0': 335, 'n_units_l1': 177, 'dropout_rate': 0.5402932655831018, 'lr': 0.0016755076362851328, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:22,725] Trial 100 finished with value: 0.2683427035808563 and parameters: {'hidden_layers': 4, 'n_units_l0': 440, 'n_units_l1': 231, 'n_units_l2': 935, 'n_units_l3': 377, 'dropout_rate': 0.523612564440401, 'lr': 0.006010259251223644, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:23,400] Trial 101 finished with value: 0.5918595194816589 and parameters: {'hidden_layers': 2, 'n_units_l0': 464, 'n_units_l1': 259, 'dropout_rate': 0.517185420074865, 'lr': 0.007405324454626796, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:24,066] Trial 102 finished with value: 0.645580530166626 and parameters: {'hidden_layers': 2, 'n_units_l0': 537, 'n_units_l1': 144, 'dropout_rate': 0.5560281381475368, 'lr': 0.008645015915398067, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:24,756] Trial 103 finished with value: 0.5544431209564209 and parameters: {'hidden_layers': 2, 'n_units_l0': 541, 'n_units_l1': 145, 'dropout_rate': 0.5588029951775476, 'lr': 0.008366892178246505, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:25,415] Trial 104 finished with value: 0.5459713339805603 and parameters: {'hidden_layers': 2, 'n_units_l0': 401, 'n_units_l1': 204, 'dropout_rate': 0.5713452659356938, 'lr': 0.005109919855633837, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:26,122] Trial 105 finished with value: 0.4765045940876007 and parameters: {'hidden_layers': 2, 'n_units_l0': 352, 'n_units_l1': 171, 'dropout_rate': 0.5359120948232493, 'lr': 0.004151869573510149, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:26,799] Trial 106 finished with value: 0.6526845097541809 and parameters: {'hidden_layers': 2, 'n_units_l0': 288, 'n_units_l1': 190, 'dropout_rate': 0.5460781462261535, 'lr': 0.0062992401122531376, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:27,449] Trial 107 finished with value: 0.5904628038406372 and parameters: {'hidden_layers': 2, 'n_units_l0': 294, 'n_units_l1': 129, 'dropout_rate': 0.5478899989373942, 'lr': 0.006346029483189714, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:28,233] Trial 108 finished with value: 0.4891194999217987 and parameters: {'hidden_layers': 2, 'n_units_l0': 276, 'n_units_l1': 290, 'dropout_rate': 0.5556535759968381, 'lr': 0.0030729856862890424, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:29,110] Trial 109 finished with value: 0.5466151833534241 and parameters: {'hidden_layers': 2, 'n_units_l0': 322, 'n_units_l1': 219, 'dropout_rate': 0.5880839397409797, 'lr': 0.007280748270045116, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:29,911] Trial 110 finished with value: 0.4359560012817383 and parameters: {'hidden_layers': 2, 'n_units_l0': 253, 'n_units_l1': 158, 'dropout_rate': 0.32372057991028835, 'lr': 0.0034247349686142596, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:30,809] Trial 111 finished with value: 0.6080583333969116 and parameters: {'hidden_layers': 2, 'n_units_l0': 371, 'n_units_l1': 187, 'dropout_rate': 0.5278193846633815, 'lr': 0.009028014231517176, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:31,686] Trial 112 finished with value: 0.6153352856636047 and parameters: {'hidden_layers': 2, 'n_units_l0': 409, 'n_units_l1': 189, 'dropout_rate': 0.5430677519664207, 'lr': 0.005545613364443602, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:32,509] Trial 113 finished with value: 0.5715593099594116 and parameters: {'hidden_layers': 2, 'n_units_l0': 313, 'n_units_l1': 142, 'dropout_rate': 0.501142109834723, 'lr': 0.007275760516293048, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:33,406] Trial 114 finished with value: 0.5764359831809998 and parameters: {'hidden_layers': 2, 'n_units_l0': 348, 'n_units_l1': 231, 'dropout_rate': 0.5784208256191172, 'lr': 0.0048230422815985306, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:34,035] Trial 115 finished with value: -306.5755920410156 and parameters: {'hidden_layers': 2, 'n_units_l0': 175, 'n_units_l1': 209, 'dropout_rate': 0.5170244572468858, 'lr': 0.008808848726192748, 'optimizer': 'SGD'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:34,885] Trial 116 finished with value: 0.5854396820068359 and parameters: {'hidden_layers': 2, 'n_units_l0': 485, 'n_units_l1': 164, 'dropout_rate': 0.5625089146915471, 'lr': 0.006408198654396337, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:35,760] Trial 117 finished with value: 0.5606402158737183 and parameters: {'hidden_layers': 2, 'n_units_l0': 378, 'n_units_l1': 308, 'dropout_rate': 0.5087884009655848, 'lr': 0.00559203985979109, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:36,647] Trial 118 finished with value: -38.59104919433594 and parameters: {'hidden_layers': 2, 'n_units_l0': 577, 'n_units_l1': 246, 'dropout_rate': 0.49056541121583214, 'lr': 1.4126521982884364e-05, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:37,396] Trial 119 finished with value: -1.099815011024475 and parameters: {'hidden_layers': 2, 'n_units_l0': 224, 'n_units_l1': 194, 'dropout_rate': 0.5368760183091553, 'lr': 0.009908485994731259, 'optimizer': 'RMSprop'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:38,374] Trial 120 finished with value: 0.31982675194740295 and parameters: {'hidden_layers': 3, 'n_units_l0': 432, 'n_units_l1': 266, 'n_units_l2': 331, 'dropout_rate': 0.5504247789619094, 'lr': 0.00392727231130917, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:39,186] Trial 121 finished with value: 0.6078673601150513 and parameters: {'hidden_layers': 2, 'n_units_l0': 526, 'n_units_l1': 177, 'dropout_rate': 0.529093807861531, 'lr': 0.00797527954063944, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:40,020] Trial 122 finished with value: 0.6374413967132568 and parameters: {'hidden_layers': 2, 'n_units_l0': 609, 'n_units_l1': 223, 'dropout_rate': 0.4891392951283689, 'lr': 0.007124016716544236, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:40,908] Trial 123 finished with value: 0.6144430041313171 and parameters: {'hidden_layers': 2, 'n_units_l0': 602, 'n_units_l1': 220, 'dropout_rate': 0.4971646883935883, 'lr': 0.006817140125196989, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:41,834] Trial 124 finished with value: 0.6173278093338013 and parameters: {'hidden_layers': 2, 'n_units_l0': 636, 'n_units_l1': 203, 'dropout_rate': 0.5184827145277321, 'lr': 0.005014856607378706, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:42,712] Trial 125 finished with value: 0.602359414100647 and parameters: {'hidden_layers': 2, 'n_units_l0': 694, 'n_units_l1': 150, 'dropout_rate': 0.2400798035701735, 'lr': 0.0060732883998979165, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:43,605] Trial 126 finished with value: 0.42123350501060486 and parameters: {'hidden_layers': 2, 'n_units_l0': 262, 'n_units_l1': 243, 'dropout_rate': 0.46758275852919345, 'lr': 0.008752314733222731, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:44,478] Trial 127 finished with value: 0.6440107822418213 and parameters: {'hidden_layers': 2, 'n_units_l0': 503, 'n_units_l1': 169, 'dropout_rate': 0.37015487223818005, 'lr': 0.007378709736720537, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:45,359] Trial 128 finished with value: 0.14541080594062805 and parameters: {'hidden_layers': 2, 'n_units_l0': 573, 'n_units_l1': 166, 'dropout_rate': 0.3905375905084079, 'lr': 0.004479901191282682, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:46,244] Trial 129 finished with value: 0.5360960960388184 and parameters: {'hidden_layers': 2, 'n_units_l0': 694, 'n_units_l1': 227, 'dropout_rate': 0.29053237801094883, 'lr': 0.002166839003866727, 'optimizer': 'Adam'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:47,058] Trial 130 finished with value: 0.6085493564605713 and parameters: {'hidden_layers': 2, 'n_units_l0': 493, 'n_units_l1': 141, 'dropout_rate': 0.4261276028838288, 'lr': 0.007621955124208012, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:47,919] Trial 131 finished with value: 0.6079328656196594 and parameters: {'hidden_layers': 2, 'n_units_l0': 501, 'n_units_l1': 187, 'dropout_rate': 0.3531300694774596, 'lr': 0.006812508541557344, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:48,726] Trial 132 finished with value: 0.6229124069213867 and parameters: {'hidden_layers': 2, 'n_units_l0': 537, 'n_units_l1': 128, 'dropout_rate': 0.3798381179839385, 'lr': 0.005547071679012759, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:49,638] Trial 133 finished with value: 0.6397077441215515 and parameters: {'hidden_layers': 2, 'n_units_l0': 563, 'n_units_l1': 206, 'dropout_rate': 0.3384303046604053, 'lr': 0.00859905073040355, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:50,520] Trial 134 finished with value: 0.5996270775794983 and parameters: {'hidden_layers': 2, 'n_units_l0': 565, 'n_units_l1': 201, 'dropout_rate': 0.3368742156137987, 'lr': 0.007549226447683215, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:51,384] Trial 135 finished with value: 0.5486046671867371 and parameters: {'hidden_layers': 2, 'n_units_l0': 554, 'n_units_l1': 167, 'dropout_rate': 0.2977202786220874, 'lr': 0.004531533158298616, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:52,301] Trial 136 finished with value: 0.6191605925559998 and parameters: {'hidden_layers': 2, 'n_units_l0': 608, 'n_units_l1': 281, 'dropout_rate': 0.3336498197053251, 'lr': 0.006174964484763038, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:53,103] Trial 137 finished with value: 0.5723713040351868 and parameters: {'hidden_layers': 2, 'n_units_l0': 667, 'n_units_l1': 220, 'dropout_rate': 0.32166386587783125, 'lr': 0.009148000109400228, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:53,748] Trial 138 finished with value: -832.8690185546875 and parameters: {'hidden_layers': 2, 'n_units_l0': 592, 'n_units_l1': 247, 'dropout_rate': 0.33755774431011165, 'lr': 0.00396804363023034, 'optimizer': 'SGD'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:54,990] Trial 139 finished with value: -0.11268796771764755 and parameters: {'hidden_layers': 5, 'n_units_l0': 632, 'n_units_l1': 178, 'n_units_l2': 210, 'n_units_l3': 776, 'n_units_l4': 646, 'dropout_rate': 0.3658392691766809, 'lr': 0.0002628859389486886, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:55,749] Trial 140 finished with value: 0.5804065465927124 and parameters: {'hidden_layers': 2, 'n_units_l0': 460, 'n_units_l1': 151, 'dropout_rate': 0.5685174930203692, 'lr': 0.005049656471568427, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:56,538] Trial 141 finished with value: 0.5068094730377197 and parameters: {'hidden_layers': 2, 'n_units_l0': 196, 'n_units_l1': 205, 'dropout_rate': 0.36800536518946425, 'lr': 0.008567701858904101, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:57,298] Trial 142 finished with value: 0.6316069960594177 and parameters: {'hidden_layers': 2, 'n_units_l0': 332, 'n_units_l1': 188, 'dropout_rate': 0.31253963318937206, 'lr': 0.007196568147412178, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:58,009] Trial 143 finished with value: 0.6178439855575562 and parameters: {'hidden_layers': 2, 'n_units_l0': 328, 'n_units_l1': 187, 'dropout_rate': 0.31341900510266213, 'lr': 0.00681578336832602, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:58,839] Trial 144 finished with value: 0.5929204821586609 and parameters: {'hidden_layers': 2, 'n_units_l0': 398, 'n_units_l1': 228, 'dropout_rate': 0.34108679771037714, 'lr': 0.005844161766919258, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:43:59,712] Trial 145 finished with value: 0.6144422292709351 and parameters: {'hidden_layers': 2, 'n_units_l0': 287, 'n_units_l1': 264, 'dropout_rate': 0.2789583370649852, 'lr': 0.007527156877740584, 'optimizer': 'AdamW'}. Best is trial 92 with value: 0.6652600765228271.\n",
      "[I 2024-09-25 14:44:00,562] Trial 146 finished with value: 0.6937033534049988 and parameters: {'hidden_layers': 2, 'n_units_l0': 515, 'n_units_l1': 160, 'dropout_rate': 0.313513560364475, 'lr': 0.009998443357243456, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:01,410] Trial 147 finished with value: 0.6444323062896729 and parameters: {'hidden_layers': 2, 'n_units_l0': 528, 'n_units_l1': 158, 'dropout_rate': 0.3042412790767689, 'lr': 0.009964281079998834, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:02,290] Trial 148 finished with value: 0.6219286322593689 and parameters: {'hidden_layers': 2, 'n_units_l0': 522, 'n_units_l1': 158, 'dropout_rate': 0.28963123584737516, 'lr': 0.008710487813120718, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:03,148] Trial 149 finished with value: 0.49980512261390686 and parameters: {'hidden_layers': 2, 'n_units_l0': 507, 'n_units_l1': 144, 'dropout_rate': 0.30530204299692554, 'lr': 0.009996987719426316, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:03,968] Trial 150 finished with value: 0.6546958088874817 and parameters: {'hidden_layers': 2, 'n_units_l0': 478, 'n_units_l1': 128, 'dropout_rate': 0.3149418829898288, 'lr': 0.008255766231410732, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:04,841] Trial 151 finished with value: 0.6639598608016968 and parameters: {'hidden_layers': 2, 'n_units_l0': 474, 'n_units_l1': 167, 'dropout_rate': 0.32642926257225086, 'lr': 0.009970498535980074, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:05,704] Trial 152 finished with value: 0.31684771180152893 and parameters: {'hidden_layers': 2, 'n_units_l0': 464, 'n_units_l1': 161, 'dropout_rate': 0.3190649302182383, 'lr': 0.00987431359919399, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:06,525] Trial 153 finished with value: 0.6404516696929932 and parameters: {'hidden_layers': 2, 'n_units_l0': 540, 'n_units_l1': 134, 'dropout_rate': 0.32951567989129393, 'lr': 0.007469315937152253, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:07,374] Trial 154 finished with value: 0.6523663401603699 and parameters: {'hidden_layers': 2, 'n_units_l0': 543, 'n_units_l1': 134, 'dropout_rate': 0.33214407609734836, 'lr': 0.008277195488697698, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:08,207] Trial 155 finished with value: 0.6485589146614075 and parameters: {'hidden_layers': 2, 'n_units_l0': 538, 'n_units_l1': 132, 'dropout_rate': 0.330011555799477, 'lr': 0.008392832675107332, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:09,318] Trial 156 finished with value: -2.1028707027435303 and parameters: {'hidden_layers': 4, 'n_units_l0': 536, 'n_units_l1': 140, 'n_units_l2': 804, 'n_units_l3': 409, 'dropout_rate': 0.32916683038004735, 'lr': 0.008440911091754941, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:10,105] Trial 157 finished with value: 0.6099773645401001 and parameters: {'hidden_layers': 2, 'n_units_l0': 563, 'n_units_l1': 129, 'dropout_rate': 0.30727705202482136, 'lr': 0.009996552776122597, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:10,905] Trial 158 finished with value: 0.6841291189193726 and parameters: {'hidden_layers': 2, 'n_units_l0': 548, 'n_units_l1': 129, 'dropout_rate': 0.3475809576904665, 'lr': 0.008163851504524842, 'optimizer': 'Adam'}. Best is trial 146 with value: 0.6937033534049988.\n",
      "[I 2024-09-25 14:44:11,692] Trial 159 finished with value: 0.6963508725166321 and parameters: {'hidden_layers': 2, 'n_units_l0': 554, 'n_units_l1': 128, 'dropout_rate': 0.35103276354919205, 'lr': 0.008359219868035709, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:12,542] Trial 160 finished with value: 0.6207129955291748 and parameters: {'hidden_layers': 2, 'n_units_l0': 478, 'n_units_l1': 129, 'dropout_rate': 0.34869331696740846, 'lr': 0.00801989373652134, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:13,426] Trial 161 finished with value: 0.6212280988693237 and parameters: {'hidden_layers': 2, 'n_units_l0': 538, 'n_units_l1': 163, 'dropout_rate': 0.3437242332135072, 'lr': 0.008561311845696345, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:14,287] Trial 162 finished with value: 0.6192640662193298 and parameters: {'hidden_layers': 2, 'n_units_l0': 513, 'n_units_l1': 147, 'dropout_rate': 0.3593265520253414, 'lr': 0.008198806434683016, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:15,149] Trial 163 finished with value: 0.5416330695152283 and parameters: {'hidden_layers': 2, 'n_units_l0': 559, 'n_units_l1': 166, 'dropout_rate': 0.32969475183025687, 'lr': 0.009989130991210123, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:15,999] Trial 164 finished with value: 0.5922394394874573 and parameters: {'hidden_layers': 2, 'n_units_l0': 589, 'n_units_l1': 135, 'dropout_rate': 0.31388692996992923, 'lr': 0.007857021394422155, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:16,819] Trial 165 finished with value: 0.6586235761642456 and parameters: {'hidden_layers': 2, 'n_units_l0': 525, 'n_units_l1': 129, 'dropout_rate': 0.32880057013169345, 'lr': 0.006620166609042312, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:17,674] Trial 166 finished with value: 0.6770989298820496 and parameters: {'hidden_layers': 2, 'n_units_l0': 516, 'n_units_l1': 152, 'dropout_rate': 0.3291142680475331, 'lr': 0.006493701944055127, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:18,549] Trial 167 finished with value: 0.6262165904045105 and parameters: {'hidden_layers': 2, 'n_units_l0': 510, 'n_units_l1': 153, 'dropout_rate': 0.29648710762859315, 'lr': 0.006399017150436057, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:19,372] Trial 168 finished with value: -10.957098960876465 and parameters: {'hidden_layers': 2, 'n_units_l0': 492, 'n_units_l1': 128, 'dropout_rate': 0.3475989339048748, 'lr': 0.00015565484340137608, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:20,261] Trial 169 finished with value: 0.5518173575401306 and parameters: {'hidden_layers': 2, 'n_units_l0': 527, 'n_units_l1': 173, 'dropout_rate': 0.32026070333392864, 'lr': 0.008935881235690777, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:21,101] Trial 170 finished with value: 0.6520934104919434 and parameters: {'hidden_layers': 2, 'n_units_l0': 482, 'n_units_l1': 150, 'dropout_rate': 0.323796869488754, 'lr': 0.0066962796934364145, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:21,968] Trial 171 finished with value: 0.5394923686981201 and parameters: {'hidden_layers': 2, 'n_units_l0': 475, 'n_units_l1': 154, 'dropout_rate': 0.3268484861175581, 'lr': 0.006774484125202232, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:22,849] Trial 172 finished with value: 0.624488890171051 and parameters: {'hidden_layers': 2, 'n_units_l0': 514, 'n_units_l1': 173, 'dropout_rate': 0.359213589939807, 'lr': 0.005829076147247274, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:23,716] Trial 173 finished with value: 0.6725214123725891 and parameters: {'hidden_layers': 2, 'n_units_l0': 549, 'n_units_l1': 146, 'dropout_rate': 0.31482913307670457, 'lr': 0.0073253606674057664, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:24,549] Trial 174 finished with value: 0.5895001292228699 and parameters: {'hidden_layers': 2, 'n_units_l0': 552, 'n_units_l1': 150, 'dropout_rate': 0.31406811288961944, 'lr': 0.00663484565189429, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:25,403] Trial 175 finished with value: 0.6095945835113525 and parameters: {'hidden_layers': 2, 'n_units_l0': 581, 'n_units_l1': 129, 'dropout_rate': 0.3080075084678521, 'lr': 0.007428231620812165, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:26,268] Trial 176 finished with value: 0.661188542842865 and parameters: {'hidden_layers': 2, 'n_units_l0': 528, 'n_units_l1': 159, 'dropout_rate': 0.32263289142709567, 'lr': 0.006316419871614455, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:27,130] Trial 177 finished with value: 0.5645825266838074 and parameters: {'hidden_layers': 2, 'n_units_l0': 547, 'n_units_l1': 149, 'dropout_rate': 0.319748595570083, 'lr': 0.0055788162429509635, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:27,973] Trial 178 finished with value: 0.5449607372283936 and parameters: {'hidden_layers': 2, 'n_units_l0': 528, 'n_units_l1': 182, 'dropout_rate': 0.28423072870416716, 'lr': 0.0062349649347135955, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:28,831] Trial 179 finished with value: 0.4259847104549408 and parameters: {'hidden_layers': 2, 'n_units_l0': 486, 'n_units_l1': 156, 'dropout_rate': 0.30431877228264814, 'lr': 0.00995975712578516, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:29,716] Trial 180 finished with value: 0.5785892605781555 and parameters: {'hidden_layers': 2, 'n_units_l0': 574, 'n_units_l1': 714, 'dropout_rate': 0.3336415041498302, 'lr': 0.008657168547757632, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:30,484] Trial 181 finished with value: 0.5628412961959839 and parameters: {'hidden_layers': 2, 'n_units_l0': 520, 'n_units_l1': 170, 'dropout_rate': 0.3461257109900164, 'lr': 0.007579480363510317, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:31,272] Trial 182 finished with value: 0.6438511610031128 and parameters: {'hidden_layers': 2, 'n_units_l0': 497, 'n_units_l1': 145, 'dropout_rate': 0.32486193879324576, 'lr': 0.006791346264748581, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:32,025] Trial 183 finished with value: 0.48377254605293274 and parameters: {'hidden_layers': 2, 'n_units_l0': 451, 'n_units_l1': 168, 'dropout_rate': 0.3391927043641216, 'lr': 0.005469706448048764, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:32,768] Trial 184 finished with value: 0.5531507730484009 and parameters: {'hidden_layers': 2, 'n_units_l0': 535, 'n_units_l1': 129, 'dropout_rate': 0.2976958792620224, 'lr': 0.008857633793884318, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:33,613] Trial 185 finished with value: -17.865415573120117 and parameters: {'hidden_layers': 2, 'n_units_l0': 503, 'n_units_l1': 184, 'dropout_rate': 0.3188260374667836, 'lr': 6.439373797581905e-05, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:34,439] Trial 186 finished with value: 0.6391120553016663 and parameters: {'hidden_layers': 2, 'n_units_l0': 551, 'n_units_l1': 154, 'dropout_rate': 0.3306395775261389, 'lr': 0.007596665565542463, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:35,200] Trial 187 finished with value: 0.6222987771034241 and parameters: {'hidden_layers': 2, 'n_units_l0': 518, 'n_units_l1': 170, 'dropout_rate': 0.3112862535279252, 'lr': 0.006392700113021881, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:36,040] Trial 188 finished with value: 0.5789437294006348 and parameters: {'hidden_layers': 2, 'n_units_l0': 491, 'n_units_l1': 145, 'dropout_rate': 0.37503290735437844, 'lr': 0.008003935219031415, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:36,890] Trial 189 finished with value: 0.6455584764480591 and parameters: {'hidden_layers': 2, 'n_units_l0': 471, 'n_units_l1': 191, 'dropout_rate': 0.35824525190737116, 'lr': 0.008933491938916975, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:37,673] Trial 190 finished with value: 0.6426194906234741 and parameters: {'hidden_layers': 2, 'n_units_l0': 469, 'n_units_l1': 128, 'dropout_rate': 0.34796347653114434, 'lr': 0.008908530191827146, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:38,574] Trial 191 finished with value: 0.6051514744758606 and parameters: {'hidden_layers': 2, 'n_units_l0': 507, 'n_units_l1': 182, 'dropout_rate': 0.35715770391637003, 'lr': 0.007255867456488228, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:39,303] Trial 192 finished with value: 0.6469601988792419 and parameters: {'hidden_layers': 2, 'n_units_l0': 548, 'n_units_l1': 160, 'dropout_rate': 0.33475386860900863, 'lr': 0.006434261733800315, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:40,099] Trial 193 finished with value: 0.622852087020874 and parameters: {'hidden_layers': 2, 'n_units_l0': 549, 'n_units_l1': 195, 'dropout_rate': 0.32543567517632704, 'lr': 0.0061795208996229525, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:40,830] Trial 194 finished with value: 0.5342168211936951 and parameters: {'hidden_layers': 2, 'n_units_l0': 576, 'n_units_l1': 151, 'dropout_rate': 0.3176903842664282, 'lr': 0.008968860875459636, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:41,549] Trial 195 finished with value: 0.43310144543647766 and parameters: {'hidden_layers': 2, 'n_units_l0': 530, 'n_units_l1': 162, 'dropout_rate': 0.3355604867201056, 'lr': 0.00992610528063404, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:42,387] Trial 196 finished with value: 0.6660365462303162 and parameters: {'hidden_layers': 2, 'n_units_l0': 567, 'n_units_l1': 142, 'dropout_rate': 0.30407628596496566, 'lr': 0.005273116271596755, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:43,213] Trial 197 finished with value: 0.6532961130142212 and parameters: {'hidden_layers': 2, 'n_units_l0': 597, 'n_units_l1': 144, 'dropout_rate': 0.33878340150970004, 'lr': 0.005053223865162066, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:44,041] Trial 198 finished with value: 0.5843705534934998 and parameters: {'hidden_layers': 2, 'n_units_l0': 592, 'n_units_l1': 141, 'dropout_rate': 0.3405978519187384, 'lr': 0.005101234101264455, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n",
      "[I 2024-09-25 14:44:44,872] Trial 199 finished with value: -36.90313720703125 and parameters: {'hidden_layers': 2, 'n_units_l0': 565, 'n_units_l1': 129, 'dropout_rate': 0.3326704246126023, 'lr': 3.7798703776320166e-05, 'optimizer': 'Adam'}. Best is trial 159 with value: 0.6963508725166321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization completed.\n",
      "Best Trial:\n",
      "  Value (MSE): 0.6963508725166321\n",
      "  Params:\n",
      "    hidden_layers: 2\n",
      "    n_units_l0: 554\n",
      "    n_units_l1: 128\n",
      "    dropout_rate: 0.35103276354919205\n",
      "    lr: 0.008359219868035709\n",
      "    optimizer: Adam\n",
      "Starting training of the best model...\n",
      "Epoch 1/500 - Training Loss: 0.1591 - Validation Loss: 0.0381\n",
      "Epoch 2/500 - Training Loss: 0.0503 - Validation Loss: 0.0340\n",
      "Epoch 3/500 - Training Loss: 0.0370 - Validation Loss: 0.0253\n",
      "Epoch 4/500 - Training Loss: 0.0296 - Validation Loss: 0.0184\n",
      "Epoch 5/500 - Training Loss: 0.0254 - Validation Loss: 0.0277\n",
      "Epoch 6/500 - Training Loss: 0.0413 - Validation Loss: 0.0165\n",
      "Epoch 7/500 - Training Loss: 0.0314 - Validation Loss: 0.0350\n",
      "Epoch 8/500 - Training Loss: 0.0273 - Validation Loss: 0.0163\n",
      "Epoch 9/500 - Training Loss: 0.0228 - Validation Loss: 0.0136\n",
      "Epoch 10/500 - Training Loss: 0.0206 - Validation Loss: 0.0113\n",
      "Epoch 11/500 - Training Loss: 0.0178 - Validation Loss: 0.0123\n",
      "Epoch 12/500 - Training Loss: 0.0175 - Validation Loss: 0.0085\n",
      "Epoch 13/500 - Training Loss: 0.0141 - Validation Loss: 0.0087\n",
      "Epoch 14/500 - Training Loss: 0.0174 - Validation Loss: 0.0078\n",
      "Epoch 15/500 - Training Loss: 0.0144 - Validation Loss: 0.0073\n",
      "Epoch 16/500 - Training Loss: 0.0152 - Validation Loss: 0.0080\n",
      "Epoch 17/500 - Training Loss: 0.0127 - Validation Loss: 0.0068\n",
      "Epoch 18/500 - Training Loss: 0.0122 - Validation Loss: 0.0069\n",
      "Epoch 19/500 - Training Loss: 0.0118 - Validation Loss: 0.0080\n",
      "Epoch 20/500 - Training Loss: 0.0132 - Validation Loss: 0.0055\n",
      "Epoch 21/500 - Training Loss: 0.0112 - Validation Loss: 0.0076\n",
      "Epoch 22/500 - Training Loss: 0.0122 - Validation Loss: 0.0057\n",
      "Epoch 23/500 - Training Loss: 0.0106 - Validation Loss: 0.0077\n",
      "Epoch 24/500 - Training Loss: 0.0120 - Validation Loss: 0.0049\n",
      "Epoch 25/500 - Training Loss: 0.0096 - Validation Loss: 0.0060\n",
      "Epoch 26/500 - Training Loss: 0.0096 - Validation Loss: 0.0053\n",
      "Epoch 27/500 - Training Loss: 0.0106 - Validation Loss: 0.0058\n",
      "Epoch 28/500 - Training Loss: 0.0095 - Validation Loss: 0.0044\n",
      "Epoch 29/500 - Training Loss: 0.0088 - Validation Loss: 0.0046\n",
      "Epoch 30/500 - Training Loss: 0.0100 - Validation Loss: 0.0045\n",
      "Epoch 31/500 - Training Loss: 0.0088 - Validation Loss: 0.0059\n",
      "Epoch 32/500 - Training Loss: 0.0082 - Validation Loss: 0.0040\n",
      "Epoch 33/500 - Training Loss: 0.0074 - Validation Loss: 0.0047\n",
      "Epoch 34/500 - Training Loss: 0.0090 - Validation Loss: 0.0037\n",
      "Epoch 35/500 - Training Loss: 0.0087 - Validation Loss: 0.0036\n",
      "Epoch 36/500 - Training Loss: 0.0075 - Validation Loss: 0.0044\n",
      "Epoch 37/500 - Training Loss: 0.0084 - Validation Loss: 0.0036\n",
      "Epoch 38/500 - Training Loss: 0.0083 - Validation Loss: 0.0042\n",
      "Epoch 39/500 - Training Loss: 0.0083 - Validation Loss: 0.0037\n",
      "Epoch 40/500 - Training Loss: 0.0070 - Validation Loss: 0.0034\n",
      "Epoch 41/500 - Training Loss: 0.0083 - Validation Loss: 0.0041\n",
      "Epoch 42/500 - Training Loss: 0.0078 - Validation Loss: 0.0033\n",
      "Epoch 43/500 - Training Loss: 0.0070 - Validation Loss: 0.0041\n",
      "Epoch 44/500 - Training Loss: 0.0076 - Validation Loss: 0.0039\n",
      "Epoch 45/500 - Training Loss: 0.0060 - Validation Loss: 0.0033\n",
      "Epoch 46/500 - Training Loss: 0.0069 - Validation Loss: 0.0038\n",
      "Epoch 47/500 - Training Loss: 0.0069 - Validation Loss: 0.0030\n",
      "Epoch 48/500 - Training Loss: 0.0062 - Validation Loss: 0.0036\n",
      "Epoch 49/500 - Training Loss: 0.0066 - Validation Loss: 0.0026\n",
      "Epoch 50/500 - Training Loss: 0.0057 - Validation Loss: 0.0031\n",
      "Epoch 51/500 - Training Loss: 0.0062 - Validation Loss: 0.0035\n",
      "Epoch 52/500 - Training Loss: 0.0060 - Validation Loss: 0.0037\n",
      "Epoch 53/500 - Training Loss: 0.0066 - Validation Loss: 0.0032\n",
      "Epoch 54/500 - Training Loss: 0.0058 - Validation Loss: 0.0036\n",
      "Epoch 55/500 - Training Loss: 0.0057 - Validation Loss: 0.0035\n",
      "Epoch 56/500 - Training Loss: 0.0062 - Validation Loss: 0.0028\n",
      "Epoch 57/500 - Training Loss: 0.0064 - Validation Loss: 0.0040\n",
      "Epoch 58/500 - Training Loss: 0.0060 - Validation Loss: 0.0031\n",
      "Epoch 59/500 - Training Loss: 0.0055 - Validation Loss: 0.0028\n",
      "Epoch 60/500 - Training Loss: 0.0067 - Validation Loss: 0.0046\n",
      "Epoch 61/500 - Training Loss: 0.0056 - Validation Loss: 0.0036\n",
      "Epoch 62/500 - Training Loss: 0.0053 - Validation Loss: 0.0029\n",
      "Epoch 63/500 - Training Loss: 0.0063 - Validation Loss: 0.0039\n",
      "Epoch 64/500 - Training Loss: 0.0062 - Validation Loss: 0.0032\n",
      "Epoch 65/500 - Training Loss: 0.0055 - Validation Loss: 0.0029\n",
      "Epoch 66/500 - Training Loss: 0.0060 - Validation Loss: 0.0034\n",
      "Epoch 67/500 - Training Loss: 0.0056 - Validation Loss: 0.0029\n",
      "Epoch 68/500 - Training Loss: 0.0054 - Validation Loss: 0.0030\n",
      "Epoch 69/500 - Training Loss: 0.0051 - Validation Loss: 0.0029\n",
      "Epoch 70/500 - Training Loss: 0.0049 - Validation Loss: 0.0030\n",
      "Epoch 71/500 - Training Loss: 0.0050 - Validation Loss: 0.0033\n",
      "Epoch 72/500 - Training Loss: 0.0059 - Validation Loss: 0.0031\n",
      "Epoch 73/500 - Training Loss: 0.0050 - Validation Loss: 0.0031\n",
      "Epoch 74/500 - Training Loss: 0.0054 - Validation Loss: 0.0030\n",
      "Epoch 75/500 - Training Loss: 0.0057 - Validation Loss: 0.0028\n",
      "Epoch 76/500 - Training Loss: 0.0054 - Validation Loss: 0.0029\n",
      "Epoch 77/500 - Training Loss: 0.0052 - Validation Loss: 0.0027\n",
      "Epoch 78/500 - Training Loss: 0.0045 - Validation Loss: 0.0024\n",
      "Epoch 79/500 - Training Loss: 0.0052 - Validation Loss: 0.0030\n",
      "Epoch 80/500 - Training Loss: 0.0053 - Validation Loss: 0.0029\n",
      "Epoch 81/500 - Training Loss: 0.0045 - Validation Loss: 0.0026\n",
      "Epoch 82/500 - Training Loss: 0.0049 - Validation Loss: 0.0027\n",
      "Epoch 83/500 - Training Loss: 0.0048 - Validation Loss: 0.0032\n",
      "Epoch 84/500 - Training Loss: 0.0051 - Validation Loss: 0.0028\n",
      "Epoch 85/500 - Training Loss: 0.0049 - Validation Loss: 0.0031\n",
      "Epoch 86/500 - Training Loss: 0.0043 - Validation Loss: 0.0025\n",
      "Epoch 87/500 - Training Loss: 0.0044 - Validation Loss: 0.0025\n",
      "Epoch 88/500 - Training Loss: 0.0044 - Validation Loss: 0.0025\n",
      "Epoch 89/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 90/500 - Training Loss: 0.0045 - Validation Loss: 0.0026\n",
      "Epoch 91/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 92/500 - Training Loss: 0.0047 - Validation Loss: 0.0026\n",
      "Epoch 93/500 - Training Loss: 0.0041 - Validation Loss: 0.0025\n",
      "Epoch 94/500 - Training Loss: 0.0047 - Validation Loss: 0.0024\n",
      "Epoch 95/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 96/500 - Training Loss: 0.0041 - Validation Loss: 0.0027\n",
      "Epoch 97/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 98/500 - Training Loss: 0.0044 - Validation Loss: 0.0022\n",
      "Epoch 99/500 - Training Loss: 0.0044 - Validation Loss: 0.0025\n",
      "Epoch 100/500 - Training Loss: 0.0041 - Validation Loss: 0.0026\n",
      "Epoch 101/500 - Training Loss: 0.0047 - Validation Loss: 0.0026\n",
      "Epoch 102/500 - Training Loss: 0.0042 - Validation Loss: 0.0034\n",
      "Epoch 103/500 - Training Loss: 0.0044 - Validation Loss: 0.0026\n",
      "Epoch 104/500 - Training Loss: 0.0045 - Validation Loss: 0.0023\n",
      "Epoch 105/500 - Training Loss: 0.0047 - Validation Loss: 0.0028\n",
      "Epoch 106/500 - Training Loss: 0.0041 - Validation Loss: 0.0028\n",
      "Epoch 107/500 - Training Loss: 0.0049 - Validation Loss: 0.0028\n",
      "Epoch 108/500 - Training Loss: 0.0040 - Validation Loss: 0.0033\n",
      "Epoch 109/500 - Training Loss: 0.0043 - Validation Loss: 0.0027\n",
      "Epoch 110/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 111/500 - Training Loss: 0.0046 - Validation Loss: 0.0033\n",
      "Epoch 112/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 113/500 - Training Loss: 0.0040 - Validation Loss: 0.0032\n",
      "Epoch 114/500 - Training Loss: 0.0040 - Validation Loss: 0.0032\n",
      "Epoch 115/500 - Training Loss: 0.0040 - Validation Loss: 0.0028\n",
      "Epoch 116/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 117/500 - Training Loss: 0.0045 - Validation Loss: 0.0025\n",
      "Epoch 118/500 - Training Loss: 0.0045 - Validation Loss: 0.0024\n",
      "Epoch 119/500 - Training Loss: 0.0047 - Validation Loss: 0.0024\n",
      "Epoch 120/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 121/500 - Training Loss: 0.0052 - Validation Loss: 0.0034\n",
      "Epoch 122/500 - Training Loss: 0.0040 - Validation Loss: 0.0028\n",
      "Epoch 123/500 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
      "Epoch 124/500 - Training Loss: 0.0044 - Validation Loss: 0.0025\n",
      "Epoch 125/500 - Training Loss: 0.0044 - Validation Loss: 0.0023\n",
      "Epoch 126/500 - Training Loss: 0.0038 - Validation Loss: 0.0022\n",
      "Epoch 127/500 - Training Loss: 0.0043 - Validation Loss: 0.0025\n",
      "Epoch 128/500 - Training Loss: 0.0043 - Validation Loss: 0.0029\n",
      "Epoch 129/500 - Training Loss: 0.0036 - Validation Loss: 0.0029\n",
      "Epoch 130/500 - Training Loss: 0.0036 - Validation Loss: 0.0032\n",
      "Epoch 131/500 - Training Loss: 0.0049 - Validation Loss: 0.0029\n",
      "Epoch 132/500 - Training Loss: 0.0046 - Validation Loss: 0.0030\n",
      "Epoch 133/500 - Training Loss: 0.0047 - Validation Loss: 0.0028\n",
      "Epoch 134/500 - Training Loss: 0.0045 - Validation Loss: 0.0024\n",
      "Epoch 135/500 - Training Loss: 0.0044 - Validation Loss: 0.0022\n",
      "Epoch 136/500 - Training Loss: 0.0041 - Validation Loss: 0.0025\n",
      "Epoch 137/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 138/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 139/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 140/500 - Training Loss: 0.0044 - Validation Loss: 0.0025\n",
      "Epoch 141/500 - Training Loss: 0.0031 - Validation Loss: 0.0023\n",
      "Epoch 142/500 - Training Loss: 0.0042 - Validation Loss: 0.0029\n",
      "Epoch 143/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 144/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 145/500 - Training Loss: 0.0036 - Validation Loss: 0.0021\n",
      "Epoch 146/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 147/500 - Training Loss: 0.0041 - Validation Loss: 0.0028\n",
      "Epoch 148/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 149/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 150/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 151/500 - Training Loss: 0.0032 - Validation Loss: 0.0024\n",
      "Epoch 152/500 - Training Loss: 0.0039 - Validation Loss: 0.0026\n",
      "Epoch 153/500 - Training Loss: 0.0047 - Validation Loss: 0.0026\n",
      "Epoch 154/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 155/500 - Training Loss: 0.0040 - Validation Loss: 0.0024\n",
      "Epoch 156/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 157/500 - Training Loss: 0.0035 - Validation Loss: 0.0026\n",
      "Epoch 158/500 - Training Loss: 0.0042 - Validation Loss: 0.0022\n",
      "Epoch 159/500 - Training Loss: 0.0042 - Validation Loss: 0.0025\n",
      "Epoch 160/500 - Training Loss: 0.0046 - Validation Loss: 0.0027\n",
      "Epoch 161/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 162/500 - Training Loss: 0.0039 - Validation Loss: 0.0024\n",
      "Epoch 163/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 164/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 165/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 166/500 - Training Loss: 0.0043 - Validation Loss: 0.0023\n",
      "Epoch 167/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 168/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 169/500 - Training Loss: 0.0041 - Validation Loss: 0.0025\n",
      "Epoch 170/500 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
      "Epoch 171/500 - Training Loss: 0.0039 - Validation Loss: 0.0029\n",
      "Epoch 172/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 173/500 - Training Loss: 0.0045 - Validation Loss: 0.0026\n",
      "Epoch 174/500 - Training Loss: 0.0039 - Validation Loss: 0.0025\n",
      "Epoch 175/500 - Training Loss: 0.0042 - Validation Loss: 0.0025\n",
      "Epoch 176/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 177/500 - Training Loss: 0.0041 - Validation Loss: 0.0028\n",
      "Epoch 178/500 - Training Loss: 0.0047 - Validation Loss: 0.0024\n",
      "Epoch 179/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 180/500 - Training Loss: 0.0032 - Validation Loss: 0.0023\n",
      "Epoch 181/500 - Training Loss: 0.0032 - Validation Loss: 0.0023\n",
      "Epoch 182/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 183/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 184/500 - Training Loss: 0.0040 - Validation Loss: 0.0024\n",
      "Epoch 185/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 186/500 - Training Loss: 0.0042 - Validation Loss: 0.0028\n",
      "Epoch 187/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 188/500 - Training Loss: 0.0041 - Validation Loss: 0.0028\n",
      "Epoch 189/500 - Training Loss: 0.0039 - Validation Loss: 0.0026\n",
      "Epoch 190/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 191/500 - Training Loss: 0.0033 - Validation Loss: 0.0027\n",
      "Epoch 192/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 193/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 194/500 - Training Loss: 0.0035 - Validation Loss: 0.0028\n",
      "Epoch 195/500 - Training Loss: 0.0038 - Validation Loss: 0.0022\n",
      "Epoch 196/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 197/500 - Training Loss: 0.0043 - Validation Loss: 0.0029\n",
      "Epoch 198/500 - Training Loss: 0.0044 - Validation Loss: 0.0026\n",
      "Epoch 199/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 200/500 - Training Loss: 0.0049 - Validation Loss: 0.0038\n",
      "Epoch 201/500 - Training Loss: 0.0046 - Validation Loss: 0.0032\n",
      "Epoch 202/500 - Training Loss: 0.0039 - Validation Loss: 0.0036\n",
      "Epoch 203/500 - Training Loss: 0.0040 - Validation Loss: 0.0036\n",
      "Epoch 204/500 - Training Loss: 0.0038 - Validation Loss: 0.0028\n",
      "Epoch 205/500 - Training Loss: 0.0043 - Validation Loss: 0.0027\n",
      "Epoch 206/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 207/500 - Training Loss: 0.0045 - Validation Loss: 0.0023\n",
      "Epoch 208/500 - Training Loss: 0.0037 - Validation Loss: 0.0025\n",
      "Epoch 209/500 - Training Loss: 0.0034 - Validation Loss: 0.0030\n",
      "Epoch 210/500 - Training Loss: 0.0037 - Validation Loss: 0.0030\n",
      "Epoch 211/500 - Training Loss: 0.0040 - Validation Loss: 0.0023\n",
      "Epoch 212/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 213/500 - Training Loss: 0.0040 - Validation Loss: 0.0028\n",
      "Epoch 214/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 215/500 - Training Loss: 0.0037 - Validation Loss: 0.0026\n",
      "Epoch 216/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 217/500 - Training Loss: 0.0042 - Validation Loss: 0.0025\n",
      "Epoch 218/500 - Training Loss: 0.0036 - Validation Loss: 0.0025\n",
      "Epoch 219/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 220/500 - Training Loss: 0.0034 - Validation Loss: 0.0027\n",
      "Epoch 221/500 - Training Loss: 0.0036 - Validation Loss: 0.0025\n",
      "Epoch 222/500 - Training Loss: 0.0030 - Validation Loss: 0.0022\n",
      "Epoch 223/500 - Training Loss: 0.0053 - Validation Loss: 0.0024\n",
      "Epoch 224/500 - Training Loss: 0.0050 - Validation Loss: 0.0026\n",
      "Epoch 225/500 - Training Loss: 0.0031 - Validation Loss: 0.0031\n",
      "Epoch 226/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 227/500 - Training Loss: 0.0040 - Validation Loss: 0.0022\n",
      "Epoch 228/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 229/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 230/500 - Training Loss: 0.0036 - Validation Loss: 0.0025\n",
      "Epoch 231/500 - Training Loss: 0.0043 - Validation Loss: 0.0030\n",
      "Epoch 232/500 - Training Loss: 0.0033 - Validation Loss: 0.0031\n",
      "Epoch 233/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 234/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 235/500 - Training Loss: 0.0044 - Validation Loss: 0.0028\n",
      "Epoch 236/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 237/500 - Training Loss: 0.0030 - Validation Loss: 0.0024\n",
      "Epoch 238/500 - Training Loss: 0.0031 - Validation Loss: 0.0025\n",
      "Epoch 239/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 240/500 - Training Loss: 0.0035 - Validation Loss: 0.0026\n",
      "Epoch 241/500 - Training Loss: 0.0040 - Validation Loss: 0.0023\n",
      "Epoch 242/500 - Training Loss: 0.0039 - Validation Loss: 0.0023\n",
      "Epoch 243/500 - Training Loss: 0.0031 - Validation Loss: 0.0024\n",
      "Epoch 244/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 245/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 246/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 247/500 - Training Loss: 0.0032 - Validation Loss: 0.0027\n",
      "Epoch 248/500 - Training Loss: 0.0036 - Validation Loss: 0.0027\n",
      "Epoch 249/500 - Training Loss: 0.0037 - Validation Loss: 0.0026\n",
      "Epoch 250/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 251/500 - Training Loss: 0.0036 - Validation Loss: 0.0030\n",
      "Epoch 252/500 - Training Loss: 0.0046 - Validation Loss: 0.0027\n",
      "Epoch 253/500 - Training Loss: 0.0032 - Validation Loss: 0.0025\n",
      "Epoch 254/500 - Training Loss: 0.0039 - Validation Loss: 0.0029\n",
      "Epoch 255/500 - Training Loss: 0.0034 - Validation Loss: 0.0023\n",
      "Epoch 256/500 - Training Loss: 0.0031 - Validation Loss: 0.0023\n",
      "Epoch 257/500 - Training Loss: 0.0038 - Validation Loss: 0.0028\n",
      "Epoch 258/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 259/500 - Training Loss: 0.0044 - Validation Loss: 0.0027\n",
      "Epoch 260/500 - Training Loss: 0.0038 - Validation Loss: 0.0027\n",
      "Epoch 261/500 - Training Loss: 0.0031 - Validation Loss: 0.0025\n",
      "Epoch 262/500 - Training Loss: 0.0030 - Validation Loss: 0.0024\n",
      "Epoch 263/500 - Training Loss: 0.0039 - Validation Loss: 0.0023\n",
      "Epoch 264/500 - Training Loss: 0.0028 - Validation Loss: 0.0023\n",
      "Epoch 265/500 - Training Loss: 0.0038 - Validation Loss: 0.0022\n",
      "Epoch 266/500 - Training Loss: 0.0048 - Validation Loss: 0.0024\n",
      "Epoch 267/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 268/500 - Training Loss: 0.0033 - Validation Loss: 0.0026\n",
      "Epoch 269/500 - Training Loss: 0.0033 - Validation Loss: 0.0030\n",
      "Epoch 270/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 271/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 272/500 - Training Loss: 0.0035 - Validation Loss: 0.0021\n",
      "Epoch 273/500 - Training Loss: 0.0041 - Validation Loss: 0.0025\n",
      "Epoch 274/500 - Training Loss: 0.0036 - Validation Loss: 0.0025\n",
      "Epoch 275/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 276/500 - Training Loss: 0.0035 - Validation Loss: 0.0026\n",
      "Epoch 277/500 - Training Loss: 0.0031 - Validation Loss: 0.0021\n",
      "Epoch 278/500 - Training Loss: 0.0040 - Validation Loss: 0.0021\n",
      "Epoch 279/500 - Training Loss: 0.0047 - Validation Loss: 0.0028\n",
      "Epoch 280/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 281/500 - Training Loss: 0.0036 - Validation Loss: 0.0024\n",
      "Epoch 282/500 - Training Loss: 0.0034 - Validation Loss: 0.0029\n",
      "Epoch 283/500 - Training Loss: 0.0030 - Validation Loss: 0.0025\n",
      "Epoch 284/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 285/500 - Training Loss: 0.0031 - Validation Loss: 0.0024\n",
      "Epoch 286/500 - Training Loss: 0.0048 - Validation Loss: 0.0026\n",
      "Epoch 287/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 288/500 - Training Loss: 0.0038 - Validation Loss: 0.0022\n",
      "Epoch 289/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 290/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 291/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 292/500 - Training Loss: 0.0040 - Validation Loss: 0.0027\n",
      "Epoch 293/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 294/500 - Training Loss: 0.0040 - Validation Loss: 0.0027\n",
      "Epoch 295/500 - Training Loss: 0.0039 - Validation Loss: 0.0024\n",
      "Epoch 296/500 - Training Loss: 0.0037 - Validation Loss: 0.0031\n",
      "Epoch 297/500 - Training Loss: 0.0049 - Validation Loss: 0.0023\n",
      "Epoch 298/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 299/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 300/500 - Training Loss: 0.0036 - Validation Loss: 0.0021\n",
      "Epoch 301/500 - Training Loss: 0.0037 - Validation Loss: 0.0022\n",
      "Epoch 302/500 - Training Loss: 0.0039 - Validation Loss: 0.0026\n",
      "Epoch 303/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 304/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 305/500 - Training Loss: 0.0038 - Validation Loss: 0.0027\n",
      "Epoch 306/500 - Training Loss: 0.0031 - Validation Loss: 0.0025\n",
      "Epoch 307/500 - Training Loss: 0.0032 - Validation Loss: 0.0026\n",
      "Epoch 308/500 - Training Loss: 0.0042 - Validation Loss: 0.0028\n",
      "Epoch 309/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 310/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 311/500 - Training Loss: 0.0035 - Validation Loss: 0.0022\n",
      "Epoch 312/500 - Training Loss: 0.0041 - Validation Loss: 0.0022\n",
      "Epoch 313/500 - Training Loss: 0.0033 - Validation Loss: 0.0023\n",
      "Epoch 314/500 - Training Loss: 0.0031 - Validation Loss: 0.0025\n",
      "Epoch 315/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 316/500 - Training Loss: 0.0041 - Validation Loss: 0.0024\n",
      "Epoch 317/500 - Training Loss: 0.0040 - Validation Loss: 0.0028\n",
      "Epoch 318/500 - Training Loss: 0.0040 - Validation Loss: 0.0024\n",
      "Epoch 319/500 - Training Loss: 0.0040 - Validation Loss: 0.0023\n",
      "Epoch 320/500 - Training Loss: 0.0033 - Validation Loss: 0.0027\n",
      "Epoch 321/500 - Training Loss: 0.0041 - Validation Loss: 0.0026\n",
      "Epoch 322/500 - Training Loss: 0.0035 - Validation Loss: 0.0027\n",
      "Epoch 323/500 - Training Loss: 0.0035 - Validation Loss: 0.0027\n",
      "Epoch 324/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 325/500 - Training Loss: 0.0039 - Validation Loss: 0.0023\n",
      "Epoch 326/500 - Training Loss: 0.0038 - Validation Loss: 0.0030\n",
      "Epoch 327/500 - Training Loss: 0.0034 - Validation Loss: 0.0027\n",
      "Epoch 328/500 - Training Loss: 0.0044 - Validation Loss: 0.0023\n",
      "Epoch 329/500 - Training Loss: 0.0030 - Validation Loss: 0.0026\n",
      "Epoch 330/500 - Training Loss: 0.0035 - Validation Loss: 0.0029\n",
      "Epoch 331/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 332/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 333/500 - Training Loss: 0.0038 - Validation Loss: 0.0027\n",
      "Epoch 334/500 - Training Loss: 0.0037 - Validation Loss: 0.0028\n",
      "Epoch 335/500 - Training Loss: 0.0037 - Validation Loss: 0.0032\n",
      "Epoch 336/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 337/500 - Training Loss: 0.0044 - Validation Loss: 0.0026\n",
      "Epoch 338/500 - Training Loss: 0.0037 - Validation Loss: 0.0036\n",
      "Epoch 339/500 - Training Loss: 0.0036 - Validation Loss: 0.0028\n",
      "Epoch 340/500 - Training Loss: 0.0042 - Validation Loss: 0.0026\n",
      "Epoch 341/500 - Training Loss: 0.0034 - Validation Loss: 0.0023\n",
      "Epoch 342/500 - Training Loss: 0.0041 - Validation Loss: 0.0029\n",
      "Epoch 343/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 344/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 345/500 - Training Loss: 0.0035 - Validation Loss: 0.0022\n",
      "Epoch 346/500 - Training Loss: 0.0032 - Validation Loss: 0.0025\n",
      "Epoch 347/500 - Training Loss: 0.0046 - Validation Loss: 0.0027\n",
      "Epoch 348/500 - Training Loss: 0.0036 - Validation Loss: 0.0036\n",
      "Epoch 349/500 - Training Loss: 0.0038 - Validation Loss: 0.0033\n",
      "Epoch 350/500 - Training Loss: 0.0037 - Validation Loss: 0.0034\n",
      "Epoch 351/500 - Training Loss: 0.0038 - Validation Loss: 0.0032\n",
      "Epoch 352/500 - Training Loss: 0.0037 - Validation Loss: 0.0028\n",
      "Epoch 353/500 - Training Loss: 0.0032 - Validation Loss: 0.0026\n",
      "Epoch 354/500 - Training Loss: 0.0030 - Validation Loss: 0.0029\n",
      "Epoch 355/500 - Training Loss: 0.0038 - Validation Loss: 0.0027\n",
      "Epoch 356/500 - Training Loss: 0.0042 - Validation Loss: 0.0023\n",
      "Epoch 357/500 - Training Loss: 0.0043 - Validation Loss: 0.0025\n",
      "Epoch 358/500 - Training Loss: 0.0037 - Validation Loss: 0.0021\n",
      "Epoch 359/500 - Training Loss: 0.0046 - Validation Loss: 0.0022\n",
      "Epoch 360/500 - Training Loss: 0.0033 - Validation Loss: 0.0023\n",
      "Epoch 361/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 362/500 - Training Loss: 0.0033 - Validation Loss: 0.0027\n",
      "Epoch 363/500 - Training Loss: 0.0036 - Validation Loss: 0.0029\n",
      "Epoch 364/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 365/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 366/500 - Training Loss: 0.0031 - Validation Loss: 0.0025\n",
      "Epoch 367/500 - Training Loss: 0.0041 - Validation Loss: 0.0024\n",
      "Epoch 368/500 - Training Loss: 0.0033 - Validation Loss: 0.0026\n",
      "Epoch 369/500 - Training Loss: 0.0033 - Validation Loss: 0.0024\n",
      "Epoch 370/500 - Training Loss: 0.0032 - Validation Loss: 0.0021\n",
      "Epoch 371/500 - Training Loss: 0.0031 - Validation Loss: 0.0021\n",
      "Epoch 372/500 - Training Loss: 0.0031 - Validation Loss: 0.0027\n",
      "Epoch 373/500 - Training Loss: 0.0048 - Validation Loss: 0.0025\n",
      "Epoch 374/500 - Training Loss: 0.0036 - Validation Loss: 0.0020\n",
      "Epoch 375/500 - Training Loss: 0.0037 - Validation Loss: 0.0022\n",
      "Epoch 376/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 377/500 - Training Loss: 0.0037 - Validation Loss: 0.0021\n",
      "Epoch 378/500 - Training Loss: 0.0038 - Validation Loss: 0.0023\n",
      "Epoch 379/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 380/500 - Training Loss: 0.0037 - Validation Loss: 0.0026\n",
      "Epoch 381/500 - Training Loss: 0.0031 - Validation Loss: 0.0027\n",
      "Epoch 382/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 383/500 - Training Loss: 0.0036 - Validation Loss: 0.0024\n",
      "Epoch 384/500 - Training Loss: 0.0036 - Validation Loss: 0.0022\n",
      "Epoch 385/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 386/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 387/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 388/500 - Training Loss: 0.0037 - Validation Loss: 0.0020\n",
      "Epoch 389/500 - Training Loss: 0.0043 - Validation Loss: 0.0024\n",
      "Epoch 390/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 391/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 392/500 - Training Loss: 0.0033 - Validation Loss: 0.0028\n",
      "Epoch 393/500 - Training Loss: 0.0033 - Validation Loss: 0.0023\n",
      "Epoch 394/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 395/500 - Training Loss: 0.0035 - Validation Loss: 0.0028\n",
      "Epoch 396/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 397/500 - Training Loss: 0.0036 - Validation Loss: 0.0032\n",
      "Epoch 398/500 - Training Loss: 0.0038 - Validation Loss: 0.0030\n",
      "Epoch 399/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 400/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 401/500 - Training Loss: 0.0031 - Validation Loss: 0.0027\n",
      "Epoch 402/500 - Training Loss: 0.0036 - Validation Loss: 0.0024\n",
      "Epoch 403/500 - Training Loss: 0.0040 - Validation Loss: 0.0025\n",
      "Epoch 404/500 - Training Loss: 0.0036 - Validation Loss: 0.0028\n",
      "Epoch 405/500 - Training Loss: 0.0033 - Validation Loss: 0.0021\n",
      "Epoch 406/500 - Training Loss: 0.0039 - Validation Loss: 0.0022\n",
      "Epoch 407/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 408/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 409/500 - Training Loss: 0.0037 - Validation Loss: 0.0020\n",
      "Epoch 410/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 411/500 - Training Loss: 0.0037 - Validation Loss: 0.0029\n",
      "Epoch 412/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 413/500 - Training Loss: 0.0036 - Validation Loss: 0.0023\n",
      "Epoch 414/500 - Training Loss: 0.0033 - Validation Loss: 0.0022\n",
      "Epoch 415/500 - Training Loss: 0.0030 - Validation Loss: 0.0024\n",
      "Epoch 416/500 - Training Loss: 0.0035 - Validation Loss: 0.0026\n",
      "Epoch 417/500 - Training Loss: 0.0034 - Validation Loss: 0.0022\n",
      "Epoch 418/500 - Training Loss: 0.0036 - Validation Loss: 0.0021\n",
      "Epoch 419/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 420/500 - Training Loss: 0.0034 - Validation Loss: 0.0023\n",
      "Epoch 421/500 - Training Loss: 0.0033 - Validation Loss: 0.0023\n",
      "Epoch 422/500 - Training Loss: 0.0029 - Validation Loss: 0.0027\n",
      "Epoch 423/500 - Training Loss: 0.0035 - Validation Loss: 0.0032\n",
      "Epoch 424/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 425/500 - Training Loss: 0.0032 - Validation Loss: 0.0024\n",
      "Epoch 426/500 - Training Loss: 0.0042 - Validation Loss: 0.0025\n",
      "Epoch 427/500 - Training Loss: 0.0032 - Validation Loss: 0.0021\n",
      "Epoch 428/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 429/500 - Training Loss: 0.0034 - Validation Loss: 0.0023\n",
      "Epoch 430/500 - Training Loss: 0.0040 - Validation Loss: 0.0020\n",
      "Epoch 431/500 - Training Loss: 0.0034 - Validation Loss: 0.0022\n",
      "Epoch 432/500 - Training Loss: 0.0036 - Validation Loss: 0.0026\n",
      "Epoch 433/500 - Training Loss: 0.0035 - Validation Loss: 0.0026\n",
      "Epoch 434/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 435/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 436/500 - Training Loss: 0.0042 - Validation Loss: 0.0024\n",
      "Epoch 437/500 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
      "Epoch 438/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 439/500 - Training Loss: 0.0036 - Validation Loss: 0.0023\n",
      "Epoch 440/500 - Training Loss: 0.0037 - Validation Loss: 0.0025\n",
      "Epoch 441/500 - Training Loss: 0.0033 - Validation Loss: 0.0025\n",
      "Epoch 442/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 443/500 - Training Loss: 0.0048 - Validation Loss: 0.0028\n",
      "Epoch 444/500 - Training Loss: 0.0037 - Validation Loss: 0.0024\n",
      "Epoch 445/500 - Training Loss: 0.0039 - Validation Loss: 0.0022\n",
      "Epoch 446/500 - Training Loss: 0.0034 - Validation Loss: 0.0022\n",
      "Epoch 447/500 - Training Loss: 0.0042 - Validation Loss: 0.0025\n",
      "Epoch 448/500 - Training Loss: 0.0031 - Validation Loss: 0.0027\n",
      "Epoch 449/500 - Training Loss: 0.0048 - Validation Loss: 0.0025\n",
      "Epoch 450/500 - Training Loss: 0.0040 - Validation Loss: 0.0030\n",
      "Epoch 451/500 - Training Loss: 0.0045 - Validation Loss: 0.0026\n",
      "Epoch 452/500 - Training Loss: 0.0043 - Validation Loss: 0.0026\n",
      "Epoch 453/500 - Training Loss: 0.0041 - Validation Loss: 0.0027\n",
      "Epoch 454/500 - Training Loss: 0.0038 - Validation Loss: 0.0026\n",
      "Epoch 455/500 - Training Loss: 0.0039 - Validation Loss: 0.0023\n",
      "Epoch 456/500 - Training Loss: 0.0045 - Validation Loss: 0.0032\n",
      "Epoch 457/500 - Training Loss: 0.0039 - Validation Loss: 0.0026\n",
      "Epoch 458/500 - Training Loss: 0.0033 - Validation Loss: 0.0030\n",
      "Epoch 459/500 - Training Loss: 0.0041 - Validation Loss: 0.0029\n",
      "Epoch 460/500 - Training Loss: 0.0031 - Validation Loss: 0.0024\n",
      "Epoch 461/500 - Training Loss: 0.0038 - Validation Loss: 0.0030\n",
      "Epoch 462/500 - Training Loss: 0.0034 - Validation Loss: 0.0026\n",
      "Epoch 463/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 464/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 465/500 - Training Loss: 0.0039 - Validation Loss: 0.0029\n",
      "Epoch 466/500 - Training Loss: 0.0038 - Validation Loss: 0.0027\n",
      "Epoch 467/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 468/500 - Training Loss: 0.0035 - Validation Loss: 0.0031\n",
      "Epoch 469/500 - Training Loss: 0.0041 - Validation Loss: 0.0029\n",
      "Epoch 470/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 471/500 - Training Loss: 0.0034 - Validation Loss: 0.0028\n",
      "Epoch 472/500 - Training Loss: 0.0037 - Validation Loss: 0.0032\n",
      "Epoch 473/500 - Training Loss: 0.0044 - Validation Loss: 0.0027\n",
      "Epoch 474/500 - Training Loss: 0.0037 - Validation Loss: 0.0025\n",
      "Epoch 475/500 - Training Loss: 0.0035 - Validation Loss: 0.0024\n",
      "Epoch 476/500 - Training Loss: 0.0036 - Validation Loss: 0.0024\n",
      "Epoch 477/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 478/500 - Training Loss: 0.0037 - Validation Loss: 0.0023\n",
      "Epoch 479/500 - Training Loss: 0.0035 - Validation Loss: 0.0023\n",
      "Epoch 480/500 - Training Loss: 0.0032 - Validation Loss: 0.0023\n",
      "Epoch 481/500 - Training Loss: 0.0038 - Validation Loss: 0.0023\n",
      "Epoch 482/500 - Training Loss: 0.0038 - Validation Loss: 0.0025\n",
      "Epoch 483/500 - Training Loss: 0.0038 - Validation Loss: 0.0024\n",
      "Epoch 484/500 - Training Loss: 0.0031 - Validation Loss: 0.0031\n",
      "Epoch 485/500 - Training Loss: 0.0047 - Validation Loss: 0.0040\n",
      "Epoch 486/500 - Training Loss: 0.0033 - Validation Loss: 0.0028\n",
      "Epoch 487/500 - Training Loss: 0.0040 - Validation Loss: 0.0026\n",
      "Epoch 488/500 - Training Loss: 0.0035 - Validation Loss: 0.0027\n",
      "Epoch 489/500 - Training Loss: 0.0032 - Validation Loss: 0.0027\n",
      "Epoch 490/500 - Training Loss: 0.0037 - Validation Loss: 0.0026\n",
      "Epoch 491/500 - Training Loss: 0.0036 - Validation Loss: 0.0025\n",
      "Epoch 492/500 - Training Loss: 0.0033 - Validation Loss: 0.0028\n",
      "Epoch 493/500 - Training Loss: 0.0035 - Validation Loss: 0.0032\n",
      "Epoch 494/500 - Training Loss: 0.0040 - Validation Loss: 0.0027\n",
      "Epoch 495/500 - Training Loss: 0.0031 - Validation Loss: 0.0022\n",
      "Epoch 496/500 - Training Loss: 0.0030 - Validation Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:44:50,494] A new study created in memory with name: no-name-4e81fe7d-24ed-447c-8f75-e4ecefdf14d8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497/500 - Training Loss: 0.0034 - Validation Loss: 0.0029\n",
      "Epoch 498/500 - Training Loss: 0.0037 - Validation Loss: 0.0026\n",
      "Epoch 499/500 - Training Loss: 0.0035 - Validation Loss: 0.0025\n",
      "Epoch 500/500 - Training Loss: 0.0037 - Validation Loss: 0.0025\n",
      "Training of the best model completed.\n",
      "\n",
      "===== Model Performance =====\n",
      "Training Set:\n",
      "  R2 Score: 0.8417\n",
      "  MSE: 0.0009\n",
      "  MAE: 0.0206\n",
      "\n",
      "Testing Set:\n",
      "  R2 Score: -254649384960.0000\n",
      "  MSE: 0.0020\n",
      "  MAE: 0.0305\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 14:44:51,199] Trial 0 finished with value: -37.64274978637695 and parameters: {'hidden_layers': 3, 'n_units_l0': 476, 'n_units_l1': 523, 'n_units_l2': 635, 'dropout_rate': 0.24755130238711764, 'lr': 0.004895522054994809, 'optimizer': 'SGD'}. Best is trial 0 with value: -37.64274978637695.\n",
      "[I 2024-09-25 14:44:52,091] Trial 1 finished with value: -19.911584854125977 and parameters: {'hidden_layers': 4, 'n_units_l0': 677, 'n_units_l1': 961, 'n_units_l2': 583, 'n_units_l3': 866, 'dropout_rate': 0.45804928934835526, 'lr': 0.0003184568090529065, 'optimizer': 'SGD'}. Best is trial 1 with value: -19.911584854125977.\n",
      "[I 2024-09-25 14:44:52,850] Trial 2 finished with value: -0.014329949393868446 and parameters: {'hidden_layers': 2, 'n_units_l0': 957, 'n_units_l1': 143, 'dropout_rate': 0.5050678275629154, 'lr': 0.0009755687068765642, 'optimizer': 'Adam'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:53,985] Trial 3 finished with value: -0.7559358477592468 and parameters: {'hidden_layers': 5, 'n_units_l0': 1024, 'n_units_l1': 257, 'n_units_l2': 461, 'n_units_l3': 364, 'n_units_l4': 434, 'dropout_rate': 0.523561688874997, 'lr': 0.00022093942410962008, 'optimizer': 'AdamW'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:54,627] Trial 4 finished with value: -286.0597229003906 and parameters: {'hidden_layers': 2, 'n_units_l0': 995, 'n_units_l1': 923, 'dropout_rate': 0.24820916649767724, 'lr': 0.0005475358222694973, 'optimizer': 'SGD'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:55,391] Trial 5 finished with value: -24.48108673095703 and parameters: {'hidden_layers': 2, 'n_units_l0': 184, 'n_units_l1': 958, 'dropout_rate': 0.29570118671436596, 'lr': 1.0652662062464062e-05, 'optimizer': 'AdamW'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:56,249] Trial 6 finished with value: -1.1199157238006592 and parameters: {'hidden_layers': 3, 'n_units_l0': 137, 'n_units_l1': 766, 'n_units_l2': 175, 'dropout_rate': 0.5726464068584638, 'lr': 0.003338274192215032, 'optimizer': 'RMSprop'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:57,398] Trial 7 finished with value: -0.5395650863647461 and parameters: {'hidden_layers': 4, 'n_units_l0': 347, 'n_units_l1': 321, 'n_units_l2': 539, 'n_units_l3': 560, 'dropout_rate': 0.4560692253427293, 'lr': 0.0003138182218198308, 'optimizer': 'AdamW'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:58,487] Trial 8 finished with value: -3.0478925704956055 and parameters: {'hidden_layers': 5, 'n_units_l0': 408, 'n_units_l1': 137, 'n_units_l2': 579, 'n_units_l3': 807, 'n_units_l4': 349, 'dropout_rate': 0.32163705465267756, 'lr': 0.0008851390676559292, 'optimizer': 'RMSprop'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:44:59,109] Trial 9 finished with value: -161.97689819335938 and parameters: {'hidden_layers': 2, 'n_units_l0': 851, 'n_units_l1': 154, 'dropout_rate': 0.2217053787567679, 'lr': 0.0001816862670259274, 'optimizer': 'SGD'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:45:00,059] Trial 10 finished with value: -4.410774230957031 and parameters: {'hidden_layers': 3, 'n_units_l0': 704, 'n_units_l1': 493, 'n_units_l2': 1017, 'dropout_rate': 0.3977864553256846, 'lr': 4.8132526167465486e-05, 'optimizer': 'Adam'}. Best is trial 2 with value: -0.014329949393868446.\n",
      "[I 2024-09-25 14:45:01,074] Trial 11 finished with value: 0.32803887128829956 and parameters: {'hidden_layers': 4, 'n_units_l0': 310, 'n_units_l1': 330, 'n_units_l2': 847, 'n_units_l3': 189, 'dropout_rate': 0.4770767382630619, 'lr': 0.0016787491813652965, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.32803887128829956.\n",
      "[I 2024-09-25 14:45:02,112] Trial 12 finished with value: 0.26461634039878845 and parameters: {'hidden_layers': 4, 'n_units_l0': 575, 'n_units_l1': 369, 'n_units_l2': 965, 'n_units_l3': 129, 'dropout_rate': 0.5002041172778656, 'lr': 0.0016330557105529815, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.32803887128829956.\n",
      "[I 2024-09-25 14:45:03,099] Trial 13 finished with value: 0.3165161609649658 and parameters: {'hidden_layers': 4, 'n_units_l0': 545, 'n_units_l1': 388, 'n_units_l2': 998, 'n_units_l3': 132, 'dropout_rate': 0.5978045305841473, 'lr': 0.009318455976989489, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.32803887128829956.\n",
      "[I 2024-09-25 14:45:04,144] Trial 14 finished with value: 0.28562676906585693 and parameters: {'hidden_layers': 4, 'n_units_l0': 276, 'n_units_l1': 674, 'n_units_l2': 839, 'n_units_l3': 133, 'dropout_rate': 0.5968182328263858, 'lr': 0.009367974755179455, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.32803887128829956.\n",
      "[I 2024-09-25 14:45:05,326] Trial 15 finished with value: 0.09884917736053467 and parameters: {'hidden_layers': 5, 'n_units_l0': 525, 'n_units_l1': 400, 'n_units_l2': 807, 'n_units_l3': 346, 'n_units_l4': 992, 'dropout_rate': 0.39467799938643766, 'lr': 0.009662756091310122, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.32803887128829956.\n",
      "[I 2024-09-25 14:45:06,374] Trial 16 finished with value: 0.4247196614742279 and parameters: {'hidden_layers': 4, 'n_units_l0': 704, 'n_units_l1': 644, 'n_units_l2': 822, 'n_units_l3': 309, 'dropout_rate': 0.5540796396525376, 'lr': 0.002567021551399634, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:07,305] Trial 17 finished with value: 0.3816417455673218 and parameters: {'hidden_layers': 3, 'n_units_l0': 698, 'n_units_l1': 646, 'n_units_l2': 773, 'dropout_rate': 0.4464922003180269, 'lr': 0.0020933265608572785, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:08,173] Trial 18 finished with value: 0.3029748797416687 and parameters: {'hidden_layers': 3, 'n_units_l0': 722, 'n_units_l1': 687, 'n_units_l2': 750, 'dropout_rate': 0.540046859896453, 'lr': 0.0027069638168497447, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:08,984] Trial 19 finished with value: -3.0095725059509277 and parameters: {'hidden_layers': 3, 'n_units_l0': 801, 'n_units_l1': 798, 'n_units_l2': 380, 'dropout_rate': 0.4269571137147046, 'lr': 9.542738102223113e-05, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:09,797] Trial 20 finished with value: 0.014591439627110958 and parameters: {'hidden_layers': 3, 'n_units_l0': 644, 'n_units_l1': 611, 'n_units_l2': 695, 'dropout_rate': 0.36022683951373025, 'lr': 0.00435149912725109, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:10,891] Trial 21 finished with value: 0.39532917737960815 and parameters: {'hidden_layers': 4, 'n_units_l0': 871, 'n_units_l1': 493, 'n_units_l2': 873, 'n_units_l3': 382, 'dropout_rate': 0.48087686376138356, 'lr': 0.0020023592566149337, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:11,973] Trial 22 finished with value: 0.4044787287712097 and parameters: {'hidden_layers': 4, 'n_units_l0': 842, 'n_units_l1': 492, 'n_units_l2': 917, 'n_units_l3': 475, 'dropout_rate': 0.5540762377222076, 'lr': 0.0013574052077612506, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:13,242] Trial 23 finished with value: 0.18258215487003326 and parameters: {'hidden_layers': 5, 'n_units_l0': 885, 'n_units_l1': 485, 'n_units_l2': 907, 'n_units_l3': 470, 'n_units_l4': 881, 'dropout_rate': 0.5517770494858909, 'lr': 0.0008286029114046264, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:14,176] Trial 24 finished with value: 0.412412166595459 and parameters: {'hidden_layers': 4, 'n_units_l0': 804, 'n_units_l1': 552, 'n_units_l2': 910, 'n_units_l3': 359, 'dropout_rate': 0.5538856570771717, 'lr': 0.0011636717345133991, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:15,284] Trial 25 finished with value: -0.003774271346628666 and parameters: {'hidden_layers': 4, 'n_units_l0': 790, 'n_units_l1': 570, 'n_units_l2': 903, 'n_units_l3': 599, 'dropout_rate': 0.5651763553675256, 'lr': 0.000609842973445277, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:16,494] Trial 26 finished with value: 0.16837096214294434 and parameters: {'hidden_layers': 5, 'n_units_l0': 772, 'n_units_l1': 769, 'n_units_l2': 702, 'n_units_l3': 294, 'n_units_l4': 135, 'dropout_rate': 0.5258047975110216, 'lr': 0.0012213977235403836, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:17,567] Trial 27 finished with value: -30.38880729675293 and parameters: {'hidden_layers': 4, 'n_units_l0': 941, 'n_units_l1': 846, 'n_units_l2': 932, 'n_units_l3': 502, 'dropout_rate': 0.5729399922013562, 'lr': 0.005388209746120459, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:18,672] Trial 28 finished with value: 0.017839867621660233 and parameters: {'hidden_layers': 4, 'n_units_l0': 624, 'n_units_l1': 587, 'n_units_l2': 747, 'n_units_l3': 675, 'dropout_rate': 0.5059852004468082, 'lr': 0.0004571965798684077, 'optimizer': 'AdamW'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:19,711] Trial 29 finished with value: -25.601604461669922 and parameters: {'hidden_layers': 5, 'n_units_l0': 463, 'n_units_l1': 540, 'n_units_l2': 956, 'n_units_l3': 435, 'n_units_l4': 718, 'dropout_rate': 0.5418918017159922, 'lr': 0.0049802349763218715, 'optimizer': 'SGD'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:20,749] Trial 30 finished with value: 0.3257485330104828 and parameters: {'hidden_layers': 4, 'n_units_l0': 749, 'n_units_l1': 446, 'n_units_l2': 299, 'n_units_l3': 254, 'dropout_rate': 0.5843632450351038, 'lr': 0.002815596586012701, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:21,798] Trial 31 finished with value: 0.38961493968963623 and parameters: {'hidden_layers': 4, 'n_units_l0': 858, 'n_units_l1': 463, 'n_units_l2': 867, 'n_units_l3': 387, 'dropout_rate': 0.48288398908966434, 'lr': 0.0015047633816883252, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4247196614742279.\n",
      "[I 2024-09-25 14:45:22,949] Trial 32 finished with value: 0.4274654984474182 and parameters: {'hidden_layers': 4, 'n_units_l0': 894, 'n_units_l1': 707, 'n_units_l2': 888, 'n_units_l3': 281, 'dropout_rate': 0.5228088409941654, 'lr': 0.002320992840176216, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:24,047] Trial 33 finished with value: 0.3228621184825897 and parameters: {'hidden_layers': 4, 'n_units_l0': 919, 'n_units_l1': 723, 'n_units_l2': 815, 'n_units_l3': 308, 'dropout_rate': 0.5230171714461846, 'lr': 0.0011628211494702756, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:25,098] Trial 34 finished with value: 0.3065398037433624 and parameters: {'hidden_layers': 4, 'n_units_l0': 818, 'n_units_l1': 624, 'n_units_l2': 685, 'n_units_l3': 244, 'dropout_rate': 0.5457785726162521, 'lr': 0.003400530424525459, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:26,085] Trial 35 finished with value: -23.526485443115234 and parameters: {'hidden_layers': 5, 'n_units_l0': 974, 'n_units_l1': 859, 'n_units_l2': 949, 'n_units_l3': 446, 'n_units_l4': 637, 'dropout_rate': 0.5173224832823643, 'lr': 0.0007337862439593892, 'optimizer': 'SGD'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:27,107] Trial 36 finished with value: -0.7288123369216919 and parameters: {'hidden_layers': 4, 'n_units_l0': 654, 'n_units_l1': 539, 'n_units_l2': 634, 'n_units_l3': 229, 'dropout_rate': 0.5619069331087542, 'lr': 0.00041231735079818286, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:28,090] Trial 37 finished with value: -0.24535945057868958 and parameters: {'hidden_layers': 3, 'n_units_l0': 918, 'n_units_l1': 718, 'n_units_l2': 1021, 'dropout_rate': 0.4944835639301135, 'lr': 0.005859906081604602, 'optimizer': 'AdamW'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:29,149] Trial 38 finished with value: -0.818822979927063 and parameters: {'hidden_layers': 4, 'n_units_l0': 754, 'n_units_l1': 665, 'n_units_l2': 787, 'n_units_l3': 542, 'dropout_rate': 0.42286537178039213, 'lr': 0.00024199202450561716, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:30,125] Trial 39 finished with value: -1.106266736984253 and parameters: {'hidden_layers': 5, 'n_units_l0': 1005, 'n_units_l1': 566, 'n_units_l2': 515, 'n_units_l3': 682, 'n_units_l4': 131, 'dropout_rate': 0.5790502627018762, 'lr': 0.0022974549093770585, 'optimizer': 'RMSprop'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:30,942] Trial 40 finished with value: -23.040590286254883 and parameters: {'hidden_layers': 4, 'n_units_l0': 824, 'n_units_l1': 1009, 'n_units_l2': 899, 'n_units_l3': 1008, 'dropout_rate': 0.5328823745679884, 'lr': 0.00099888774635496, 'optimizer': 'SGD'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:31,874] Trial 41 finished with value: 0.3435068428516388 and parameters: {'hidden_layers': 4, 'n_units_l0': 854, 'n_units_l1': 441, 'n_units_l2': 873, 'n_units_l3': 363, 'dropout_rate': 0.4597107483522224, 'lr': 0.0020105475672575475, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:32,773] Trial 42 finished with value: -4.26467227935791 and parameters: {'hidden_layers': 4, 'n_units_l0': 872, 'n_units_l1': 514, 'n_units_l2': 853, 'n_units_l3': 410, 'dropout_rate': 0.47622237051761573, 'lr': 1.2602625095074582e-05, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:33,760] Trial 43 finished with value: 0.14553874731063843 and parameters: {'hidden_layers': 4, 'n_units_l0': 898, 'n_units_l1': 611, 'n_units_l2': 977, 'n_units_l3': 282, 'dropout_rate': 0.5148785535247029, 'lr': 0.0035914472403574624, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:34,710] Trial 44 finished with value: 0.3368660509586334 and parameters: {'hidden_layers': 4, 'n_units_l0': 944, 'n_units_l1': 255, 'n_units_l2': 917, 'n_units_l3': 331, 'dropout_rate': 0.5589352794885509, 'lr': 0.0013939795145911998, 'optimizer': 'AdamW'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:35,688] Trial 45 finished with value: 0.4153185486793518 and parameters: {'hidden_layers': 4, 'n_units_l0': 739, 'n_units_l1': 722, 'n_units_l2': 878, 'n_units_l3': 394, 'dropout_rate': 0.49374482763530747, 'lr': 0.00693162860311861, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:36,615] Trial 46 finished with value: 0.09397643804550171 and parameters: {'hidden_layers': 4, 'n_units_l0': 603, 'n_units_l1': 718, 'n_units_l2': 816, 'n_units_l3': 501, 'dropout_rate': 0.5854674863021706, 'lr': 0.007483427319694138, 'optimizer': 'Adam'}. Best is trial 32 with value: 0.4274654984474182.\n",
      "[I 2024-09-25 14:45:37,484] Trial 47 finished with value: 0.44764959812164307 and parameters: {'hidden_layers': 3, 'n_units_l0': 678, 'n_units_l1': 891, 'n_units_l2': 134, 'dropout_rate': 0.5013741799259396, 'lr': 0.006503159357620761, 'optimizer': 'Adam'}. Best is trial 47 with value: 0.44764959812164307.\n",
      "[I 2024-09-25 14:45:38,249] Trial 48 finished with value: 0.5144373774528503 and parameters: {'hidden_layers': 2, 'n_units_l0': 725, 'n_units_l1': 857, 'dropout_rate': 0.4954562001740129, 'lr': 0.006679718204387742, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:39,007] Trial 49 finished with value: 0.476483553647995 and parameters: {'hidden_layers': 2, 'n_units_l0': 679, 'n_units_l1': 881, 'dropout_rate': 0.4607436457519693, 'lr': 0.006834519362814553, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:39,710] Trial 50 finished with value: -17.84600067138672 and parameters: {'hidden_layers': 2, 'n_units_l0': 679, 'n_units_l1': 913, 'dropout_rate': 0.42968583051208553, 'lr': 0.003998636425654425, 'optimizer': 'RMSprop'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:40,448] Trial 51 finished with value: 0.41604357957839966 and parameters: {'hidden_layers': 2, 'n_units_l0': 716, 'n_units_l1': 900, 'dropout_rate': 0.4645790295315976, 'lr': 0.007098040285139856, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:41,234] Trial 52 finished with value: 0.42045533657073975 and parameters: {'hidden_layers': 2, 'n_units_l0': 591, 'n_units_l1': 912, 'dropout_rate': 0.459687233394378, 'lr': 0.006997114138402958, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:41,988] Trial 53 finished with value: 0.3849404752254486 and parameters: {'hidden_layers': 2, 'n_units_l0': 557, 'n_units_l1': 969, 'dropout_rate': 0.4393949620872958, 'lr': 0.005799049360982427, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:42,760] Trial 54 finished with value: 0.4720762073993683 and parameters: {'hidden_layers': 2, 'n_units_l0': 493, 'n_units_l1': 839, 'dropout_rate': 0.4036131097392158, 'lr': 0.008012516035955092, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:43,470] Trial 55 finished with value: 0.48923638463020325 and parameters: {'hidden_layers': 2, 'n_units_l0': 486, 'n_units_l1': 815, 'dropout_rate': 0.38144210325275185, 'lr': 0.0028443951005828185, 'optimizer': 'Adam'}. Best is trial 48 with value: 0.5144373774528503.\n",
      "[I 2024-09-25 14:45:44,199] Trial 56 finished with value: 0.5393936038017273 and parameters: {'hidden_layers': 2, 'n_units_l0': 486, 'n_units_l1': 831, 'dropout_rate': 0.3682497485493311, 'lr': 0.00886381393143796, 'optimizer': 'Adam'}. Best is trial 56 with value: 0.5393936038017273.\n",
      "[I 2024-09-25 14:45:44,913] Trial 57 finished with value: 0.5990438461303711 and parameters: {'hidden_layers': 2, 'n_units_l0': 497, 'n_units_l1': 794, 'dropout_rate': 0.34425511563022093, 'lr': 0.009514867170942408, 'optimizer': 'AdamW'}. Best is trial 57 with value: 0.5990438461303711.\n",
      "[I 2024-09-25 14:45:45,613] Trial 58 finished with value: 0.5558072924613953 and parameters: {'hidden_layers': 2, 'n_units_l0': 481, 'n_units_l1': 816, 'dropout_rate': 0.33395807680909045, 'lr': 0.009673802264798938, 'optimizer': 'AdamW'}. Best is trial 57 with value: 0.5990438461303711.\n",
      "[I 2024-09-25 14:45:46,326] Trial 59 finished with value: 0.5966888666152954 and parameters: {'hidden_layers': 2, 'n_units_l0': 398, 'n_units_l1': 782, 'dropout_rate': 0.330447690563935, 'lr': 0.008733789877817779, 'optimizer': 'AdamW'}. Best is trial 57 with value: 0.5990438461303711.\n",
      "[I 2024-09-25 14:45:47,034] Trial 60 finished with value: 0.5140236020088196 and parameters: {'hidden_layers': 2, 'n_units_l0': 412, 'n_units_l1': 801, 'dropout_rate': 0.32487952453167035, 'lr': 0.00991146424812852, 'optimizer': 'AdamW'}. Best is trial 57 with value: 0.5990438461303711.\n",
      "[I 2024-09-25 14:45:47,749] Trial 61 finished with value: 0.5215259194374084 and parameters: {'hidden_layers': 2, 'n_units_l0': 405, 'n_units_l1': 803, 'dropout_rate': 0.31795284631186627, 'lr': 0.009521151988387126, 'optimizer': 'AdamW'}. Best is trial 57 with value: 0.5990438461303711.\n",
      "[I 2024-09-25 14:45:48,449] Trial 62 finished with value: 0.6171602010726929 and parameters: {'hidden_layers': 2, 'n_units_l0': 392, 'n_units_l1': 768, 'dropout_rate': 0.3167153017767974, 'lr': 0.009539183760416509, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:49,165] Trial 63 finished with value: 0.4215542674064636 and parameters: {'hidden_layers': 2, 'n_units_l0': 360, 'n_units_l1': 768, 'dropout_rate': 0.2830344560003666, 'lr': 0.00437304998218113, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:49,874] Trial 64 finished with value: 0.5493043065071106 and parameters: {'hidden_layers': 2, 'n_units_l0': 418, 'n_units_l1': 757, 'dropout_rate': 0.3425759872691817, 'lr': 0.009047589030487888, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:50,602] Trial 65 finished with value: 0.4856953024864197 and parameters: {'hidden_layers': 2, 'n_units_l0': 434, 'n_units_l1': 747, 'dropout_rate': 0.33896983849997936, 'lr': 0.009946607431678956, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:51,349] Trial 66 finished with value: 0.49119576811790466 and parameters: {'hidden_layers': 2, 'n_units_l0': 249, 'n_units_l1': 947, 'dropout_rate': 0.28635216454793455, 'lr': 0.005255920886694675, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:52,072] Trial 67 finished with value: 0.03841441497206688 and parameters: {'hidden_layers': 2, 'n_units_l0': 374, 'n_units_l1': 817, 'dropout_rate': 0.35404058795477383, 'lr': 0.008985318199835902, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:52,774] Trial 68 finished with value: 0.44188985228538513 and parameters: {'hidden_layers': 2, 'n_units_l0': 520, 'n_units_l1': 784, 'dropout_rate': 0.3120519825991366, 'lr': 0.004127376167472974, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:53,492] Trial 69 finished with value: 0.5455134510993958 and parameters: {'hidden_layers': 2, 'n_units_l0': 445, 'n_units_l1': 756, 'dropout_rate': 0.2542135730441802, 'lr': 0.004900892701291532, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:54,232] Trial 70 finished with value: 0.5305585265159607 and parameters: {'hidden_layers': 2, 'n_units_l0': 326, 'n_units_l1': 751, 'dropout_rate': 0.2588920812499454, 'lr': 0.004832821644864111, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:54,948] Trial 71 finished with value: 0.5618229508399963 and parameters: {'hidden_layers': 2, 'n_units_l0': 333, 'n_units_l1': 751, 'dropout_rate': 0.2421042786690629, 'lr': 0.005106820684792536, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:55,730] Trial 72 finished with value: 0.21627826988697052 and parameters: {'hidden_layers': 2, 'n_units_l0': 279, 'n_units_l1': 830, 'dropout_rate': 0.23524449537877953, 'lr': 0.008352408608882145, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:56,430] Trial 73 finished with value: -5.622974872589111 and parameters: {'hidden_layers': 2, 'n_units_l0': 453, 'n_units_l1': 748, 'dropout_rate': 0.368856174391695, 'lr': 9.642163808665715e-05, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:57,248] Trial 74 finished with value: 0.5031454563140869 and parameters: {'hidden_layers': 2, 'n_units_l0': 387, 'n_units_l1': 688, 'dropout_rate': 0.2693278834845091, 'lr': 0.0033620938839797887, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:58,049] Trial 75 finished with value: 0.4922056198120117 and parameters: {'hidden_layers': 2, 'n_units_l0': 435, 'n_units_l1': 777, 'dropout_rate': 0.20811191000811374, 'lr': 0.005698251013933537, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:58,799] Trial 76 finished with value: 0.5521922707557678 and parameters: {'hidden_layers': 2, 'n_units_l0': 510, 'n_units_l1': 873, 'dropout_rate': 0.3408310495968366, 'lr': 0.008076976760173967, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:45:59,573] Trial 77 finished with value: 0.5740465521812439 and parameters: {'hidden_layers': 2, 'n_units_l0': 512, 'n_units_l1': 868, 'dropout_rate': 0.30363802862362776, 'lr': 0.005033876814086607, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:00,427] Trial 78 finished with value: 0.25235602259635925 and parameters: {'hidden_layers': 3, 'n_units_l0': 524, 'n_units_l1': 943, 'n_units_l2': 309, 'dropout_rate': 0.34363736608712137, 'lr': 0.008314805753201399, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:01,148] Trial 79 finished with value: 0.3500271141529083 and parameters: {'hidden_layers': 2, 'n_units_l0': 330, 'n_units_l1': 861, 'dropout_rate': 0.3085580432167953, 'lr': 0.005752427568179609, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:01,836] Trial 80 finished with value: 0.5553689002990723 and parameters: {'hidden_layers': 2, 'n_units_l0': 214, 'n_units_l1': 869, 'dropout_rate': 0.3312066853848065, 'lr': 0.0039256391102540904, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:02,573] Trial 81 finished with value: 0.22226527333259583 and parameters: {'hidden_layers': 2, 'n_units_l0': 268, 'n_units_l1': 980, 'dropout_rate': 0.3029614220147626, 'lr': 0.007878811465808483, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:03,267] Trial 82 finished with value: 0.5408464670181274 and parameters: {'hidden_layers': 2, 'n_units_l0': 159, 'n_units_l1': 876, 'dropout_rate': 0.3286899847393268, 'lr': 0.004426410122277786, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:03,990] Trial 83 finished with value: 0.5284392833709717 and parameters: {'hidden_layers': 2, 'n_units_l0': 200, 'n_units_l1': 791, 'dropout_rate': 0.29505238331313516, 'lr': 0.006274467578768888, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:04,811] Trial 84 finished with value: 0.4595792889595032 and parameters: {'hidden_layers': 2, 'n_units_l0': 507, 'n_units_l1': 877, 'dropout_rate': 0.33831971804091526, 'lr': 0.003078080577093395, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:05,545] Trial 85 finished with value: 0.5224228501319885 and parameters: {'hidden_layers': 2, 'n_units_l0': 567, 'n_units_l1': 688, 'dropout_rate': 0.35032425612101864, 'lr': 0.0038255670315495522, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:06,417] Trial 86 finished with value: -5.483255386352539 and parameters: {'hidden_layers': 3, 'n_units_l0': 467, 'n_units_l1': 737, 'n_units_l2': 224, 'dropout_rate': 0.3328374218754429, 'lr': 2.765448834440258e-05, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:07,172] Trial 87 finished with value: 0.4436788856983185 and parameters: {'hidden_layers': 2, 'n_units_l0': 300, 'n_units_l1': 932, 'dropout_rate': 0.3798481161665276, 'lr': 0.0052648065308907215, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:07,939] Trial 88 finished with value: 0.5413644909858704 and parameters: {'hidden_layers': 2, 'n_units_l0': 540, 'n_units_l1': 818, 'dropout_rate': 0.27542962189413145, 'lr': 0.007611424548793683, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:08,473] Trial 89 finished with value: -61.12028503417969 and parameters: {'hidden_layers': 2, 'n_units_l0': 215, 'n_units_l1': 854, 'dropout_rate': 0.2979644332848654, 'lr': 0.009922064557543138, 'optimizer': 'SGD'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:09,173] Trial 90 finished with value: 0.6114204525947571 and parameters: {'hidden_layers': 2, 'n_units_l0': 404, 'n_units_l1': 1000, 'dropout_rate': 0.31596164640094365, 'lr': 0.006295084814989544, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:09,942] Trial 91 finished with value: 0.4076899290084839 and parameters: {'hidden_layers': 2, 'n_units_l0': 417, 'n_units_l1': 775, 'dropout_rate': 0.32115393057393715, 'lr': 0.006239953720380851, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:10,766] Trial 92 finished with value: 0.5845107436180115 and parameters: {'hidden_layers': 2, 'n_units_l0': 349, 'n_units_l1': 1015, 'dropout_rate': 0.35834527929958265, 'lr': 0.007690102894916778, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:11,568] Trial 93 finished with value: 0.5456306338310242 and parameters: {'hidden_layers': 2, 'n_units_l0': 387, 'n_units_l1': 1022, 'dropout_rate': 0.3595445675667227, 'lr': 0.007855328792452008, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:12,305] Trial 94 finished with value: 0.4938308298587799 and parameters: {'hidden_layers': 2, 'n_units_l0': 345, 'n_units_l1': 980, 'dropout_rate': 0.31184425444290603, 'lr': 0.00460554289158376, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:13,011] Trial 95 finished with value: 0.49711504578590393 and parameters: {'hidden_layers': 2, 'n_units_l0': 130, 'n_units_l1': 1002, 'dropout_rate': 0.35137833031627586, 'lr': 0.006113015561157168, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:13,708] Trial 96 finished with value: -14.988027572631836 and parameters: {'hidden_layers': 2, 'n_units_l0': 305, 'n_units_l1': 913, 'dropout_rate': 0.38572889521277476, 'lr': 0.0035045930289972385, 'optimizer': 'RMSprop'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:14,442] Trial 97 finished with value: 0.54991215467453 and parameters: {'hidden_layers': 2, 'n_units_l0': 359, 'n_units_l1': 964, 'dropout_rate': 0.36922139467896103, 'lr': 0.002566964577528648, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:15,164] Trial 98 finished with value: 0.4750986695289612 and parameters: {'hidden_layers': 2, 'n_units_l0': 504, 'n_units_l1': 893, 'dropout_rate': 0.33140951255526013, 'lr': 0.0017188890903317715, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:15,915] Trial 99 finished with value: 0.5232970118522644 and parameters: {'hidden_layers': 2, 'n_units_l0': 471, 'n_units_l1': 996, 'dropout_rate': 0.405646245457906, 'lr': 0.00711138034290927, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:16,689] Trial 100 finished with value: 0.5274040102958679 and parameters: {'hidden_layers': 2, 'n_units_l0': 375, 'n_units_l1': 930, 'dropout_rate': 0.3183426263825801, 'lr': 0.005196614735293105, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:17,399] Trial 101 finished with value: 0.4349275529384613 and parameters: {'hidden_layers': 2, 'n_units_l0': 347, 'n_units_l1': 960, 'dropout_rate': 0.3630593464950983, 'lr': 0.0041823262813158405, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:18,163] Trial 102 finished with value: 0.3154168128967285 and parameters: {'hidden_layers': 2, 'n_units_l0': 398, 'n_units_l1': 993, 'dropout_rate': 0.3743338698219633, 'lr': 0.0025097623641586243, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:19,011] Trial 103 finished with value: 0.45528218150138855 and parameters: {'hidden_layers': 2, 'n_units_l0': 550, 'n_units_l1': 963, 'dropout_rate': 0.38982556006026425, 'lr': 0.008293833963548986, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:19,817] Trial 104 finished with value: 0.13928818702697754 and parameters: {'hidden_layers': 2, 'n_units_l0': 356, 'n_units_l1': 1019, 'dropout_rate': 0.34768275619210354, 'lr': 0.00308255315409769, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:20,347] Trial 105 finished with value: -149.41281127929688 and parameters: {'hidden_layers': 2, 'n_units_l0': 430, 'n_units_l1': 869, 'dropout_rate': 0.29220967027087763, 'lr': 0.0066493851364408905, 'optimizer': 'SGD'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:21,074] Trial 106 finished with value: 0.5200250148773193 and parameters: {'hidden_layers': 2, 'n_units_l0': 239, 'n_units_l1': 841, 'dropout_rate': 0.3347488096758813, 'lr': 0.0038106921921613373, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:21,961] Trial 107 finished with value: 0.22493760287761688 and parameters: {'hidden_layers': 3, 'n_units_l0': 450, 'n_units_l1': 946, 'n_units_l2': 433, 'dropout_rate': 0.3572754721871997, 'lr': 0.007249395809651041, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:22,684] Trial 108 finished with value: -6.234602928161621 and parameters: {'hidden_layers': 2, 'n_units_l0': 320, 'n_units_l1': 898, 'dropout_rate': 0.3062980608551339, 'lr': 0.00015911289053294488, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:23,493] Trial 109 finished with value: 0.4418221116065979 and parameters: {'hidden_layers': 2, 'n_units_l0': 285, 'n_units_l1': 807, 'dropout_rate': 0.32375718561244665, 'lr': 0.00566604576812954, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:24,230] Trial 110 finished with value: -12.742137908935547 and parameters: {'hidden_layers': 2, 'n_units_l0': 612, 'n_units_l1': 980, 'dropout_rate': 0.37169235284644375, 'lr': 0.008624004859312908, 'optimizer': 'RMSprop'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:24,970] Trial 111 finished with value: 0.4777163565158844 and parameters: {'hidden_layers': 2, 'n_units_l0': 422, 'n_units_l1': 791, 'dropout_rate': 0.34574128313966346, 'lr': 0.009195507646614235, 'optimizer': 'AdamW'}. Best is trial 62 with value: 0.6171602010726929.\n",
      "[I 2024-09-25 14:46:25,723] Trial 112 finished with value: 0.6356415748596191 and parameters: {'hidden_layers': 2, 'n_units_l0': 391, 'n_units_l1': 701, 'dropout_rate': 0.34276659224531025, 'lr': 0.009909372447747275, 'optimizer': 'AdamW'}. Best is trial 112 with value: 0.6356415748596191.\n",
      "[I 2024-09-25 14:46:26,432] Trial 113 finished with value: 0.6364295482635498 and parameters: {'hidden_layers': 2, 'n_units_l0': 365, 'n_units_l1': 701, 'dropout_rate': 0.23935333427872188, 'lr': 0.006395329500289049, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:27,203] Trial 114 finished with value: 0.4998181462287903 and parameters: {'hidden_layers': 2, 'n_units_l0': 389, 'n_units_l1': 727, 'dropout_rate': 0.267208054580027, 'lr': 0.007612563988346695, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:27,925] Trial 115 finished with value: 0.5617703795433044 and parameters: {'hidden_layers': 2, 'n_units_l0': 369, 'n_units_l1': 646, 'dropout_rate': 0.31337900436390076, 'lr': 0.006483189200877108, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:28,635] Trial 116 finished with value: 0.5974382758140564 and parameters: {'hidden_layers': 2, 'n_units_l0': 338, 'n_units_l1': 656, 'dropout_rate': 0.2378372614537568, 'lr': 0.006547248623014664, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:29,329] Trial 117 finished with value: 0.5001408457756042 and parameters: {'hidden_layers': 2, 'n_units_l0': 338, 'n_units_l1': 704, 'dropout_rate': 0.237243522894038, 'lr': 0.006299246625657708, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:30,074] Trial 118 finished with value: 0.5857111215591431 and parameters: {'hidden_layers': 2, 'n_units_l0': 369, 'n_units_l1': 647, 'dropout_rate': 0.24025389957127796, 'lr': 0.0049426146513149045, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:30,817] Trial 119 finished with value: 0.520918071269989 and parameters: {'hidden_layers': 2, 'n_units_l0': 376, 'n_units_l1': 649, 'dropout_rate': 0.2383243780349638, 'lr': 0.005303987094592449, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:31,591] Trial 120 finished with value: 0.5143240690231323 and parameters: {'hidden_layers': 2, 'n_units_l0': 314, 'n_units_l1': 665, 'dropout_rate': 0.2209660076400572, 'lr': 0.004599089996438425, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:32,297] Trial 121 finished with value: 0.5683955550193787 and parameters: {'hidden_layers': 2, 'n_units_l0': 369, 'n_units_l1': 616, 'dropout_rate': 0.22306987882938759, 'lr': 0.009789069102209612, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:33,016] Trial 122 finished with value: 0.5597556829452515 and parameters: {'hidden_layers': 2, 'n_units_l0': 405, 'n_units_l1': 620, 'dropout_rate': 0.22329176292257907, 'lr': 0.00996117483962534, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:33,765] Trial 123 finished with value: 0.5698572397232056 and parameters: {'hidden_layers': 2, 'n_units_l0': 366, 'n_units_l1': 603, 'dropout_rate': 0.20287144398883708, 'lr': 0.006884523945077104, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:34,516] Trial 124 finished with value: 0.5708194375038147 and parameters: {'hidden_layers': 2, 'n_units_l0': 296, 'n_units_l1': 583, 'dropout_rate': 0.20057763805272666, 'lr': 0.0074237426765105, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:35,237] Trial 125 finished with value: 0.5825260281562805 and parameters: {'hidden_layers': 2, 'n_units_l0': 294, 'n_units_l1': 580, 'dropout_rate': 0.20291643097541787, 'lr': 0.007417671606853541, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:35,828] Trial 126 finished with value: -89.17561340332031 and parameters: {'hidden_layers': 2, 'n_units_l0': 299, 'n_units_l1': 557, 'dropout_rate': 0.20456457243901646, 'lr': 0.007517142052896545, 'optimizer': 'SGD'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:36,549] Trial 127 finished with value: 0.5256031155586243 and parameters: {'hidden_layers': 2, 'n_units_l0': 251, 'n_units_l1': 588, 'dropout_rate': 0.21169791539382077, 'lr': 0.005961587996838931, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:37,289] Trial 128 finished with value: 0.5429628491401672 and parameters: {'hidden_layers': 2, 'n_units_l0': 269, 'n_units_l1': 582, 'dropout_rate': 0.21477849288725423, 'lr': 0.007010120295476822, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:38,026] Trial 129 finished with value: 0.43859514594078064 and parameters: {'hidden_layers': 2, 'n_units_l0': 341, 'n_units_l1': 630, 'dropout_rate': 0.20173671781032523, 'lr': 0.008421482395041242, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:38,729] Trial 130 finished with value: 0.5161926746368408 and parameters: {'hidden_layers': 2, 'n_units_l0': 395, 'n_units_l1': 596, 'dropout_rate': 0.246090198633644, 'lr': 0.004807220859360883, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:39,449] Trial 131 finished with value: 0.5622196793556213 and parameters: {'hidden_layers': 2, 'n_units_l0': 358, 'n_units_l1': 602, 'dropout_rate': 0.22824004005159146, 'lr': 0.009978591951808133, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:40,174] Trial 132 finished with value: 0.5650317668914795 and parameters: {'hidden_layers': 2, 'n_units_l0': 288, 'n_units_l1': 661, 'dropout_rate': 0.21640897191404626, 'lr': 0.008671530476618533, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:40,974] Trial 133 finished with value: 0.48773062229156494 and parameters: {'hidden_layers': 2, 'n_units_l0': 319, 'n_units_l1': 695, 'dropout_rate': 0.2301158681850596, 'lr': 0.006960793063518357, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:41,709] Trial 134 finished with value: 0.5016380548477173 and parameters: {'hidden_layers': 2, 'n_units_l0': 387, 'n_units_l1': 636, 'dropout_rate': 0.255456408539939, 'lr': 0.005713224597254987, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:42,465] Trial 135 finished with value: 0.46845439076423645 and parameters: {'hidden_layers': 2, 'n_units_l0': 362, 'n_units_l1': 675, 'dropout_rate': 0.20916823722986397, 'lr': 0.007938435672776157, 'optimizer': 'AdamW'}. Best is trial 113 with value: 0.6364295482635498.\n",
      "[I 2024-09-25 14:46:43,206] Trial 136 finished with value: 0.6516819000244141 and parameters: {'hidden_layers': 2, 'n_units_l0': 412, 'n_units_l1': 515, 'dropout_rate': 0.20131702059037898, 'lr': 0.0064846940160609495, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:43,912] Trial 137 finished with value: -22.721811294555664 and parameters: {'hidden_layers': 2, 'n_units_l0': 442, 'n_units_l1': 509, 'dropout_rate': 0.20289847008195408, 'lr': 0.006242154605748221, 'optimizer': 'RMSprop'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:44,640] Trial 138 finished with value: 0.6372838020324707 and parameters: {'hidden_layers': 2, 'n_units_l0': 417, 'n_units_l1': 545, 'dropout_rate': 0.24708153629697616, 'lr': 0.005214787032295236, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:45,379] Trial 139 finished with value: 0.5512242317199707 and parameters: {'hidden_layers': 2, 'n_units_l0': 463, 'n_units_l1': 466, 'dropout_rate': 0.2520391224917411, 'lr': 0.004454930266419241, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:46,124] Trial 140 finished with value: 0.6301239132881165 and parameters: {'hidden_layers': 2, 'n_units_l0': 403, 'n_units_l1': 565, 'dropout_rate': 0.27687169860108884, 'lr': 0.005196541570640937, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:46,889] Trial 141 finished with value: 0.5256596803665161 and parameters: {'hidden_layers': 2, 'n_units_l0': 413, 'n_units_l1': 553, 'dropout_rate': 0.283610037616007, 'lr': 0.0053577767000216985, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:47,672] Trial 142 finished with value: 0.5211869478225708 and parameters: {'hidden_layers': 2, 'n_units_l0': 426, 'n_units_l1': 533, 'dropout_rate': 0.2634314272591572, 'lr': 0.0048889760680267715, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:48,436] Trial 143 finished with value: 0.6072458624839783 and parameters: {'hidden_layers': 2, 'n_units_l0': 405, 'n_units_l1': 570, 'dropout_rate': 0.27760821057165963, 'lr': 0.007838174932465415, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:49,188] Trial 144 finished with value: 0.32114750146865845 and parameters: {'hidden_layers': 2, 'n_units_l0': 404, 'n_units_l1': 516, 'dropout_rate': 0.24546590371831942, 'lr': 0.005989830588091698, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:49,942] Trial 145 finished with value: 0.593915581703186 and parameters: {'hidden_layers': 2, 'n_units_l0': 447, 'n_units_l1': 529, 'dropout_rate': 0.27575807320177226, 'lr': 0.008155106263289373, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:50,689] Trial 146 finished with value: 0.5971934795379639 and parameters: {'hidden_layers': 2, 'n_units_l0': 440, 'n_units_l1': 534, 'dropout_rate': 0.2781474162693734, 'lr': 0.008410581173876781, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:51,411] Trial 147 finished with value: 0.5405668020248413 and parameters: {'hidden_layers': 2, 'n_units_l0': 444, 'n_units_l1': 533, 'dropout_rate': 0.2765922847171187, 'lr': 0.008559085092221546, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:52,194] Trial 148 finished with value: 0.6070696711540222 and parameters: {'hidden_layers': 2, 'n_units_l0': 459, 'n_units_l1': 495, 'dropout_rate': 0.27496152989432904, 'lr': 0.008516795993158717, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:52,948] Trial 149 finished with value: 0.4581853151321411 and parameters: {'hidden_layers': 2, 'n_units_l0': 458, 'n_units_l1': 417, 'dropout_rate': 0.27803324020660647, 'lr': 0.008521034566155116, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:53,673] Trial 150 finished with value: 0.6422755718231201 and parameters: {'hidden_layers': 2, 'n_units_l0': 482, 'n_units_l1': 495, 'dropout_rate': 0.28881312191123243, 'lr': 0.006720826965892035, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:54,466] Trial 151 finished with value: 0.6271275877952576 and parameters: {'hidden_layers': 2, 'n_units_l0': 492, 'n_units_l1': 488, 'dropout_rate': 0.2871233423799656, 'lr': 0.0065796310702365525, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:55,170] Trial 152 finished with value: 0.5644776821136475 and parameters: {'hidden_layers': 2, 'n_units_l0': 482, 'n_units_l1': 476, 'dropout_rate': 0.2861066028245927, 'lr': 0.006504684984544756, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:55,949] Trial 153 finished with value: 0.5951971411705017 and parameters: {'hidden_layers': 2, 'n_units_l0': 497, 'n_units_l1': 494, 'dropout_rate': 0.26271361541087646, 'lr': 0.008805829159549769, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:56,516] Trial 154 finished with value: -163.87228393554688 and parameters: {'hidden_layers': 2, 'n_units_l0': 534, 'n_units_l1': 498, 'dropout_rate': 0.2597162407416833, 'lr': 0.009998358961771777, 'optimizer': 'SGD'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:57,299] Trial 155 finished with value: 0.5874316096305847 and parameters: {'hidden_layers': 2, 'n_units_l0': 424, 'n_units_l1': 493, 'dropout_rate': 0.29080537163931086, 'lr': 0.006671145315528031, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:58,089] Trial 156 finished with value: 0.5739634037017822 and parameters: {'hidden_layers': 2, 'n_units_l0': 484, 'n_units_l1': 423, 'dropout_rate': 0.2634891739547304, 'lr': 0.008569015320644009, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:58,848] Trial 157 finished with value: 0.5345796346664429 and parameters: {'hidden_layers': 2, 'n_units_l0': 496, 'n_units_l1': 547, 'dropout_rate': 0.27058819103865633, 'lr': 0.005663313979021585, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:46:59,587] Trial 158 finished with value: 0.27845633029937744 and parameters: {'hidden_layers': 2, 'n_units_l0': 468, 'n_units_l1': 476, 'dropout_rate': 0.2929539634138367, 'lr': 0.008948539634486368, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:00,327] Trial 159 finished with value: 0.4404500424861908 and parameters: {'hidden_layers': 2, 'n_units_l0': 431, 'n_units_l1': 510, 'dropout_rate': 0.300871364951785, 'lr': 0.006703239391862733, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:01,016] Trial 160 finished with value: -96.22312927246094 and parameters: {'hidden_layers': 2, 'n_units_l0': 409, 'n_units_l1': 447, 'dropout_rate': 0.2702987372243708, 'lr': 0.007413096838334928, 'optimizer': 'RMSprop'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:01,737] Trial 161 finished with value: 0.5231040716171265 and parameters: {'hidden_layers': 2, 'n_units_l0': 454, 'n_units_l1': 560, 'dropout_rate': 0.27919492513868566, 'lr': 0.008177230058687176, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:02,474] Trial 162 finished with value: 0.5105152130126953 and parameters: {'hidden_layers': 2, 'n_units_l0': 437, 'n_units_l1': 534, 'dropout_rate': 0.27268081172080355, 'lr': 0.008643957372063334, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:03,198] Trial 163 finished with value: 0.5176406502723694 and parameters: {'hidden_layers': 2, 'n_units_l0': 387, 'n_units_l1': 522, 'dropout_rate': 0.250541966599551, 'lr': 0.0060305822286649624, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:03,908] Trial 164 finished with value: -2.050837278366089 and parameters: {'hidden_layers': 2, 'n_units_l0': 467, 'n_units_l1': 495, 'dropout_rate': 0.28479926574781395, 'lr': 0.00030094835090330684, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:04,635] Trial 165 finished with value: 0.42836275696754456 and parameters: {'hidden_layers': 2, 'n_units_l0': 496, 'n_units_l1': 464, 'dropout_rate': 0.26021655422027856, 'lr': 0.007631386595920295, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:05,377] Trial 166 finished with value: 0.5794988870620728 and parameters: {'hidden_layers': 2, 'n_units_l0': 409, 'n_units_l1': 523, 'dropout_rate': 0.2962779359939486, 'lr': 0.009734673206187549, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:06,073] Trial 167 finished with value: -15.004626274108887 and parameters: {'hidden_layers': 2, 'n_units_l0': 448, 'n_units_l1': 569, 'dropout_rate': 0.30677825170750117, 'lr': 3.436098707090947e-05, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:06,785] Trial 168 finished with value: 0.6466019749641418 and parameters: {'hidden_layers': 2, 'n_units_l0': 399, 'n_units_l1': 362, 'dropout_rate': 0.26625445391958186, 'lr': 0.005641248332860238, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:07,465] Trial 169 finished with value: 0.4461197555065155 and parameters: {'hidden_layers': 2, 'n_units_l0': 395, 'n_units_l1': 392, 'dropout_rate': 0.2667475193177135, 'lr': 0.00396853777792295, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:08,194] Trial 170 finished with value: 0.5915123224258423 and parameters: {'hidden_layers': 2, 'n_units_l0': 421, 'n_units_l1': 303, 'dropout_rate': 0.25016451897731123, 'lr': 0.00553754728041844, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:08,924] Trial 171 finished with value: 0.5891588926315308 and parameters: {'hidden_layers': 2, 'n_units_l0': 515, 'n_units_l1': 306, 'dropout_rate': 0.28232693757340044, 'lr': 0.006841972594625879, 'optimizer': 'AdamW'}. Best is trial 136 with value: 0.6516819000244141.\n",
      "[I 2024-09-25 14:47:09,649] Trial 172 finished with value: 0.6549976468086243 and parameters: {'hidden_layers': 2, 'n_units_l0': 388, 'n_units_l1': 218, 'dropout_rate': 0.2757112941180931, 'lr': 0.0077308138638853136, 'optimizer': 'AdamW'}. Best is trial 172 with value: 0.6549976468086243.\n",
      "[I 2024-09-25 14:47:10,404] Trial 173 finished with value: 0.6018316149711609 and parameters: {'hidden_layers': 2, 'n_units_l0': 379, 'n_units_l1': 439, 'dropout_rate': 0.2582454837328747, 'lr': 0.006108754721593292, 'optimizer': 'AdamW'}. Best is trial 172 with value: 0.6549976468086243.\n",
      "[I 2024-09-25 14:47:11,070] Trial 174 finished with value: 0.6559937000274658 and parameters: {'hidden_layers': 2, 'n_units_l0': 380, 'n_units_l1': 226, 'dropout_rate': 0.31773642384668216, 'lr': 0.005895493149529302, 'optimizer': 'AdamW'}. Best is trial 174 with value: 0.6559937000274658.\n",
      "[I 2024-09-25 14:47:11,771] Trial 175 finished with value: 0.6425415277481079 and parameters: {'hidden_layers': 2, 'n_units_l0': 389, 'n_units_l1': 226, 'dropout_rate': 0.28950101977197135, 'lr': 0.0054891661947320215, 'optimizer': 'AdamW'}. Best is trial 174 with value: 0.6559937000274658.\n",
      "[I 2024-09-25 14:47:12,501] Trial 176 finished with value: 0.6755660176277161 and parameters: {'hidden_layers': 2, 'n_units_l0': 381, 'n_units_l1': 157, 'dropout_rate': 0.2907335625657539, 'lr': 0.004372258491601088, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:13,246] Trial 177 finished with value: 0.5653681755065918 and parameters: {'hidden_layers': 2, 'n_units_l0': 390, 'n_units_l1': 200, 'dropout_rate': 0.29193018213866345, 'lr': 0.0035461000356176433, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:13,972] Trial 178 finished with value: 0.2707495093345642 and parameters: {'hidden_layers': 2, 'n_units_l0': 380, 'n_units_l1': 168, 'dropout_rate': 0.3154564383679979, 'lr': 0.004345043915792669, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:14,687] Trial 179 finished with value: 0.6549955606460571 and parameters: {'hidden_layers': 2, 'n_units_l0': 406, 'n_units_l1': 178, 'dropout_rate': 0.300482252671351, 'lr': 0.004844527893417053, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:15,436] Trial 180 finished with value: 0.5371702909469604 and parameters: {'hidden_layers': 2, 'n_units_l0': 408, 'n_units_l1': 240, 'dropout_rate': 0.29793467715580824, 'lr': 0.004820788180626763, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:16,142] Trial 181 finished with value: 0.6574370265007019 and parameters: {'hidden_layers': 2, 'n_units_l0': 377, 'n_units_l1': 189, 'dropout_rate': 0.3017652321746811, 'lr': 0.005425443140209567, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:16,824] Trial 182 finished with value: 0.5704030394554138 and parameters: {'hidden_layers': 2, 'n_units_l0': 377, 'n_units_l1': 182, 'dropout_rate': 0.2882375160579697, 'lr': 0.005351467128343944, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:17,516] Trial 183 finished with value: 0.516749918460846 and parameters: {'hidden_layers': 2, 'n_units_l0': 397, 'n_units_l1': 137, 'dropout_rate': 0.3038482077483482, 'lr': 0.005555778867139614, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:18,242] Trial 184 finished with value: 0.5759324431419373 and parameters: {'hidden_layers': 2, 'n_units_l0': 354, 'n_units_l1': 208, 'dropout_rate': 0.30871990477408795, 'lr': 0.004244956090794739, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:18,973] Trial 185 finished with value: 0.5534785985946655 and parameters: {'hidden_layers': 2, 'n_units_l0': 375, 'n_units_l1': 221, 'dropout_rate': 0.32188389451108834, 'lr': 0.0031457339832696198, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:19,649] Trial 186 finished with value: -2.233250141143799 and parameters: {'hidden_layers': 2, 'n_units_l0': 419, 'n_units_l1': 159, 'dropout_rate': 0.3003032510541768, 'lr': 0.0006030231607023211, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:20,225] Trial 187 finished with value: -270.4032287597656 and parameters: {'hidden_layers': 2, 'n_units_l0': 390, 'n_units_l1': 257, 'dropout_rate': 0.28632004247607706, 'lr': 0.004872520928883308, 'optimizer': 'SGD'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:20,989] Trial 188 finished with value: 0.5602267980575562 and parameters: {'hidden_layers': 2, 'n_units_l0': 411, 'n_units_l1': 357, 'dropout_rate': 0.27278098723647104, 'lr': 0.005972187405690224, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:21,649] Trial 189 finished with value: 0.6361626386642456 and parameters: {'hidden_layers': 2, 'n_units_l0': 350, 'n_units_l1': 176, 'dropout_rate': 0.2946789156910667, 'lr': 0.0037133495184688003, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:22,340] Trial 190 finished with value: 0.6582697033882141 and parameters: {'hidden_layers': 2, 'n_units_l0': 352, 'n_units_l1': 181, 'dropout_rate': 0.3137386939270809, 'lr': 0.0035582844495090477, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:23,064] Trial 191 finished with value: 0.5511383414268494 and parameters: {'hidden_layers': 2, 'n_units_l0': 348, 'n_units_l1': 183, 'dropout_rate': 0.3154552219817089, 'lr': 0.0041649453890600235, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:23,790] Trial 192 finished with value: 0.5965659618377686 and parameters: {'hidden_layers': 2, 'n_units_l0': 326, 'n_units_l1': 220, 'dropout_rate': 0.2924695421160452, 'lr': 0.0034160506280291243, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:24,486] Trial 193 finished with value: 0.6316903233528137 and parameters: {'hidden_layers': 2, 'n_units_l0': 360, 'n_units_l1': 147, 'dropout_rate': 0.29992973041153614, 'lr': 0.0038293992990112717, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:25,175] Trial 194 finished with value: 0.5830193758010864 and parameters: {'hidden_layers': 2, 'n_units_l0': 357, 'n_units_l1': 150, 'dropout_rate': 0.3102877641849252, 'lr': 0.004520184003846975, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:25,947] Trial 195 finished with value: 0.5589240193367004 and parameters: {'hidden_layers': 2, 'n_units_l0': 366, 'n_units_l1': 185, 'dropout_rate': 0.32610641721582817, 'lr': 0.002793115165735277, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:26,673] Trial 196 finished with value: 0.4902277886867523 and parameters: {'hidden_layers': 2, 'n_units_l0': 332, 'n_units_l1': 266, 'dropout_rate': 0.2940336685207461, 'lr': 0.0036854034428794157, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:27,475] Trial 197 finished with value: 0.6226159930229187 and parameters: {'hidden_layers': 2, 'n_units_l0': 395, 'n_units_l1': 200, 'dropout_rate': 0.3051647757382575, 'lr': 0.005024662496277049, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:28,225] Trial 198 finished with value: -1.038959264755249 and parameters: {'hidden_layers': 2, 'n_units_l0': 346, 'n_units_l1': 136, 'dropout_rate': 0.30018784540196025, 'lr': 0.003225217923622664, 'optimizer': 'RMSprop'}. Best is trial 176 with value: 0.6755660176277161.\n",
      "[I 2024-09-25 14:47:29,101] Trial 199 finished with value: 0.6096339225769043 and parameters: {'hidden_layers': 2, 'n_units_l0': 381, 'n_units_l1': 169, 'dropout_rate': 0.3103739479237172, 'lr': 0.0040604687317330615, 'optimizer': 'AdamW'}. Best is trial 176 with value: 0.6755660176277161.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization completed.\n",
      "Best Trial:\n",
      "  Value (MSE): 0.6755660176277161\n",
      "  Params:\n",
      "    hidden_layers: 2\n",
      "    n_units_l0: 381\n",
      "    n_units_l1: 157\n",
      "    dropout_rate: 0.2907335625657539\n",
      "    lr: 0.004372258491601088\n",
      "    optimizer: AdamW\n",
      "Starting training of the best model...\n",
      "Epoch 1/500 - Training Loss: 0.1780 - Validation Loss: 0.0290\n",
      "Epoch 2/500 - Training Loss: 0.0520 - Validation Loss: 0.0306\n",
      "Epoch 3/500 - Training Loss: 0.0408 - Validation Loss: 0.0211\n",
      "Epoch 4/500 - Training Loss: 0.0314 - Validation Loss: 0.0243\n",
      "Epoch 5/500 - Training Loss: 0.0272 - Validation Loss: 0.0159\n",
      "Epoch 6/500 - Training Loss: 0.0237 - Validation Loss: 0.0146\n",
      "Epoch 7/500 - Training Loss: 0.0219 - Validation Loss: 0.0129\n",
      "Epoch 8/500 - Training Loss: 0.0191 - Validation Loss: 0.0128\n",
      "Epoch 9/500 - Training Loss: 0.0181 - Validation Loss: 0.0097\n",
      "Epoch 10/500 - Training Loss: 0.0171 - Validation Loss: 0.0096\n",
      "Epoch 11/500 - Training Loss: 0.0160 - Validation Loss: 0.0116\n",
      "Epoch 12/500 - Training Loss: 0.0184 - Validation Loss: 0.0085\n",
      "Epoch 13/500 - Training Loss: 0.0166 - Validation Loss: 0.0101\n",
      "Epoch 14/500 - Training Loss: 0.0148 - Validation Loss: 0.0080\n",
      "Epoch 15/500 - Training Loss: 0.0166 - Validation Loss: 0.0108\n",
      "Epoch 16/500 - Training Loss: 0.0140 - Validation Loss: 0.0062\n",
      "Epoch 17/500 - Training Loss: 0.0131 - Validation Loss: 0.0069\n",
      "Epoch 18/500 - Training Loss: 0.0123 - Validation Loss: 0.0066\n",
      "Epoch 19/500 - Training Loss: 0.0114 - Validation Loss: 0.0056\n",
      "Epoch 20/500 - Training Loss: 0.0111 - Validation Loss: 0.0048\n",
      "Epoch 21/500 - Training Loss: 0.0105 - Validation Loss: 0.0099\n",
      "Epoch 22/500 - Training Loss: 0.0108 - Validation Loss: 0.0056\n",
      "Epoch 23/500 - Training Loss: 0.0100 - Validation Loss: 0.0069\n",
      "Epoch 24/500 - Training Loss: 0.0109 - Validation Loss: 0.0049\n",
      "Epoch 25/500 - Training Loss: 0.0085 - Validation Loss: 0.0043\n",
      "Epoch 26/500 - Training Loss: 0.0099 - Validation Loss: 0.0054\n",
      "Epoch 27/500 - Training Loss: 0.0091 - Validation Loss: 0.0052\n",
      "Epoch 28/500 - Training Loss: 0.0086 - Validation Loss: 0.0055\n",
      "Epoch 29/500 - Training Loss: 0.0092 - Validation Loss: 0.0048\n",
      "Epoch 30/500 - Training Loss: 0.0104 - Validation Loss: 0.0056\n",
      "Epoch 31/500 - Training Loss: 0.0106 - Validation Loss: 0.0057\n",
      "Epoch 32/500 - Training Loss: 0.0096 - Validation Loss: 0.0049\n",
      "Epoch 33/500 - Training Loss: 0.0091 - Validation Loss: 0.0052\n",
      "Epoch 34/500 - Training Loss: 0.0082 - Validation Loss: 0.0054\n",
      "Epoch 35/500 - Training Loss: 0.0083 - Validation Loss: 0.0043\n",
      "Epoch 36/500 - Training Loss: 0.0083 - Validation Loss: 0.0045\n",
      "Epoch 37/500 - Training Loss: 0.0078 - Validation Loss: 0.0040\n",
      "Epoch 38/500 - Training Loss: 0.0085 - Validation Loss: 0.0052\n",
      "Epoch 39/500 - Training Loss: 0.0084 - Validation Loss: 0.0043\n",
      "Epoch 40/500 - Training Loss: 0.0079 - Validation Loss: 0.0041\n",
      "Epoch 41/500 - Training Loss: 0.0073 - Validation Loss: 0.0058\n",
      "Epoch 42/500 - Training Loss: 0.0083 - Validation Loss: 0.0035\n",
      "Epoch 43/500 - Training Loss: 0.0074 - Validation Loss: 0.0045\n",
      "Epoch 44/500 - Training Loss: 0.0087 - Validation Loss: 0.0039\n",
      "Epoch 45/500 - Training Loss: 0.0078 - Validation Loss: 0.0037\n",
      "Epoch 46/500 - Training Loss: 0.0065 - Validation Loss: 0.0039\n",
      "Epoch 47/500 - Training Loss: 0.0073 - Validation Loss: 0.0043\n",
      "Epoch 48/500 - Training Loss: 0.0061 - Validation Loss: 0.0046\n",
      "Epoch 49/500 - Training Loss: 0.0075 - Validation Loss: 0.0040\n",
      "Epoch 50/500 - Training Loss: 0.0066 - Validation Loss: 0.0040\n",
      "Epoch 51/500 - Training Loss: 0.0065 - Validation Loss: 0.0032\n",
      "Epoch 52/500 - Training Loss: 0.0063 - Validation Loss: 0.0035\n",
      "Epoch 53/500 - Training Loss: 0.0061 - Validation Loss: 0.0036\n",
      "Epoch 54/500 - Training Loss: 0.0057 - Validation Loss: 0.0031\n",
      "Epoch 55/500 - Training Loss: 0.0060 - Validation Loss: 0.0038\n",
      "Epoch 56/500 - Training Loss: 0.0055 - Validation Loss: 0.0039\n",
      "Epoch 57/500 - Training Loss: 0.0056 - Validation Loss: 0.0041\n",
      "Epoch 58/500 - Training Loss: 0.0060 - Validation Loss: 0.0033\n",
      "Epoch 59/500 - Training Loss: 0.0057 - Validation Loss: 0.0034\n",
      "Epoch 60/500 - Training Loss: 0.0057 - Validation Loss: 0.0033\n",
      "Epoch 61/500 - Training Loss: 0.0053 - Validation Loss: 0.0036\n",
      "Epoch 62/500 - Training Loss: 0.0061 - Validation Loss: 0.0038\n",
      "Epoch 63/500 - Training Loss: 0.0054 - Validation Loss: 0.0036\n",
      "Epoch 64/500 - Training Loss: 0.0048 - Validation Loss: 0.0041\n",
      "Epoch 65/500 - Training Loss: 0.0056 - Validation Loss: 0.0035\n",
      "Epoch 66/500 - Training Loss: 0.0050 - Validation Loss: 0.0033\n",
      "Epoch 67/500 - Training Loss: 0.0049 - Validation Loss: 0.0033\n",
      "Epoch 68/500 - Training Loss: 0.0050 - Validation Loss: 0.0034\n",
      "Epoch 69/500 - Training Loss: 0.0055 - Validation Loss: 0.0036\n",
      "Epoch 70/500 - Training Loss: 0.0053 - Validation Loss: 0.0033\n",
      "Epoch 71/500 - Training Loss: 0.0060 - Validation Loss: 0.0040\n",
      "Epoch 72/500 - Training Loss: 0.0049 - Validation Loss: 0.0032\n",
      "Epoch 73/500 - Training Loss: 0.0049 - Validation Loss: 0.0031\n",
      "Epoch 74/500 - Training Loss: 0.0042 - Validation Loss: 0.0038\n",
      "Epoch 75/500 - Training Loss: 0.0049 - Validation Loss: 0.0035\n",
      "Epoch 76/500 - Training Loss: 0.0055 - Validation Loss: 0.0036\n",
      "Epoch 77/500 - Training Loss: 0.0046 - Validation Loss: 0.0036\n",
      "Epoch 78/500 - Training Loss: 0.0048 - Validation Loss: 0.0035\n",
      "Epoch 79/500 - Training Loss: 0.0048 - Validation Loss: 0.0032\n",
      "Epoch 80/500 - Training Loss: 0.0045 - Validation Loss: 0.0036\n",
      "Epoch 81/500 - Training Loss: 0.0049 - Validation Loss: 0.0031\n",
      "Epoch 82/500 - Training Loss: 0.0052 - Validation Loss: 0.0033\n",
      "Epoch 83/500 - Training Loss: 0.0051 - Validation Loss: 0.0042\n",
      "Epoch 84/500 - Training Loss: 0.0049 - Validation Loss: 0.0032\n",
      "Epoch 85/500 - Training Loss: 0.0041 - Validation Loss: 0.0040\n",
      "Epoch 86/500 - Training Loss: 0.0053 - Validation Loss: 0.0034\n",
      "Epoch 87/500 - Training Loss: 0.0041 - Validation Loss: 0.0036\n",
      "Epoch 88/500 - Training Loss: 0.0044 - Validation Loss: 0.0034\n",
      "Epoch 89/500 - Training Loss: 0.0043 - Validation Loss: 0.0034\n",
      "Epoch 90/500 - Training Loss: 0.0046 - Validation Loss: 0.0035\n",
      "Epoch 91/500 - Training Loss: 0.0041 - Validation Loss: 0.0031\n",
      "Epoch 92/500 - Training Loss: 0.0039 - Validation Loss: 0.0033\n",
      "Epoch 93/500 - Training Loss: 0.0046 - Validation Loss: 0.0032\n",
      "Epoch 94/500 - Training Loss: 0.0044 - Validation Loss: 0.0030\n",
      "Epoch 95/500 - Training Loss: 0.0043 - Validation Loss: 0.0039\n",
      "Epoch 96/500 - Training Loss: 0.0045 - Validation Loss: 0.0033\n",
      "Epoch 97/500 - Training Loss: 0.0041 - Validation Loss: 0.0031\n",
      "Epoch 98/500 - Training Loss: 0.0039 - Validation Loss: 0.0032\n",
      "Epoch 99/500 - Training Loss: 0.0034 - Validation Loss: 0.0032\n",
      "Epoch 100/500 - Training Loss: 0.0044 - Validation Loss: 0.0035\n",
      "Epoch 101/500 - Training Loss: 0.0038 - Validation Loss: 0.0036\n",
      "Epoch 102/500 - Training Loss: 0.0040 - Validation Loss: 0.0036\n",
      "Epoch 103/500 - Training Loss: 0.0037 - Validation Loss: 0.0035\n",
      "Epoch 104/500 - Training Loss: 0.0039 - Validation Loss: 0.0035\n",
      "Epoch 105/500 - Training Loss: 0.0041 - Validation Loss: 0.0031\n",
      "Epoch 106/500 - Training Loss: 0.0036 - Validation Loss: 0.0031\n",
      "Epoch 107/500 - Training Loss: 0.0038 - Validation Loss: 0.0034\n",
      "Epoch 108/500 - Training Loss: 0.0036 - Validation Loss: 0.0031\n",
      "Epoch 109/500 - Training Loss: 0.0034 - Validation Loss: 0.0033\n",
      "Epoch 110/500 - Training Loss: 0.0033 - Validation Loss: 0.0029\n",
      "Epoch 111/500 - Training Loss: 0.0038 - Validation Loss: 0.0031\n",
      "Epoch 112/500 - Training Loss: 0.0039 - Validation Loss: 0.0037\n",
      "Epoch 113/500 - Training Loss: 0.0035 - Validation Loss: 0.0033\n",
      "Epoch 114/500 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
      "Epoch 115/500 - Training Loss: 0.0037 - Validation Loss: 0.0030\n",
      "Epoch 116/500 - Training Loss: 0.0038 - Validation Loss: 0.0035\n",
      "Epoch 117/500 - Training Loss: 0.0042 - Validation Loss: 0.0033\n",
      "Epoch 118/500 - Training Loss: 0.0037 - Validation Loss: 0.0033\n",
      "Epoch 119/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 120/500 - Training Loss: 0.0038 - Validation Loss: 0.0030\n",
      "Epoch 121/500 - Training Loss: 0.0035 - Validation Loss: 0.0033\n",
      "Epoch 122/500 - Training Loss: 0.0034 - Validation Loss: 0.0034\n",
      "Epoch 123/500 - Training Loss: 0.0032 - Validation Loss: 0.0032\n",
      "Epoch 124/500 - Training Loss: 0.0034 - Validation Loss: 0.0027\n",
      "Epoch 125/500 - Training Loss: 0.0029 - Validation Loss: 0.0031\n",
      "Epoch 126/500 - Training Loss: 0.0032 - Validation Loss: 0.0028\n",
      "Epoch 127/500 - Training Loss: 0.0040 - Validation Loss: 0.0029\n",
      "Epoch 128/500 - Training Loss: 0.0035 - Validation Loss: 0.0035\n",
      "Epoch 129/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 130/500 - Training Loss: 0.0034 - Validation Loss: 0.0027\n",
      "Epoch 131/500 - Training Loss: 0.0031 - Validation Loss: 0.0031\n",
      "Epoch 132/500 - Training Loss: 0.0037 - Validation Loss: 0.0027\n",
      "Epoch 133/500 - Training Loss: 0.0032 - Validation Loss: 0.0032\n",
      "Epoch 134/500 - Training Loss: 0.0036 - Validation Loss: 0.0029\n",
      "Epoch 135/500 - Training Loss: 0.0031 - Validation Loss: 0.0031\n",
      "Epoch 136/500 - Training Loss: 0.0039 - Validation Loss: 0.0041\n",
      "Epoch 137/500 - Training Loss: 0.0036 - Validation Loss: 0.0030\n",
      "Epoch 138/500 - Training Loss: 0.0039 - Validation Loss: 0.0027\n",
      "Epoch 139/500 - Training Loss: 0.0037 - Validation Loss: 0.0036\n",
      "Epoch 140/500 - Training Loss: 0.0032 - Validation Loss: 0.0024\n",
      "Epoch 141/500 - Training Loss: 0.0031 - Validation Loss: 0.0027\n",
      "Epoch 142/500 - Training Loss: 0.0034 - Validation Loss: 0.0033\n",
      "Epoch 143/500 - Training Loss: 0.0033 - Validation Loss: 0.0030\n",
      "Epoch 144/500 - Training Loss: 0.0028 - Validation Loss: 0.0029\n",
      "Epoch 145/500 - Training Loss: 0.0032 - Validation Loss: 0.0029\n",
      "Epoch 146/500 - Training Loss: 0.0029 - Validation Loss: 0.0030\n",
      "Epoch 147/500 - Training Loss: 0.0033 - Validation Loss: 0.0027\n",
      "Epoch 148/500 - Training Loss: 0.0036 - Validation Loss: 0.0030\n",
      "Epoch 149/500 - Training Loss: 0.0026 - Validation Loss: 0.0028\n",
      "Epoch 150/500 - Training Loss: 0.0030 - Validation Loss: 0.0028\n",
      "Epoch 151/500 - Training Loss: 0.0026 - Validation Loss: 0.0031\n",
      "Epoch 152/500 - Training Loss: 0.0030 - Validation Loss: 0.0031\n",
      "Epoch 153/500 - Training Loss: 0.0034 - Validation Loss: 0.0030\n",
      "Epoch 154/500 - Training Loss: 0.0026 - Validation Loss: 0.0028\n",
      "Epoch 155/500 - Training Loss: 0.0035 - Validation Loss: 0.0028\n",
      "Epoch 156/500 - Training Loss: 0.0026 - Validation Loss: 0.0027\n",
      "Epoch 157/500 - Training Loss: 0.0034 - Validation Loss: 0.0027\n",
      "Epoch 158/500 - Training Loss: 0.0030 - Validation Loss: 0.0031\n",
      "Epoch 159/500 - Training Loss: 0.0032 - Validation Loss: 0.0028\n",
      "Epoch 160/500 - Training Loss: 0.0026 - Validation Loss: 0.0027\n",
      "Epoch 161/500 - Training Loss: 0.0030 - Validation Loss: 0.0026\n",
      "Epoch 162/500 - Training Loss: 0.0036 - Validation Loss: 0.0027\n",
      "Epoch 163/500 - Training Loss: 0.0031 - Validation Loss: 0.0033\n",
      "Epoch 164/500 - Training Loss: 0.0030 - Validation Loss: 0.0028\n",
      "Epoch 165/500 - Training Loss: 0.0031 - Validation Loss: 0.0034\n",
      "Epoch 166/500 - Training Loss: 0.0033 - Validation Loss: 0.0028\n",
      "Epoch 167/500 - Training Loss: 0.0034 - Validation Loss: 0.0025\n",
      "Epoch 168/500 - Training Loss: 0.0027 - Validation Loss: 0.0032\n",
      "Epoch 169/500 - Training Loss: 0.0032 - Validation Loss: 0.0027\n",
      "Epoch 170/500 - Training Loss: 0.0028 - Validation Loss: 0.0032\n",
      "Epoch 171/500 - Training Loss: 0.0029 - Validation Loss: 0.0028\n",
      "Epoch 172/500 - Training Loss: 0.0027 - Validation Loss: 0.0027\n",
      "Epoch 173/500 - Training Loss: 0.0033 - Validation Loss: 0.0031\n",
      "Epoch 174/500 - Training Loss: 0.0028 - Validation Loss: 0.0027\n",
      "Epoch 175/500 - Training Loss: 0.0028 - Validation Loss: 0.0024\n",
      "Epoch 176/500 - Training Loss: 0.0022 - Validation Loss: 0.0027\n",
      "Epoch 177/500 - Training Loss: 0.0024 - Validation Loss: 0.0029\n",
      "Epoch 178/500 - Training Loss: 0.0029 - Validation Loss: 0.0027\n",
      "Epoch 179/500 - Training Loss: 0.0026 - Validation Loss: 0.0027\n",
      "Epoch 180/500 - Training Loss: 0.0027 - Validation Loss: 0.0026\n",
      "Epoch 181/500 - Training Loss: 0.0024 - Validation Loss: 0.0030\n",
      "Epoch 182/500 - Training Loss: 0.0026 - Validation Loss: 0.0026\n",
      "Epoch 183/500 - Training Loss: 0.0030 - Validation Loss: 0.0029\n",
      "Epoch 184/500 - Training Loss: 0.0024 - Validation Loss: 0.0029\n",
      "Epoch 185/500 - Training Loss: 0.0025 - Validation Loss: 0.0028\n",
      "Epoch 186/500 - Training Loss: 0.0030 - Validation Loss: 0.0030\n",
      "Epoch 187/500 - Training Loss: 0.0028 - Validation Loss: 0.0029\n",
      "Epoch 188/500 - Training Loss: 0.0027 - Validation Loss: 0.0028\n",
      "Epoch 189/500 - Training Loss: 0.0028 - Validation Loss: 0.0029\n",
      "Epoch 190/500 - Training Loss: 0.0024 - Validation Loss: 0.0028\n",
      "Epoch 191/500 - Training Loss: 0.0026 - Validation Loss: 0.0027\n",
      "Epoch 192/500 - Training Loss: 0.0027 - Validation Loss: 0.0030\n",
      "Epoch 193/500 - Training Loss: 0.0026 - Validation Loss: 0.0028\n",
      "Epoch 194/500 - Training Loss: 0.0028 - Validation Loss: 0.0028\n",
      "Epoch 195/500 - Training Loss: 0.0023 - Validation Loss: 0.0025\n",
      "Epoch 196/500 - Training Loss: 0.0025 - Validation Loss: 0.0023\n",
      "Epoch 197/500 - Training Loss: 0.0028 - Validation Loss: 0.0029\n",
      "Epoch 198/500 - Training Loss: 0.0023 - Validation Loss: 0.0027\n",
      "Epoch 199/500 - Training Loss: 0.0025 - Validation Loss: 0.0025\n",
      "Epoch 200/500 - Training Loss: 0.0022 - Validation Loss: 0.0029\n",
      "Epoch 201/500 - Training Loss: 0.0024 - Validation Loss: 0.0027\n",
      "Epoch 202/500 - Training Loss: 0.0024 - Validation Loss: 0.0026\n",
      "Epoch 203/500 - Training Loss: 0.0023 - Validation Loss: 0.0027\n",
      "Epoch 204/500 - Training Loss: 0.0027 - Validation Loss: 0.0029\n",
      "Epoch 205/500 - Training Loss: 0.0025 - Validation Loss: 0.0029\n",
      "Epoch 206/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 207/500 - Training Loss: 0.0024 - Validation Loss: 0.0028\n",
      "Epoch 208/500 - Training Loss: 0.0029 - Validation Loss: 0.0024\n",
      "Epoch 209/500 - Training Loss: 0.0026 - Validation Loss: 0.0028\n",
      "Epoch 210/500 - Training Loss: 0.0024 - Validation Loss: 0.0030\n",
      "Epoch 211/500 - Training Loss: 0.0021 - Validation Loss: 0.0027\n",
      "Epoch 212/500 - Training Loss: 0.0025 - Validation Loss: 0.0026\n",
      "Epoch 213/500 - Training Loss: 0.0021 - Validation Loss: 0.0028\n",
      "Epoch 214/500 - Training Loss: 0.0027 - Validation Loss: 0.0024\n",
      "Epoch 215/500 - Training Loss: 0.0023 - Validation Loss: 0.0028\n",
      "Epoch 216/500 - Training Loss: 0.0023 - Validation Loss: 0.0027\n",
      "Epoch 217/500 - Training Loss: 0.0022 - Validation Loss: 0.0030\n",
      "Epoch 218/500 - Training Loss: 0.0020 - Validation Loss: 0.0030\n",
      "Epoch 219/500 - Training Loss: 0.0024 - Validation Loss: 0.0028\n",
      "Epoch 220/500 - Training Loss: 0.0022 - Validation Loss: 0.0024\n",
      "Epoch 221/500 - Training Loss: 0.0024 - Validation Loss: 0.0026\n",
      "Epoch 222/500 - Training Loss: 0.0026 - Validation Loss: 0.0030\n",
      "Epoch 223/500 - Training Loss: 0.0028 - Validation Loss: 0.0026\n",
      "Epoch 224/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 225/500 - Training Loss: 0.0022 - Validation Loss: 0.0026\n",
      "Epoch 226/500 - Training Loss: 0.0025 - Validation Loss: 0.0029\n",
      "Epoch 227/500 - Training Loss: 0.0027 - Validation Loss: 0.0034\n",
      "Epoch 228/500 - Training Loss: 0.0028 - Validation Loss: 0.0027\n",
      "Epoch 229/500 - Training Loss: 0.0024 - Validation Loss: 0.0035\n",
      "Epoch 230/500 - Training Loss: 0.0025 - Validation Loss: 0.0026\n",
      "Epoch 231/500 - Training Loss: 0.0024 - Validation Loss: 0.0025\n",
      "Epoch 232/500 - Training Loss: 0.0021 - Validation Loss: 0.0029\n",
      "Epoch 233/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 234/500 - Training Loss: 0.0026 - Validation Loss: 0.0026\n",
      "Epoch 235/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 236/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 237/500 - Training Loss: 0.0020 - Validation Loss: 0.0029\n",
      "Epoch 238/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 239/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 240/500 - Training Loss: 0.0023 - Validation Loss: 0.0024\n",
      "Epoch 241/500 - Training Loss: 0.0026 - Validation Loss: 0.0022\n",
      "Epoch 242/500 - Training Loss: 0.0022 - Validation Loss: 0.0026\n",
      "Epoch 243/500 - Training Loss: 0.0021 - Validation Loss: 0.0025\n",
      "Epoch 244/500 - Training Loss: 0.0022 - Validation Loss: 0.0026\n",
      "Epoch 245/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 246/500 - Training Loss: 0.0021 - Validation Loss: 0.0022\n",
      "Epoch 247/500 - Training Loss: 0.0023 - Validation Loss: 0.0023\n",
      "Epoch 248/500 - Training Loss: 0.0023 - Validation Loss: 0.0026\n",
      "Epoch 249/500 - Training Loss: 0.0020 - Validation Loss: 0.0028\n",
      "Epoch 250/500 - Training Loss: 0.0021 - Validation Loss: 0.0025\n",
      "Epoch 251/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 252/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 253/500 - Training Loss: 0.0023 - Validation Loss: 0.0024\n",
      "Epoch 254/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 255/500 - Training Loss: 0.0020 - Validation Loss: 0.0027\n",
      "Epoch 256/500 - Training Loss: 0.0024 - Validation Loss: 0.0024\n",
      "Epoch 257/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 258/500 - Training Loss: 0.0022 - Validation Loss: 0.0027\n",
      "Epoch 259/500 - Training Loss: 0.0022 - Validation Loss: 0.0026\n",
      "Epoch 260/500 - Training Loss: 0.0027 - Validation Loss: 0.0025\n",
      "Epoch 261/500 - Training Loss: 0.0024 - Validation Loss: 0.0025\n",
      "Epoch 262/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 263/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 264/500 - Training Loss: 0.0021 - Validation Loss: 0.0025\n",
      "Epoch 265/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 266/500 - Training Loss: 0.0020 - Validation Loss: 0.0028\n",
      "Epoch 267/500 - Training Loss: 0.0020 - Validation Loss: 0.0029\n",
      "Epoch 268/500 - Training Loss: 0.0022 - Validation Loss: 0.0029\n",
      "Epoch 269/500 - Training Loss: 0.0022 - Validation Loss: 0.0024\n",
      "Epoch 270/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 271/500 - Training Loss: 0.0022 - Validation Loss: 0.0029\n",
      "Epoch 272/500 - Training Loss: 0.0019 - Validation Loss: 0.0028\n",
      "Epoch 273/500 - Training Loss: 0.0020 - Validation Loss: 0.0024\n",
      "Epoch 274/500 - Training Loss: 0.0023 - Validation Loss: 0.0024\n",
      "Epoch 275/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 276/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 277/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 278/500 - Training Loss: 0.0022 - Validation Loss: 0.0024\n",
      "Epoch 279/500 - Training Loss: 0.0021 - Validation Loss: 0.0024\n",
      "Epoch 280/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 281/500 - Training Loss: 0.0019 - Validation Loss: 0.0029\n",
      "Epoch 282/500 - Training Loss: 0.0020 - Validation Loss: 0.0028\n",
      "Epoch 283/500 - Training Loss: 0.0021 - Validation Loss: 0.0027\n",
      "Epoch 284/500 - Training Loss: 0.0021 - Validation Loss: 0.0024\n",
      "Epoch 285/500 - Training Loss: 0.0020 - Validation Loss: 0.0027\n",
      "Epoch 286/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 287/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 288/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 289/500 - Training Loss: 0.0023 - Validation Loss: 0.0028\n",
      "Epoch 290/500 - Training Loss: 0.0020 - Validation Loss: 0.0028\n",
      "Epoch 291/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 292/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 293/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 294/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 295/500 - Training Loss: 0.0024 - Validation Loss: 0.0024\n",
      "Epoch 296/500 - Training Loss: 0.0021 - Validation Loss: 0.0026\n",
      "Epoch 297/500 - Training Loss: 0.0024 - Validation Loss: 0.0031\n",
      "Epoch 298/500 - Training Loss: 0.0024 - Validation Loss: 0.0027\n",
      "Epoch 299/500 - Training Loss: 0.0019 - Validation Loss: 0.0028\n",
      "Epoch 300/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 301/500 - Training Loss: 0.0023 - Validation Loss: 0.0027\n",
      "Epoch 302/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 303/500 - Training Loss: 0.0021 - Validation Loss: 0.0027\n",
      "Epoch 304/500 - Training Loss: 0.0018 - Validation Loss: 0.0022\n",
      "Epoch 305/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 306/500 - Training Loss: 0.0022 - Validation Loss: 0.0023\n",
      "Epoch 307/500 - Training Loss: 0.0021 - Validation Loss: 0.0024\n",
      "Epoch 308/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 309/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 310/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 311/500 - Training Loss: 0.0017 - Validation Loss: 0.0026\n",
      "Epoch 312/500 - Training Loss: 0.0021 - Validation Loss: 0.0023\n",
      "Epoch 313/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 314/500 - Training Loss: 0.0019 - Validation Loss: 0.0022\n",
      "Epoch 315/500 - Training Loss: 0.0019 - Validation Loss: 0.0022\n",
      "Epoch 316/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 317/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 318/500 - Training Loss: 0.0017 - Validation Loss: 0.0022\n",
      "Epoch 319/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 320/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 321/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 322/500 - Training Loss: 0.0020 - Validation Loss: 0.0027\n",
      "Epoch 323/500 - Training Loss: 0.0022 - Validation Loss: 0.0024\n",
      "Epoch 324/500 - Training Loss: 0.0025 - Validation Loss: 0.0025\n",
      "Epoch 325/500 - Training Loss: 0.0017 - Validation Loss: 0.0029\n",
      "Epoch 326/500 - Training Loss: 0.0019 - Validation Loss: 0.0028\n",
      "Epoch 327/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 328/500 - Training Loss: 0.0022 - Validation Loss: 0.0030\n",
      "Epoch 329/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 330/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 331/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 332/500 - Training Loss: 0.0024 - Validation Loss: 0.0024\n",
      "Epoch 333/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 334/500 - Training Loss: 0.0018 - Validation Loss: 0.0027\n",
      "Epoch 335/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 336/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 337/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 338/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 339/500 - Training Loss: 0.0023 - Validation Loss: 0.0026\n",
      "Epoch 340/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 341/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 342/500 - Training Loss: 0.0019 - Validation Loss: 0.0029\n",
      "Epoch 343/500 - Training Loss: 0.0018 - Validation Loss: 0.0025\n",
      "Epoch 344/500 - Training Loss: 0.0016 - Validation Loss: 0.0027\n",
      "Epoch 345/500 - Training Loss: 0.0016 - Validation Loss: 0.0025\n",
      "Epoch 346/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 347/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 348/500 - Training Loss: 0.0017 - Validation Loss: 0.0022\n",
      "Epoch 349/500 - Training Loss: 0.0018 - Validation Loss: 0.0025\n",
      "Epoch 350/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 351/500 - Training Loss: 0.0015 - Validation Loss: 0.0027\n",
      "Epoch 352/500 - Training Loss: 0.0017 - Validation Loss: 0.0023\n",
      "Epoch 353/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 354/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 355/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 356/500 - Training Loss: 0.0018 - Validation Loss: 0.0028\n",
      "Epoch 357/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 358/500 - Training Loss: 0.0021 - Validation Loss: 0.0027\n",
      "Epoch 359/500 - Training Loss: 0.0019 - Validation Loss: 0.0033\n",
      "Epoch 360/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 361/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 362/500 - Training Loss: 0.0016 - Validation Loss: 0.0023\n",
      "Epoch 363/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 364/500 - Training Loss: 0.0017 - Validation Loss: 0.0031\n",
      "Epoch 365/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 366/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 367/500 - Training Loss: 0.0019 - Validation Loss: 0.0029\n",
      "Epoch 368/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 369/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 370/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 371/500 - Training Loss: 0.0020 - Validation Loss: 0.0025\n",
      "Epoch 372/500 - Training Loss: 0.0020 - Validation Loss: 0.0024\n",
      "Epoch 373/500 - Training Loss: 0.0021 - Validation Loss: 0.0023\n",
      "Epoch 374/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 375/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 376/500 - Training Loss: 0.0019 - Validation Loss: 0.0022\n",
      "Epoch 377/500 - Training Loss: 0.0021 - Validation Loss: 0.0022\n",
      "Epoch 378/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 379/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 380/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 381/500 - Training Loss: 0.0020 - Validation Loss: 0.0027\n",
      "Epoch 382/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 383/500 - Training Loss: 0.0022 - Validation Loss: 0.0027\n",
      "Epoch 384/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 385/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 386/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 387/500 - Training Loss: 0.0021 - Validation Loss: 0.0030\n",
      "Epoch 388/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 389/500 - Training Loss: 0.0016 - Validation Loss: 0.0024\n",
      "Epoch 390/500 - Training Loss: 0.0016 - Validation Loss: 0.0028\n",
      "Epoch 391/500 - Training Loss: 0.0018 - Validation Loss: 0.0022\n",
      "Epoch 392/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 393/500 - Training Loss: 0.0017 - Validation Loss: 0.0028\n",
      "Epoch 394/500 - Training Loss: 0.0019 - Validation Loss: 0.0027\n",
      "Epoch 395/500 - Training Loss: 0.0015 - Validation Loss: 0.0028\n",
      "Epoch 396/500 - Training Loss: 0.0017 - Validation Loss: 0.0029\n",
      "Epoch 397/500 - Training Loss: 0.0021 - Validation Loss: 0.0026\n",
      "Epoch 398/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 399/500 - Training Loss: 0.0019 - Validation Loss: 0.0029\n",
      "Epoch 400/500 - Training Loss: 0.0017 - Validation Loss: 0.0023\n",
      "Epoch 401/500 - Training Loss: 0.0017 - Validation Loss: 0.0026\n",
      "Epoch 402/500 - Training Loss: 0.0015 - Validation Loss: 0.0029\n",
      "Epoch 403/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 404/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 405/500 - Training Loss: 0.0018 - Validation Loss: 0.0028\n",
      "Epoch 406/500 - Training Loss: 0.0018 - Validation Loss: 0.0027\n",
      "Epoch 407/500 - Training Loss: 0.0018 - Validation Loss: 0.0025\n",
      "Epoch 408/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 409/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 410/500 - Training Loss: 0.0021 - Validation Loss: 0.0026\n",
      "Epoch 411/500 - Training Loss: 0.0015 - Validation Loss: 0.0028\n",
      "Epoch 412/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 413/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 414/500 - Training Loss: 0.0020 - Validation Loss: 0.0023\n",
      "Epoch 415/500 - Training Loss: 0.0022 - Validation Loss: 0.0028\n",
      "Epoch 416/500 - Training Loss: 0.0020 - Validation Loss: 0.0026\n",
      "Epoch 417/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 418/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 419/500 - Training Loss: 0.0017 - Validation Loss: 0.0023\n",
      "Epoch 420/500 - Training Loss: 0.0016 - Validation Loss: 0.0028\n",
      "Epoch 421/500 - Training Loss: 0.0016 - Validation Loss: 0.0027\n",
      "Epoch 422/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 423/500 - Training Loss: 0.0015 - Validation Loss: 0.0024\n",
      "Epoch 424/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 425/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 426/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 427/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 428/500 - Training Loss: 0.0015 - Validation Loss: 0.0025\n",
      "Epoch 429/500 - Training Loss: 0.0013 - Validation Loss: 0.0029\n",
      "Epoch 430/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 431/500 - Training Loss: 0.0022 - Validation Loss: 0.0024\n",
      "Epoch 432/500 - Training Loss: 0.0018 - Validation Loss: 0.0028\n",
      "Epoch 433/500 - Training Loss: 0.0016 - Validation Loss: 0.0023\n",
      "Epoch 434/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 435/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 436/500 - Training Loss: 0.0019 - Validation Loss: 0.0030\n",
      "Epoch 437/500 - Training Loss: 0.0019 - Validation Loss: 0.0029\n",
      "Epoch 438/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 439/500 - Training Loss: 0.0014 - Validation Loss: 0.0030\n",
      "Epoch 440/500 - Training Loss: 0.0017 - Validation Loss: 0.0028\n",
      "Epoch 441/500 - Training Loss: 0.0017 - Validation Loss: 0.0030\n",
      "Epoch 442/500 - Training Loss: 0.0016 - Validation Loss: 0.0034\n",
      "Epoch 443/500 - Training Loss: 0.0019 - Validation Loss: 0.0031\n",
      "Epoch 444/500 - Training Loss: 0.0016 - Validation Loss: 0.0027\n",
      "Epoch 445/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 446/500 - Training Loss: 0.0018 - Validation Loss: 0.0028\n",
      "Epoch 447/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 448/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 449/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 450/500 - Training Loss: 0.0016 - Validation Loss: 0.0027\n",
      "Epoch 451/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 452/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 453/500 - Training Loss: 0.0018 - Validation Loss: 0.0027\n",
      "Epoch 454/500 - Training Loss: 0.0017 - Validation Loss: 0.0025\n",
      "Epoch 455/500 - Training Loss: 0.0016 - Validation Loss: 0.0025\n",
      "Epoch 456/500 - Training Loss: 0.0019 - Validation Loss: 0.0026\n",
      "Epoch 457/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 458/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 459/500 - Training Loss: 0.0019 - Validation Loss: 0.0031\n",
      "Epoch 460/500 - Training Loss: 0.0016 - Validation Loss: 0.0030\n",
      "Epoch 461/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 462/500 - Training Loss: 0.0020 - Validation Loss: 0.0031\n",
      "Epoch 463/500 - Training Loss: 0.0017 - Validation Loss: 0.0028\n",
      "Epoch 464/500 - Training Loss: 0.0020 - Validation Loss: 0.0028\n",
      "Epoch 465/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 466/500 - Training Loss: 0.0015 - Validation Loss: 0.0026\n",
      "Epoch 467/500 - Training Loss: 0.0017 - Validation Loss: 0.0031\n",
      "Epoch 468/500 - Training Loss: 0.0018 - Validation Loss: 0.0032\n",
      "Epoch 469/500 - Training Loss: 0.0022 - Validation Loss: 0.0027\n",
      "Epoch 470/500 - Training Loss: 0.0023 - Validation Loss: 0.0029\n",
      "Epoch 471/500 - Training Loss: 0.0017 - Validation Loss: 0.0022\n",
      "Epoch 472/500 - Training Loss: 0.0019 - Validation Loss: 0.0023\n",
      "Epoch 473/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 474/500 - Training Loss: 0.0018 - Validation Loss: 0.0024\n",
      "Epoch 475/500 - Training Loss: 0.0017 - Validation Loss: 0.0026\n",
      "Epoch 476/500 - Training Loss: 0.0018 - Validation Loss: 0.0029\n",
      "Epoch 477/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 478/500 - Training Loss: 0.0018 - Validation Loss: 0.0021\n",
      "Epoch 479/500 - Training Loss: 0.0022 - Validation Loss: 0.0025\n",
      "Epoch 480/500 - Training Loss: 0.0017 - Validation Loss: 0.0020\n",
      "Epoch 481/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 482/500 - Training Loss: 0.0021 - Validation Loss: 0.0023\n",
      "Epoch 483/500 - Training Loss: 0.0018 - Validation Loss: 0.0023\n",
      "Epoch 484/500 - Training Loss: 0.0019 - Validation Loss: 0.0024\n",
      "Epoch 485/500 - Training Loss: 0.0018 - Validation Loss: 0.0025\n",
      "Epoch 486/500 - Training Loss: 0.0018 - Validation Loss: 0.0023\n",
      "Epoch 487/500 - Training Loss: 0.0017 - Validation Loss: 0.0029\n",
      "Epoch 488/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 489/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 490/500 - Training Loss: 0.0016 - Validation Loss: 0.0026\n",
      "Epoch 491/500 - Training Loss: 0.0016 - Validation Loss: 0.0029\n",
      "Epoch 492/500 - Training Loss: 0.0018 - Validation Loss: 0.0026\n",
      "Epoch 493/500 - Training Loss: 0.0018 - Validation Loss: 0.0028\n",
      "Epoch 494/500 - Training Loss: 0.0019 - Validation Loss: 0.0033\n",
      "Epoch 495/500 - Training Loss: 0.0017 - Validation Loss: 0.0027\n",
      "Epoch 496/500 - Training Loss: 0.0018 - Validation Loss: 0.0031\n",
      "Epoch 497/500 - Training Loss: 0.0019 - Validation Loss: 0.0025\n",
      "Epoch 498/500 - Training Loss: 0.0017 - Validation Loss: 0.0024\n",
      "Epoch 499/500 - Training Loss: 0.0018 - Validation Loss: 0.0025\n",
      "Epoch 500/500 - Training Loss: 0.0019 - Validation Loss: 0.0022\n",
      "Training of the best model completed.\n",
      "\n",
      "===== Model Performance =====\n",
      "Training Set:\n",
      "  R2 Score: 0.9189\n",
      "  MSE: 0.0005\n",
      "  MAE: 0.0142\n",
      "\n",
      "Testing Set:\n",
      "  R2 Score: 0.8266\n",
      "  MSE: 0.0018\n",
      "  MAE: 0.0246\n",
      "Desired R2 score achieved! Stopping hyperparameter tuning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACobElEQVR4nOzdd3xT9f7H8fdJ0j1pCy0bZC+hsgRZigoooCKuixOceK24EK8/RUFcoCAoigNx4BYUBcUNKkuW4kChDAsto4PukfX7oyQQymiVtqfp6/l49HFNcnLOJ8k3vbz7XYbb7XYLAAAAAACcdJbqLgAAAAAAAH9F6AYAAAAAoJIQugEAAAAAqCSEbgAAAAAAKgmhGwAAAACASkLoBgAAAACgkhC6AQAAAACoJIRuAAAAAAAqCaEbAIBazu12V3cJAAD4LUI3APiBCRMmqE2bNsf9Oeuss/7VNRYsWKA2bdpo165dlfocs5o1a5batGlzzMcHDRqk888//5iPOxwO9erVS+PGjSvX9c466yxNmDBBkrRr1y61adNGCxYsKPdzymvdunW66aabvLfLe62ToSqv9W/s2rVLEydO1MCBA9WpUyf16dNHN998s3744YfqLu2E3/2ePXtWS11t2rTRrFmzquXaAGA2tuouAADw740dO1aXX3659/bs2bP1+++/69lnn/XeFxgY+K+uMWDAAL377ruqV69epT6nprr44ov11FNP6Y8//lC7du3KPL5s2TJlZmZq5MiRFT53vXr19O6776pJkyYno1Qf77//vrZu3Vol16qJVq5cqVtvvVXx8fEaM2aMWrZsqczMTH366acaM2aMrrnmGv3vf/+r1hrr1q3r810/nM3GP/UAoLrxmxgA/ECTJk18QlJMTIwCAwPVpUuXk3aNmJgYxcTEVPpzaqoLL7xQM2bM0KJFi44auj/66CM1bNhQvXv3rvC5T/ZnaZZrmd3evXuVlJSkxMREzZ49W0FBQd7HBg8erHnz5umxxx5Tq1atdMkll1RbnXxmAGBuDC8HgFpk9erVatOmjd555x2deeaZ6t27t3eI7Pvvv68RI0aoS5cuOvXUU3XBBRdoyZIl3uceOVR8woQJuvbaa/Xhhx9q0KBB6tixo4YPH65ly5b9q+dI0oYNGzRq1Ch16dJFAwYM0GuvvaZrr732hEOnv/rqK/3nP/9RYmKiOnbsqMGDB+vNN98s8/pXrlyp0aNHq3Pnzurdu7eeeOIJORwO73HFxcV67LHHdMYZZygxMVH33XefiouLj3vtevXqqV+/fvr000/lcrl8Hjtw4IC+/fZbjRgxQhaLRbt27dL48ePVp08fdejQQb169dL48eOVlZV11HMfbRj25s2bdd111ykxMVFnnnmmFi1aVOZ5mZmZevjhh3XmmWeqY8eO6tGjh2699Vafz2PhwoXavXu39/xHu9aOHTuUlJSkM844Q126dNFVV12ldevWlanvs88+84bU7t276/7771d+fv5x37fyKC4u1nPPPafBgwerU6dOOvfcc/Xiiy/6vM8pKSm65ZZb1LNnT3Xu3FmXXXaZT7sqLi7Www8/rH79+nnbxty5c4973Xnz5ik/P1+PPPKIT+D2uPbaa9WlSxc9//zzcrvdeuGFF9ShQwdlZmb6HPfWW2+pffv22r9/vyQpNTVVd955p3r06KHOnTvrmmuu0e+//+493vN+vvrqqxoyZIh69OhxUobgX3XVVZowYYLmzJmjM844Q6eddppuueUWpaSk+By3adMmjRkzRj179tRpp52mm2++WVu2bPE5JiMjQ//73//Uu3dvJSYmatSoUT5tQpLy8vJ0//33q0ePHkpMTFRSUpIyMjK8j5/oMwMAf0HoBoBaaPr06br33nt17733qkuXLpo/f74efPBBDRw4UHPmzNHUqVMVEBCge+65R6mpqcc8z6+//qpXXnlFSUlJeu6552Sz2ZSUlKTs7Ox//Jzk5GRde+21kqSnn35at912m1588cUy/6A/0nfffadbb71VHTp00OzZszVr1iw1bNhQkydP1vr1632Ovfvuu9W1a1e98MILGjZsmObOnasPPvjA+/g999yjd999VzfccINmzJih7OxszZs37wTvqjRy5Ejt27dPq1ev9rl/8eLFcjqduvjii1VYWKirr75aycnJmjhxol555RVdeeWV+vTTT/X000+f8BpSaQ/slVdeqezsbE2dOlW33367pk2bpr1793qPcbvduummm/Tjjz/qrrvu0iuvvKKxY8dqxYoVevDBByWVTkvo37+/6tatq3fffVcDBgwoc62tW7dqxIgRSklJ0f/93/9p2rRpMgxD11xzjdasWeNz7MSJE9WwYUPNnj1b119/vT788EO98MIL5XpNx+J2u3XzzTfr5Zdf1siRI/XCCy9o8ODBmjFjhiZOnChJcrlcuummm1RQUKAnn3xSs2fPVnR0tMaOHaudO3dKkqZMmaJly5bp3nvv1SuvvKKBAwfqiSeeOG6Y/eGHH9SuXTvVr1//mMcMGTJEu3fv1h9//KHhw4fL6XTqiy++8Dnm008/Va9evVS3bl1lZmbq8ssv12+//aYHHnhATz31lFwul0aNGqXk5GSf502fPl1jxozRI488otNPP/2475PD4Tjqz5GL5H399df68MMPdf/992vSpEnavHmzrr76ahUUFEiSVq1apSuuuEIul0tTpkzRI488orS0NF1++eXe+goKCnT55ZdrxYoVuuuuu/Tss88qLCxM119/vc9reP3112W32/XMM8/ojjvu0DfffKOHH3643J8ZAPgLhpcDQC10+eWXa/Dgwd7bKSkpGj16tG699VbvfY0aNdKIESO0fv16NWjQ4Kjnyc3N1YIFC7xD20NDQ3XllVdq1apVGjRo0D96zpw5cxQeHq6XX35ZISEhkqRTTjnFZ8760WzdulUXXnih7r//fu99iYmJ6tmzp3766Seddtpp3vsvueQS72vt1auXvvrqK3333Xe6/PLLtWXLFi1dulQPPvigRo0aJUnq27evhg0b5jP3+WgGDBiguLg4LVq0SL169fLe/9FHH+mMM85Q/fr19ccffyghIUGPP/649z04/fTTtWnTpjIh9ljmzZsnh8Ohl156SbGxsZKk5s2b69JLL/Ues2/fPoWEhOjee+9Vt27dJEk9e/bUrl279M4770gqnZZw5FQET/jyePbZZxUQEKDXX39dERER3tc5dOhQTZ06Ve+//7732P79++vee+/1vq8//vijvvvuO911113lel1Hs3z5cq1YsUJTp07V8OHDJUlnnHGGgoOD9cwzz+iaa65RVFSUkpOTdfPNN6t///6SpFNPPVXPPvusd4TCmjVr1Lt3b+9idz179lRoaKjq1KlzzGvv2rVL/fr1O259TZs2lSTt3r1b7du3V/fu3bV48WJve01NTdX69ev15JNPSpJee+01HThwQG+//bYaNmwoSerXr5/OO+88PfPMM5o5c6b33Oeee2651gDYvXu3OnTocNTHbr/9do0dO9Z7u6CgQB9++KG37Z1yyim66KKLtHDhQo0aNUpPPfWUGjdurJdffllWq1WS1KdPH51zzjmaNWuWZsyYoYULFyolJUUfffSR2rZtK0nq1q2bLrzwQv30009q0aKFJKlTp07e192rVy/98ssvWr58uaTSnvITfWYA4C8I3QBQCx25Crdn2HZubq527NihHTt2aOXKlZIku91+zPPExMT4zCVPSEiQJBUWFv7j56xatUr9+/f3Bm6pNDx7AsqxXH/99ZJKQ8Xff/+t7du3a9OmTUd9DYmJiT63ExISvGFz7dq1kqSBAwd6H7dYLBo0aNAJQ7fNZtPw4cP13nvv6aGHHlJQUJC2bdumX375xRum2rVrp7feeksul0spKSnasWOHtmzZom3btvkMcT+edevWqUuXLt7ALUmdO3f2+eNIfHy8Xn/9dUmlwW/nzp1KTk7W+vXrj/uZHmnNmjU688wzvYHb8zrPP/98Pffccz7Dx4+cV5yQkKDdu3eX+1rHur7VatV5553nc//w4cP1zDPPaPXq1frPf/6jli1b6oEHHtCKFSvUr18/9enTR/fdd5/3+J49e+qdd97R3r17deaZZ6p///4+f2Q6GrfbfcKFyDzB1NOjfMEFF+iBBx7Qvn37VK9ePS1evFghISE655xzJJUuzNauXTvFx8d7P2+LxaJ+/fqVmSLQunXrcrxDpQupPf/880d9LD4+3ud2YmKiz/evffv2aty4sdauXauLLrpImzZt0q233up9XZIUGRmpM8880zv0e+3atWrUqJE3cEtSUFCQPvvsM59rde3a1ed248aNlZOTI0mKi4s74WcGAP6C0A0AtdDhYU2S/v77bz344INatWqVbDabTjnlFG8wP94ezocHY0kyDEOSysxprshzMjMzy9QnlQaL48nMzNTEiRP11VdfyTAMNW3a1PuP/iNfQ3BwsM9ti8XiPcYzzP3IBeBOdH2PkSNHau7cufrmm280ZMgQffTRR4qJifHZsu3VV1/VnDlzlJWVpbi4OHXo0EEhISHKzc0t1zWys7PVqFGjMvcfWeOiRYv09NNPKy0tTdHR0Wrbtm2Z116ea8XFxZW5Py4uTm63W3l5ed77jvxsD39f/6ns7GzVqVOnTPj1vNbc3FwZhqG5c+fq+eef15dffqmFCxcqICBAZ599th566CFFR0fr/vvvV0JCghYtWuQd4pyYmKgHH3xQ7du3P+q1GzZseMI/GnjmQ3v+4DF48GBNnjxZn332ma655hp9+umnOvfcc73vzYEDB7Rz585j9kwf/gero73vRxMYGKhOnTqV69ij7SQQGxurnJwc5ebmyu12H/Pz9rTPAwcOHPU7eqTQ0FCf24e3h/J8ZgDgL5jTDQC1nMvl0o033qiMjAy999572rhxoxYtWuSzd3NVSkhI8FlsyeNo9x3u7rvv1i+//KJXX31VGzdu1GeffeYz1Ly8PMON09PTfe4/cOBAuZ7fokULJSYm6pNPPpHb7daiRYt04YUXKiAgQJL0ySef6PHHH9fo0aO1cuVK/fjjj3rxxRfVrFmzCtV4ZH1H1rh27Vrde++9Ouecc7Rs2TKtXr1ar732WoVXuY6KijrqtTyLgh1vePbJEBUVpaysrDKjAPbt2+dz/fj4eD300EP64Ycf9NFHH2nMmDH64osvNH36dEmlwfSWW27RZ599pm+//VYPPvigUlJSjjv0/ayzztKmTZuUlpZ2zGOWLl2q+vXre4N7eHi4Bg4cqM8++0zJycnavHmzLrjgAu/xERER6tGjhz744IOj/vzbrf1O5GjtOD09XTExMYqIiJBhGMf8vD1BOCIiosxicVLpAohHLrh2PCf6zADAXxC6AaCWy8rK0vbt2zVy5Eideuqp3h5Fz9zL4/VaV4bu3btr+fLlPvM6//jjD++K28eybt06DRo0SKeffro3uPyT1+BZsOrzzz/3uf/bb78t9zkuvvhi/fDDD/rxxx+VlpbmMy933bp1ioiI0I033ujtTc/Pz9e6devKXefpp5+uDRs2+CyctnXrVp9VqDds2CCXy6WkpCTvEH6n06kVK1ZIOvSeWCzH/6dA9+7d9e233/r0wjudTi1evFidOnWq9JDYo0cPOZ1On5X0JXmHYnft2lUbNmxQ79699csvv8gwDLVr10533HGHWrdurT179qioqEiDBg3yrlbeoEEDjRo1Sueff7727NlzzGtfddVVCg8PP+bq9W+99ZZWr16tm266yed9vOCCC/Tzzz9r/vz5qlevns8iaD169ND27dvVvHlzderUyfuzaNEivf/++z7DuivDhg0bfALzb7/9pl27dqlXr14KDQ1Vx44dtWTJEjmdTu8xubm5+u6777wjR7p166aUlBT9+eef3mNKSkp022236b333it3Hcf7zADAnzC8HABqudjYWDVs2FDz589XQkKCIiMj9cMPP+i1116TdPz52ZXh5ptv1pIlS3T99ddr9OjRysnJ0TPPPCPDMLxD0Y/m1FNP1SeffKIOHTooISFBGzZs0Jw5c2QYRoVeQ9OmTXXZZZdp+vTpcjgcateunT7++GOfgHEi5513nh599FFNnjxZiYmJ3oWlPHW+/fbbevzxx3XmmWdq3759euWVV5Senq6oqKhynf+aa67RBx98oDFjxui2226T0+nUjBkzvL3pnutI0qRJk3TxxRcrJydHb775pjZv3iypdO57eHi4IiMjlZ6ermXLlh11f/H//ve/Wr58ua6++mrdeOONCgwM1JtvvqmUlBS9/PLL5X5PjufHH3/0zvU93ODBg9WvXz/17NlTEydO1L59+9S+fXutWbNGL730ki666CK1bNlSxcXFCg4O1vjx43XbbbcpLi5OK1as0B9//KGrr75awcHB6tChg3dRuDZt2mj79u1auHDhMRf8k0qHYj/zzDNKSkrSiBEjdPXVV+uUU05Rdna2PvvsMy1evFijRo3SFVdc4fO8Pn36KCYmRu+8846uvfZan0B+7bXX6uOPP9a1116r0aNHq06dOlqyZInee++9fzyfuaSkRBs3bjzm461bt/YO9S4sLNQNN9ygW265Rfn5+Zo+fbpat26toUOHSpLuuusujRkzRtdff72uvPJK2e12vfjiiyopKdF///tfSdKIESP0xhtv6JZbbtHtt9+umJgYzZ8/X0VFRbrqqqvKVXP79u2P+5kBgD8hdAMANHv2bE2ZMkUTJkxQYGCgWrZsqeeff16PPvqo1q5dW+5/SJ8MTZs21SuvvKInn3xSSUlJio2N1U033aTnn39eYWFhx3ze448/rsmTJ2vy5MmSpGbNmunhhx/WokWLvIujldfEiRMVFxenN998U9nZ2erbt69uvvlmzZgxo1zPDwsL05AhQ/Thhx/qxhtv9Hnsoosu0q5du/Thhx/qrbfeUnx8vPr376///Oc/euCBB7R161a1bNnyuOevU6eO3n77be9n5tmu6fDe4J49e+rBBx/Uq6++qs8//1xxcXHq2bOnnn32Wd16661at26d+vfvrxEjRmjZsmW69dZblZSUVGbBslatWumtt97S008/rf/9738yDEOnnnqqXn/9de+q6P/Wp59+qk8//bTM/e3atVNCQoLmzJmjmTNn6vXXX1dmZqYaNWqkO+64Q9ddd52k0kW85s6dq6eeekpTpkxRTk6OmjVrpkmTJmnEiBGSSv/4MGPGDM2dO1f79+9XbGysRo4cqdtvv/24tZ1++un66KOPNG/ePM2dO1dpaWmKjIxUp06d9NJLL6lv375lnmO1WnX++efrtdde86647hEfH6933nlHTz31lB566CEVFxerWbNmmjJlSrlWKj+a/fv367LLLjvm4x988IF3zne3bt10+umne6denHXWWRo/frx3xEKvXr306quvaubMmbrzzjsVGBiobt266YknnlCrVq0klQ6hf/PNN/Xkk09qypQpcjgc6ty5s9544w2fRdqOpzyfGQD4C8P9b1c4AQDgJFq5cqUCAgJ8Al12drbOOOMMjR8/nl4w4B/y/PHsjTfeqOZKAKB2oacbAGAqv/32m7eXrUOHDsrKytLcuXMVERHhHQILAABQUxC6AQCmMnr0aJWUlOjtt99WWlqaQkND1aNHDz3xxBNltvECAAAwO4aXAwAAAABQSdgyDAAAAACASkLoBgAAAACgkhC6AQAAAACoJIRuAAAAAAAqCaEbAAAAAIBKwpZh5ZCRkSszrvFuGFJsbIRp60PtQ5uEGdEuYTa0SZgNbRJmVBPapafGEyF0l4PbLdN+0JL560PtQ5uEGdEuYTa0SZgNbRJm5A/tkuHlAAAAAABUEkI3AAAAAACVhNANAAAAAEAlYU43AAAAgFrD5XLJ6XRUdxk4AcOQioqKZLeXVNucbqvVJovl3/dTE7oBAAAA+D23262cnEwVFuZVdykop8xMi1wuV7XWEBISrsjIGBmG8Y/PQegGAAAA4Pc8gTs8vI4CA4P+VYhC1bBaDTmd1dPN7Xa7VVJSrLy8LElSVFTsPz4XoRsAAACAX3O5nN7AHR4eWd3loJxsNoscjurr6Q4MDJIk5eVlKSKizj8eas5CagAAAAD8mtPplHQoRAHl5Wkz/2YdAEI3AAAAgFqBIeWoqJPRZgjdAAAAAABUEkI3AAAAAACVhIXUAAAAAMBkpk59VF988Zmk0jnpdrtdwcHB3senTZupzp0TK3TOu+5KUufOXXT11aNPeOyVV16qq6++TueeO6RihZ/A+vVrlZR0s374Ye1JPa+ZEboBAAAAwGTuued/uuee/0mSliz5RHPnvqgPPvjkX53zqadmlvvYN998719dC4cQugEAAADUOm63W0VVvB1VsM1y0hZzS0tL1SWXDNdll43S4sWLdM45g5WUdKdefHG2Vqz4Xvv27VNQUJAGDjxH48bdI8Mw9N//3qjExK4aM+YmTZnykAIDA7V//35t2LBO0dF1dOmlV+iSSy6XJI0cOUyjR9+o884bpv/+90Z17HiqNm36WX/9tVn16sVr9OibNHDgOd5apk59TL/++ovi4uJ0wQUjNGvW9H/Um11cXKRXXpmjr776QoWFBWrRopVuvfV2tWvXQZK0cOEHevvtN5STk634+ASNHHm5hg27UJL0yitztHjxIhUWFqphw0a69tox6tOn/0l5v/8NQjcAAACAWsXtduv6d37WL6k5VXrdzg0i9dLlnU/qKuoFBQX65JMvVFRUpPfee0urVv2oZ555QXFxcfr111906603qG/fAerWrUeZ5y5Z8omefHK6Hn10qj799GNNn/6kBgw4S3Xr1itz7KJFCzVjxnNq3ryFXn31JU2dOkV9+vSTzWbTPfeMU/v2HfTxx58rO/uA7rvvrn/8eqZNe1x//fWnZs58QQ0bNtD777+n228fq9dff1dOp0OzZj2tefPeUpMmzbR69Urdd9/d6tWrj/7+e4cWLVqoV155U7Gxsfr44wV6/PHJ+uijM2SzVW/sZSE1AAAAALWOv2weNmTI+QoICFBERISGDbtIzzzzvGJjY5Wenq7i4mKFhoZp//59R31uYmI3de9+umw2m4YOvUBOp1O7d+866rFnnjlQrVu3VUBAgIYMGaq8vDxlZWXpt982KSVlp+64Y7xCQkKUkFBfN9449h+9luLiYn311VLdfPOtatSosQICAnTppVeoadOm+vLLz2W12uR2u/XRRx/ql182qmvX7vrqq+8VFxenwMBA5ebmaNGiBfrrrz81bNiF+uSTL6s9cEv0dAMAAACoZQzD0EuXd67Rw8s94uLqev+7qKhQ06c/qQ0b1qtevXpq3bqt3G633G73UZ8bGxvr/W9POHW5jv6exMSUPdbtdmnfvr2Kjo5WSEiI9/EGDRr9o9eSm5sru91e5vn16zfUnj2pSkhI0KxZczR//usaP/4OuVwunXfeMN1yy23q2PFUPfLIk/rgg3f01luvKzg4WCNHXq6rrx4ti6V6+5oJ3QAAAABqHcMwFBJgre4y/rXDQ/wTT0xRZGSkPv74cwUFBcnlcmnIkDMr9foJCfV14MABFRUVeVdX37Mn7R+dKyYmRoGBQdq9e5eaNm3mvX/37l0644y+ysrKlNPp0mOPTZPL5dKmTb/o//5vvBo3bqIzzuinmJgYPf30s7Lb7Vq7drXuv3+8Wrduq969+5yMl/qPMbwcAAAAAPxAfn6eAgMDZbVaVVCQr+eee0b5+fmy2+2Vds327TuqWbNT9Oyz01VUVKT9+/fp5ZdfOOHz9u3b6/OTnp4ui8Wi888frjlzntOuXSmy2+167723tX37Np199iDt3btHd9xxq9at+0kWi0VxcXGSpOjoaG3e/Jvuuus2bdnylwICAlSnTmnPfFRUdKW99vKip7uGyy6w66s/96vPKbEKsvE3FAAAAKC2GjfuHj355BQNGXKmQkPD1Lt3H/Xs2Vvbtm2ttGtaLBY98sgTmjbtMQ0derbq1YtXnz79tGXLn8d93ogR5/vcjomJ1aJFS3Xrrbdr7tw5uv32W5Sbm6NTTmmpp5+epSZNmkqS7rxzvKZOfUwZGfsVHh6hiy4aqbPOOkeGYSgl5W9NmHCnsrMPqE6dWCUl3akOHTpW2msvL8N9rAH+8EpPz5UZ3yXDkF5YlaKXf9iu/zu3lS7oVL+6S0ItZxhSXFyEab8zqJ1olzAb2iTMpja0Sbu9RBkZaYqNra+AgMDqLsevFBcX6ddfN6lLl9NktZYO1//hh+WaNu0xffTRZ//q3DabRY4qnnd/pOO1Hc9350ToGq3hsgpKh4ocKHRUcyUAAAAAahubLUAPPDBBn3yyUC6XS1lZmXrnnTerfR61mRC6azjLwXUTGLAAAAAAoKpZrVY99thTWrLkUw0ZcqauvvpyNW/eQrfddmd1l2YazOmu4TyLFRK5AQAAAFSHzp276MUX51V3GaZVLT3dGRkZGjt2rLp166aePXtqypQpcjiOPzx66dKlGjhwoM99iYmJPj+dO3dWmzZt9Omnn0qSfv75Z7Vt29bnmFGjRlXa66oOhkpTNx3dAAAAAGA+1dLTPW7cOMXHx+v7779Xenq6brnlFs2bN0/XX399mWPtdrvmzZunGTNmKD4+3uexDRs2+NweP368MjIyNHjwYEnSpk2b1L17d73xxhuV92Kq2aGeblI3AAAAAJhNlfd079y5U2vWrNE999yjkJAQNW7cWGPHjtX8+fOPevzo0aO1evVq3XDDDcc974IFC7RixQpNmzZNNlvp3xI2bdqkjh2rf4n4ymQY9HQDAAAAgFlVeU/3li1bFB0d7dNr3aJFC6WmpionJ0eRkZE+x0+dOlUJCQlasGDBMc+Zm5urJ554QhMnTlSdOnW892/atElxcXE699xzlZeXpx49emjChAlKSEioUM2e3mSzMQzfOd1mrRO1h6cN0hZhJrRLmA1tEmZTG9qkP782VI3Ds9fh95VHlYfu/Px8hYSE+NznuV1QUFAmdJcnIL/++utq2LChhgwZ4r3P6XSqXr166t27t6644grZ7XZNnjxZN954oxYuXOjdQ648YmNPvPdadfF8ziEhgeXaIw6oCmb+zqD2ol3CbGiTMBt/bpNFRUXKzLTIajVks7GBU01S3Z+Xy2XIYrGoTp0wBQcH/6NzVHnoDg0NVWFhoc99ntthYWEVPp/b7dYHH3ygpKQk71BrqXTp+nnz5vkc+8ADD6hXr15KTk5W69aty32NjIxcUw7fNgzJcvA1FxQUKz09t5orQm1nGKX/h23W7wxqJ9olzIY2CbOpDW3Sbi+Ry+WS0+mWw+Gq7nJqvPT0dIWFhZXpTD3ZbDZLtX9eTqf74P7j+QoIsPs85vnunEiV/9mgVatWOnDggNLT0733JScnKyEhQRERFf/r2qZNm3wWT/NIS0vTY489pvz8fO99JSUlklThv1C43eb98fydwWmCWvjhx+02/3eGn9r5Q7vkx2w/tEl+zPZTG9pkTXPHHbfqf/+756iPLVq0UMOGnevNN0eTlpaqPn26KS0tVZJ0zjl99fPPG4567Pr1a9WnT7dy1ZWZmaErrrhIBw5kSZJef32u7rorqVzPragLLzxfS5Z8Uinnrqh/066qPHQ3a9ZMXbt21aOPPqq8vDylpKRo9uzZGjly5D8637p169ShQ4cyf2WpU6eOFi9erOnTp6u4uFiZmZl6+OGH1atXLzVp0uRkvBRT8Pbt18TfJAAAAACOauTIy/Xjj8uVkZFe5rGPPvpAF154sQIDA8t9vi+//F6dOyf+67qKi4t9Ri5fffVoPfXUzH99Xn9WLVuGzZw5U5MmTdLAgQNlsVh04YUXauzYsZJK995++OGHNXz48HKdKyUlpcxWYlJpb/bLL7+sJ554Qn369JEkDRgwQI899tjJeyEm4F29vJrrAAAAAGoUt1tyFJ74uJPJFlLu1bd69TpDCQn1tWTJp7rqqmu99//66yZt25asJ598Rjt2bNfs2c9o69YtOnDggBo0aKBbbknSGWf0LXO+Pn26aebMF3Taad2Unp6uqVOnaMOG9YqKitbZZ5/rc+wPPyzXm2/O065dKSosLFC7dh10773/pwYNGuqqqy6VJF111aW6774HtWPHdm3YsE7PPvuiJGn58u80b97L2rUrRbGxsbroopEaOfJyWSwWTZnykAIDA7V//35t2LBO0dF1dOmlV+iSSy7/R2/n8a61bVuynnrqcSUnb1VYWJgSE7vqzjvHKzQ0TBs3rtesWdO1e3eKoqKi1bt3X9166+3eXbBOtmoJ3XFxcZo58+h/DTly722PESNGaMSIEWXuf/DBB495nbZt2+rVV1/9Z0XWEN7Vy0ndAAAAQPm43YpecJEC9qyt0sva63fXgYsWlCt4WywWXXTRSC1c+IGuvPIab2fbRx99oLPOOkdxcXEaN+4W9enTX48+Ok1ut1vPPz9TTz31+FFD9+EmTrxPUVHR+uijJcrNzdWECXd6H9u3b68efHCCJk16XH369FN29gH973/3aN68l/TAA5P1xhvv6ZJLhuuNN95T/foN9Morc7zPXb9+rR58cIIeeGCy+vc/U8nJW3XffXfJ7XbrsstGSZKWLPlETz45XY8+OlWffvqxpk9/UgMGnKW6detV6L080bWefvoJdevWQ88++6Kys7N1++03a9Gihbr88is1efKDuv76mzVkyFClpaXqllvGqHPnLhowYGCFaigvlu6r4YyDA8xdhG4AAACg/GrAPmJDh16ozMwMrV9f+seBnJxsffPNV96e4SefnKHRo2+Uy+VSWlqqIiIitX//vuOec8+eNP388wbdcsttCg0NU3x8gkaPvtH7eJ06MXrjjffUp08/FRTka9++vYqKitb+/ftPWO/ixYvUt+8ADRx4jmw2m9q0aasrr7xWH398aPvnxMRu6t79dNlsNg0deoGcTqd2795V4ffmRNcKDAzSqlUr9O23X8tiMfTqq2/p8suvlCQFBQXpm2++1I8/fq+oqCgtWLC40gK3VE093Th5Dv2uIHUDAAAA5WIYpT3OJh5eLknh4eEaNOg8LVq0UF27dtenny5S69Zt1K5dB0nSli1/acKEO5WZmaGmTZsrOjpa7hMMgfWE8vj4Q1szN2zY6FCJNpu+/PJzffzxAhmGoVNOaaH8/PxybbmclZWpVq3a+NxXv34D7dmT5r0dGxvrcy1JcrkqvkL5ia41adJjmjt3jl588Tk99NBuderUWXfdNUGnnNJCzzzzvObOfVFPPfW4MjLS1bNnb9199wTVq1d22vLJQE93DWdheDkAAABQcYYhBYRW7c8/6F2/+OLL9P333yk7+4AWLVqokSMvkySlp+/Xgw9O0I033qpPP/1Kzz33ks45Z/BxzyVJdeuWBsvU1N3e+/btO9Q7/s03X+rDD9/TrFlztGDBYk2bNlOtW7cpc56jSUioX6bXOjV1l2Jj48r1/Io43rVcLpf++muzRo++Se+8s1Dvv79IderE6NFHH1ZxcbF27Nimu+6aoAULFuuNN95Tfn6eZs58+qTX6EHoruFYSA0AAADwX82bn6JOnbpo1qzpKi4u8g6DLijIl9Pp9O7itH37Nr366suSJLvdfszzJSQkqEeP0zVr1nTl5OQoIyNdc+e+6H08Ly9PFotFQUFBcrvdWrVqhT7/fLEcDockeVdMz8vLK3Pu88+/QD/8sEzffPOVnE6n/vprs+bPf13nn1++RbKPJjc3R/v27fX5KSkpOe61LBaLZsyYqpdemq3i4mJFR9dRUFCgoqKiZRiGHnrofr3zzptyOByKjY2VzWZTdHT0P67xRBheXsN5/lbmoqsbAAAA8EsjR16q++67WzfccIt3SHaTJs00duztmjTp/1RUVKS6deM1fPhFmj37GSUnb1VUVNQxz/fQQ1P01FOPa+TIYQoLC9N55w3T77//KkkaMmSofvllo6666lJZrVY1adJMl176H3344Xuy2+2KiYlVv35n6uabr9Ntt93hc94OHTrqkUee0Ny5L+mxxyYpKipKF154sUaNuuYfv/ZZs6Zr1qzpPvdNmzZTp5/e+7jXmjz5CT399JO64ILBcrtd6tz5NI0f/z8FBgbq8cef1rPPztAbb7wqi8WqXr3O0M033/aPazwRw32iQf9QenquKYdvG4b00k+7NGfZNv2na0PdMaBFdZeEWs4wpLi4CNN+Z1A70S5hNrRJmE1taJN2e4kyMtIUG1tfAQHl39sa1ctms8jhqPh875PpeG3H8905EYaX13AWz/ByP/0FCQAAAAA1GaG7hmN4OQAAAACYF6G7hqsB2wsCAAAAQK1F6K7hDDG8HAAAAADMitBdw3n36a7eMgAAAAAAR0HorukOji9nTjcAAABwfGzchIo6GW2G0F3DMaUbAAAAOD6r1SpJKikpruZKUNN42ozVavvH5/jnz4QpsGUYAAAAcHwWi1UhIeHKy8uSJAUGBslgRWLTc7kMOZ3VE3TcbrdKSoqVl5elkJBwWSz/vL+a0F3DeX5XMLwcAAAAOLbIyBhJ8gZvmJ/FYpHL5arWGkJCwr1t558idNdwnr/PEbkBAACAYzMMQ1FRsYqIqCOn01Hd5eAEDEOqUydMWVn51Taq12q1/asebg9Cdw1nkLoBAACAcrNYLLJYAqu7DJyAYUjBwcEKCLDX+Km0LKRWw3nmorhJ3QAAAABgOoTuGu7QnO7qrQMAAAAAUBahu4Yz5OnpBgAAAACYDaG7hrN453QTuwEAAADAbAjdNZxneDmRGwAAAADMh9Bdw3mGlzOnGwAAAADMh9Bdw3l7uhleDgAAAACmQ+iu4QzvRt0AAAAAALMhdNdwnsjN8HIAAAAAMB9Cdw13aHh59dYBAAAAACiL0F3DWbzDy0ndAAAAAGA2hO4aji3DAAAAAMC8CN01HHO6AQAAAMC8CN013cGubrYMAwAAAADzIXTXcBZ2DAMAAAAA0yJ013DGwQHmDC8HAAAAAPMhdNdwh7YMI3UDAAAAgNkQums4C6uXAwAAAIBpEbprOENM6gYAAAAAsyJ013QHM7eL4eUAAAAAYDqE7hrO089N5gYAAAAA8yF013AWzz7d1VwHAAAAAKAsQncNx+rlAAAAAGBehO4a7lDort46AAAAAABlEbprOM/q5WRuAAAAADAfQncNZ7BPNwAAAACYFqG7hjM8C6kxvhwAAAAATIfQXcOxZRgAAAAAmBehu4ZjyzAAAAAAMC9Cdw1n0NUNAAAAAKZF6K7hPJnbReYGAAAAANMhdNdwrF4OAAAAAOZF6K7hDq1eXs2FAAAAAADKIHTXcN4p3fR1AwAAAIDpELprOHq6AQAAAMC8CN01nIU53QAAAABgWoTuGs67kBpd3QAAAABgOoTuGs4Qw8sBAAAAwKwI3TUdw8sBAAAAwLSqJXRnZGRo7Nix6tatm3r27KkpU6bI4XAc9zlLly7VwIEDfe5zuVxKTExUly5dlJiY6P0pKCiQJBUUFOi+++5Tz5491bVrV40fP175+fmV9rqqg8W7kBqxGwAAAADMplpC97hx4xQaGqrvv/9eH3zwgVauXKl58+Yd9Vi73a6XXnpJd955Z5lguXXrVtntdq1Zs0YbNmzw/oSGhkqSJk+erLS0NC1dulRffPGF0tLSNG3atMp+eVXq0JZhAAAAAACzqfLQvXPnTq1Zs0b33HOPQkJC1LhxY40dO1bz588/6vGjR4/W6tWrdcMNN5R5bNOmTWrTpo0CAwPLPFZYWKhPPvlESUlJio6OVmxsrO6++24tWLBAhYWFJ/11VZdDC6lVbx0AAAAAgLJsVX3BLVu2KDo6WvHx8d77WrRoodTUVOXk5CgyMtLn+KlTpyohIUELFiwoc65NmzapuLhYF198sXbv3q0WLVrorrvu0mmnnaadO3fKbrerdevWPtcpKirSjh071K5du3LX7Am2ZmMYhw0vl9u0daL28LRB2iLMhHYJs6FNwmxokzCjmtAuy1tblYfu/Px8hYSE+NznuV1QUFAmdCckJBzzXMHBwTr11FN1++23KyoqSvPnz9eYMWO0aNEi5eXlSZJ3qPnh16novO7Y2IgKHV+VjKxiSZLValFcnHnrRO1i5u8Mai/aJcyGNgmzoU3CjPyhXVZ56A4NDS0zvNtzOywsrELnmjBhgs/tMWPGaMGCBVq2bJlOO+0077k95/VcJzw8vELXycjINeXwbcOQd1K33e5SenputdYDGEbpL0azfmdQO9EuYTa0SZgNbRJmVBPapafGE6ny0N2qVSsdOHBA6enpiouLkyQlJycrISFBEREV+yvG9OnTNWjQILVv3957X0lJiYKCgtS8eXMFBARo69at6ty5s/c6AQEBatasWYWu43abd860d59uuU1bI2ofM39nUHvRLmE2tEmYDW0SZuQP7bLKF1Jr1qyZunbtqkcffVR5eXlKSUnR7NmzNXLkyAqf66+//tKUKVO0f/9+lZSU6Nlnn1VeXp7OOecchYSEaMiQIZo2bZoyMzOVmZmpadOmaejQoQoODq6EV1Y9LCykBgAAAACmVS1bhs2cOVMOh0MDBw7UpZdeqr59+2rs2LGSpMTERC1atKhc53nsscfUpEkTXXDBBerZs6fWrFmjV199VdHR0ZKkiRMnqlmzZho2bJgGDx6sRo0a6cEHH6ysl1UtDO9CagAAAAAAszHcR25+jTLS0805j8AwpO15dl3ywko1jg7WgjE9qrsk1HKGIcXFRZj2O4PaiXYJs6FNwmxokzCjmtAuPTWeSLX0dOPk8Q4vr94yAAAAAABHQeiu8Q4OLyd1AwAAAIDpELprOMO7kBqpGwAAAADMhtBdwx3M3AwvBwAAAAATInTXcBaD4eUAAAAAYFaE7hrOYCE1AAAAADAtQncNZ3gXUiN2AwAAAIDZELprOHq6AQAAAMC8CN013KHVy6u3DgAAAABAWYTuGs4zvNxF6gYAAAAA0yF013Cenm4AAAAAgPkQums4tgwDAAAAAPMidNdwLKQGAAAAAOZF6K7hPKPL2TIMAAAAAMyH0F3D0dMNAAAAAOZF6K7hDOZ0AwAAAIBpEbprOM/wcrYMAwAAAADzIXTXcAZ7hgEAAACAaRG6aziLZ043Hd0AAAAAYDqE7hrOODjA3M1SagAAAABgOoTuGs4zutxF5gYAAAAA0yF0AwAAAABQSQjdNZzF4tkyjK5uAAAAADAbQncN51m7nMgNAAAAAOZD6K7hmNMNAAAAAOZF6K7hLAZ7hgEAAACAWRG6aziGlwMAAACAeRG6azqGlwMAAACAaRG6azjD29cNAAAAADAbQncNZzksc7NtGAAAAACYC6G7hjOMQ6mbyA0AAAAA5kLoruEOH1zOvG4AAAAAMBdCdw1nMXzGl1dfIQAAAACAMgjdNd3hmbv6qgAAAAAAHAWhu4Y7vKOb4eUAAAAAYC6E7hru8DndrF4OAAAAAOZC6K7hfOZ0AwAAAABMhdBdwxnM6QYAAAAA0yJ013DGYQPMXQwvBwAAAABTIXTXcOwYBgAAAADmReiu4ZjSDQAAAADmReiu4Q4fXk5PNwAAAACYC6G7hvPdp5vUDQAAAABmQuiu4Q7fMozIDQAAAADmQuiu4XymdJO6AQAAAMBUCN01nM/wclI3AAAAAJgKobuGMwwWUgMAAAAAsyJ0+wFP7CZzAwAAAIC5ELr9gLezm65uAAAAADAVQrcf8GRuF5kbAAAAAEyF0O0HPPO6ydwAAAAAYC6Ebj/gGV7uZng5AAAAAJgKodsPsJAaAAAAAJgTodsPeIeXk7oBAAAAwFQI3X7gUE83qRsAAAAAzKRaQndGRobGjh2rbt26qWfPnpoyZYocDsdxn7N06VINHDjQ577i4mJNmTJF/fr1U9euXXXJJZdo1apV3sd//vlntW3bVomJid6fUaNGVcprqk6H5nRXbx0AAAAAAF/VErrHjRun0NBQff/99/rggw+0cuVKzZs376jH2u12vfTSS7rzzjvLLBQ2bdo0rV+/Xu+++67WrFmjSy65RDfffLNSU1MlSZs2bVL37t21YcMG78/8+fMr++VVOUMMLwcAAAAAM6ry0L1z506tWbNG99xzj0JCQtS4cWONHTv2mGF49OjRWr16tW644YYyjxUXFyspKUn169eX1WrVpZdeqsDAQP3222+SSkN3x44dK/X1mIG3p7t6ywAAAAAAHMFW1RfcsmWLoqOjFR8f772vRYsWSk1NVU5OjiIjI32Onzp1qhISErRgwYIy55o0aZLP7ZUrVyo3N1dt27aVVBq64+LidO655yovL089evTQhAkTlJCQUAmvrPqwZRgAAAAAmFOVh+78/HyFhIT43Oe5XVBQUCZ0lzcgb9y4UePGjdN///tfNW7cWE6nU/Xq1VPv3r11xRVXyG63a/Lkybrxxhu1cOFCWa3WctfsCbVm46nLkFHmPqA6eNsk7RAmQruE2dAmYTa0SZhRTWiX5a2tykN3aGioCgsLfe7z3A4LC/tH53z//ff16KOPKikpSdddd50kyWq1lpkn/sADD6hXr15KTk5W69aty33+2NiIf1RXVbFYSj/t6Dqhioszd62oHcz+nUHtRLuE2dAmYTa0SZiRP7TLKg/drVq10oEDB5Senq64uDhJUnJyshISEhQRUbE31Ol06uGHH9YXX3yh5557Tr179/Y+lpaWpnnz5ikpKckb5ktKSiRJwcHBFbpORkauKRcpMwxPIywtLjMzX+lsAodq5GmTZv3OoHaiXcJsaJMwG9okzKgmtMtDeez4qjx0N2vWTF27dtWjjz6qSZMmKSsrS7Nnz9bIkSMrfK7HHntMy5cv14cffqiGDRv6PFanTh0tXrxYTqdT99xzj/Lz8/Xwww+rV69eatKkSYWu43abe2Vwz/Byl8nrRO1h9u8MaifaJcyGNgmzoU3CjPyhXVZLv+jMmTPlcDg0cOBAXXrpperbt6/Gjh0rSUpMTNSiRYtOeI7MzEzNnz9f6enpGjp0qM9e3IsWLVJwcLBefvllJScnq0+fPho0aJDCw8M1Y8aMSn51Vc8zlaCmN0YAAAAA8DeGmyWvTyg93ZxDGgxDiouLUNdJXyijwK63rj5NreqGV3dZqMU8bdKs3xnUTrRLmA1tEmZDm4QZ1YR26anxRJgB7A8OLptn1sYIAAAAALUVodsPeIeXV2sVAAAAAIAjEbr9gGd/OGYKAAAAAIC5ELr9gMUTuqu3DAAAAADAEQjdfoE53QAAAABgRoRuP3BoyzBSNwAAAACYCaHbDzC8HAAAAADMidDtBwy2DAMAAAAAUyJ0+xEyNwAAAACYC6HbD7BlGAAAAACYE6HbD1gYXg4AAAAApkTo9iNkbgAAAAAwF0K3H/AML3fR1Q0AAAAApkLo9gMW707dAAAAAAAzIXT7gUMLqVVvHQAAAAAAX4RuP+JmVjcAAAAAmAqh2w8cmtNdvXUAAAAAAHwRuv2AxTu+vHrrAAAAAAD4InT7Ac8yagwvBwAAAABzIXT7Azq6AQAAAMCUCN1+wDO8nDndAAAAAGAuhG4/4N2lm9ANAAAAAKZC6PYDzOkGAAAAAHMidPsDhpcDAAAAgCkRuv2AxbOQGqEbAAAAAEyF0O0HvHO6GV4OAAAAAKZC6PYDxsHh5fR0AwAAAIC5ELr9iKu6CwAAAAAA+CB0+wGLd/lyuroBAAAAwEwI3X7A8CykVr1lAAAAAACOQOj2A4bYMgwAAAAAzIjQ7Qe8Pd0MLwcAAAAAUyF0+wHjxIcAAAAAAKoBodsPsGUYAAAAAJgTodsPeHq6XSylBgAAAACmQuj2A4fmdFdvHQAAAAAAX4RuP+AZXg4AAAAAMBdCtx/wDi+nqxsAAAAATIXQ7QcYXg4AAAAA5kTo9gPGwb5uMjcAAAAAmAuh2w94p3STugEAAADAVAjdfoA53QAAAABgToRuP+BZvZzIDQAAAADmQuj2A96F1Kq3DAAAAADAEQjdfsC7SzfDywEAAADAVAjdfsAzvNxF5gYAAAAAUyF0+wEWLwcAAAAAcyJ0+wHvnG5SNwAAAACYCqHbD3h7ukndAAAAAGAqhG4/wJZhAAAAAGBOhG4/wJZhAAAAAGBOhG4/wPByAAAAADAnQrcfsHiGl5O5AQAAAMBUbBU52OFw6JtvvtHKlSuVlpYmq9Wq+vXrq2/fvjrjjDNks1XodDhZGF4OAAAAAKZU7pT84YcfatasWQoICFCXLl3UqlUrORwO7du3Tw8//LAkKSkpSRdeeGFl1YpjYHg5AAAAAJhTuUL3rbfeqrCwMM2cOVOnnnrqUY/ZsGGDXnvtNS1dulTPP//8SS0Sx3codFdrGQAAAACAI5RrTveYMWP05JNPHjNwS1JiYqJmzJihMWPGnPB8GRkZGjt2rLp166aePXtqypQpcjgcx33O0qVLNXDgwDL3v/TSS+rXr5+6dOmiq666Stu2bfM+VlBQoPvuu089e/ZU165dNX78eOXn55+wvprGwpZhAAAAAGBK5Qrdp512WrlP2K1btxMeM27cOIWGhur777/XBx98oJUrV2revHlHPdZut+ull17SnXfeWWb49MKFC/XGG2/olVde0erVq9WhQwclJSV5j5s8ebLS0tK0dOlSffHFF0pLS9O0adPK/VpqDM+cbrq6AQAAAMBUyhW6H3roIZ/b69ev97ldkXncO3fu1Jo1a3TPPfcoJCREjRs31tixYzV//vyjHj969GitXr1aN9xwQ5nH3nvvPf3nP/9Rq1atFBQUpLvuukupqalavXq1CgsL9cknnygpKUnR0dGKjY3V3XffrQULFqiwsLDc9dYE3uHl1VoFAAAAAOBI5ZrTvWjRIp/gffPNN2vNmjXe2zt37iz3Bbds2aLo6GjFx8d772vRooVSU1OVk5OjyMhIn+OnTp2qhIQELViwoMy5tm7d6hPGAwIC1KxZM23evFnR0dGy2+1q3bq1z3WKioq0Y8cOtWvXrtw1G8aJj6kOnrqMw4aXm7VW1A6H2mT11gEcjnYJs6FNwmxokzCjmtAuy1tbuUL3kcOWj7xtVOCdyM/PV0hIiM99ntsFBQVlQndCQkKFzhUcHKyCggLl5eVJkkJDQ8tcp6LzumNjIyp0fFULDQk4+L+Bioszd62oHcz+nUHtRLuE2dAmYTa0SZiRP7TLcoXuI0N1RUL2kUJDQ8sM7/bcDgsLq9C5QkJCVFRU5HNfUVGRwsLCvGG7sLDQe17PdcLDwyt0nYyMXFOuDG4YpY2wuMguScovKFZ6em41V4XazNMmzfqdQe1Eu4TZ0CZhNrRJmFFNaJeeGk+k3Pt0nyytWrXSgQMHlJ6erri4OElScnKyEhISFBFRsb9itGrVSlu2bNGZZ54pqXTRtR07dqh169Zq3ry5AgICtHXrVnXu3Nl7Hc8Q9Ipwu02+HdfBP4K4zF4nag3Tf2dQK9EuYTa0SZgNbRJm5A/tslwLqZ1MzZo1U9euXfXoo48qLy9PKSkpmj17tkaOHFnhc1188cV68803tXnzZhUXF+upp55SXFycunXrppCQEA0ZMkTTpk1TZmamMjMzNW3aNA0dOlTBwcGV8Mqqj+dDZPVyAAAAADCXcvV0FxUV6eqrr/bezs/P97ldXFxcoYvOnDlTkyZN0sCBA2WxWHThhRdq7Nixkkr3+3744Yc1fPjwE55n5MiRys3N1a233qrMzEx16tRJc+bMUUBA6RzniRMn6oknntCwYcNkt9s1cOBAPfDAAxWqtSYwvFuGVW8dAAAAAABfhrsc3aPPPvvsCU/03//+96QUZEbp6eacR2AYUlxchO55Z4Pe35iqMac30c1nNKvuslCLedqkWb8zqJ1olzAb2iTMhjYJM6oJ7dJT44mUq6fbnwO1PzjU023S1ggAAAAAtVS553Tn5eVp+/bt3tsffvihHnnkEa1evbpSCkP5WTyhu3rLAAAAAAAcoVyhOzk5Weecc45efvllSdK8efP08MMPa9++fUpKStIPP/xQqUXi+AyVpm46ugEAAADAXMo1vPyZZ57RsGHDdO+990qS5s6dq3Hjxmn06NFatmyZ5syZoz59+lRqoTgOeroBAAAAwJTK1dO9du1aJSUlyWq1aseOHdq/f7/OOeccSVLPnj31xx9/VGqROD62DAMAAAAAcypX6C4qKlJ4eLgk6eeff1ZMTIwaN25cegKLRU6ns/IqxAkZBsPLAQAAAMCMyhW6Y2NjlZaWJklatWqVunfv7n1s8+bNqlevXuVUh3I5OLqc4eUAAAAAYDLlmtM9ePBgjR8/Xn379tXixYs1c+ZMSdLWrVv1+OOP6+yzz67UInECB1O3i65uAAAAADCVcvV033bbbWrWrJk+/vhj3XzzzRowYIAkacSIEZKkW265pdIKxIlZPBt1AwAAAABMpVw93YGBgZo8eXKZ+xcuXKgWLVqc9KJQMd7h5XR0AwAAAICplCt0p6amHvX+kJAQ72MNGjQ4eVWhQgy2DAMAAAAAUypX6D7rrLMOWyH7ULQzDENut1uGYbBtWDXyhm66ugEAAADAVMoVulu3bq3U1FSdf/75uuiii1S3bt3KrgsVYIgtwwAAAADAjMoVuhctWqRff/1VH374oW655RZ17txZl1xyiQYMGCCr1VrZNeIE2DIMAAAAAMypXKuXS1LHjh01ceJELVu2TOeff77eeOMNDRgwQFOnTtW2bdsqs0acgMGWYQAAAABgSuUO3R6BgYEaNmyY5s2bp5dfflnLly/X+eefXxm1oZwMtgwDAAAAAFMq1/DyI61YsUILFy7UV199pebNm+v//u//TnZdqAC2DAMAAAAAcyp36N6xY4cWLlyojz/+WCUlJRo+fLjeffddtW7dujLrQzkc2jKM1A0AAAAAZlKu0H355Zfrt99+U//+/fXAAw+wgJrJeIaXu8jcAAAAAGAq5QrdGzduVEREhH7//Xf98ccfevTRR8sc8/XXX5/04lA+3hndhG4AAAAAMJVyhe7HHnussuvAv8DwcgAAAAAwp3KF7uHDh5d7OLnT6WToeRVjITUAAAAAMKdybRk2atQorVy58oTHLV++XKNGjfrXRaFivHO6q7kOAAAAAICvcvV0T506Vffdd58eeeQRDR06VImJiYqPj5fL5dK+ffu0bt06ff7554qKitKTTz5Z2TXjCIfmdNPVDQAAAABmUq7Q3bhxY7355pv67rvv9Pbbb+vFF19UYWGhJCkkJER9+vTR3XffrQEDBlRmrTiGQ3O6AQAAAABmUu59uiVpwIABGjBggNxut7KysmSxWBQdHV1JpaG82DIMAAAAAMypQqHbwzAMxcTEnOxa8A8dWkiN1A0AAAAAZlKuhdRgboZx4mMAAAAAAFWP0O0HjIN93XR0AwAAAIC5ELr9gKenmzndAAAAAGAuFQ7dv/76qyQpJydHU6dO1SuvvCKHw3HSC0P5HRpdTuoGAAAAADOp0EJqzz//vF5++WWtW7dOjzzyiH799VdZLBbt2bNH999/f2XViBPwrF5O5AYAAAAAc6lQT/enn36q+fPnq6SkREuXLtXTTz+t1157TUuWLKms+lAOnp5uhpcDAAAAgLlUqKd73759atu2rVauXKmIiAi1bdtWklRYWFgpxaF8PHO62TIMAAAAAMylQj3d8fHx+umnn/TRRx+pV69ekkp7vxs3blwpxaF82DIMAAAAAMypQj3dt912m66//noFBwfr7bff1sqVK3Xfffdp1qxZlVUfyoEtwwAAAADAnCoUugcNGqQBAwZIkoKCghQfH6+vv/5a9erVq4zaUE4W75ZhpG4AAAAAMJMKDS93uVxavny5goKCtHfvXt1///164YUXlJeXV1n1oTw8c7qrtwoAAAAAwBEqFLoff/xxPfLII5KkiRMnKj09Xdu2bdOkSZMqpTiUjyEmdQMAAACAGVVoePmyZcv09ttvKz8/Xz/88IMWL16s2NhYDRw4sLLqQzl4IjejywEAAADAXCrU052VlaUGDRrop59+Ur169dS0aVOFhITI6XRWVn0oB+Z0AwAAAIA5Vainu3Hjxvroo4/0+eefq0+fPnK5XJo7d65atmxZWfWhPA7uGUbkBgAAAABzqVDonjBhgu69914FBwdr0qRJWrVqlV555RW98MILlVUfysE7o5vUDQAAAACmUqHQ3b17d33zzTfe29HR0Vq+fLkCAwNPemEoP4aXAwAAAIA5VSh0S9JXX32ld999V7t371bdunU1cuRIDRs2rDJqQzkZDC8HAAAAAFOq0EJqn3zyiSZMmKDWrVvrqquuUvv27fXQQw/p/fffr6z6UAGEbgAAAAAwlwr1dL/00kt69tlndfrpp3vv69+/vyZNmqRLLrnkpBeH8jHYMwwAAAAATKlCPd2pqanq2bOnz309evTQnj17TmpRqBjLwaXUXGRuAAAAADCVCoXuhIQE/fTTTz73/fTTT2rQoMFJLQoV4+npJnMDAAAAgLlUaHj5Nddco1tvvVWXXXaZGjdurL///lvvvvuu7rvvvsqqDxXB8HIAAAAAMJUKhe5LLrlEVqtVCxYs0FdffaWGDRvqkUce0eDBgyurPpSDxWB4OQAAAACYUYW3DBsxYoRGjBjhve10OrV9+3Y1b978pBaG8mN4OQAAAACYU4XmdB9Nenq6zjvvvJNRC/4hFi8HAAAAAHP616FbktykverlSd30dQMAAACAqZyU0G14N4pGdWBONwAAAACY00kJ3ahe3uHl1VoFAAAAAOBI5VpI7ci9uQ+XmZlZ4YtmZGTogQce0Jo1a2S1WjV8+HDde++9stnKlrNs2TJNmzZNKSkpql+/vsaPH68zzzxTkpSYmOhzrMvlUlFRkZ566ikNHTpUP//8sy677DKFhIR4j2nfvr3mz59f4ZrN7NCcbmI3AAAAAJhJuUL3VVddddzHKzq8fNy4cYqPj9f333+v9PR03XLLLZo3b56uv/56n+N27Nih2267TU8//bQGDBigL774QuPGjdMXX3yh+Ph4bdiwwef48ePHKyMjw7uF2aZNm9S9e3e98cYbFaqvpvG8/2RuAAAAADCXcoXuzZs3n7QL7ty5U2vWrNHy5csVEhKixo0ba+zYsZo6dWqZ0L1w4UJ169ZNZ599tiTpvPPO04IFC/Tuu+8qKSnJ59gFCxZoxYoV+uSTT7w95ps2bVLHjh1PWu1mxZZhAAAAAGBOFd6n+9/asmWLoqOjFR8f772vRYsWSk1NVU5OjiIjI733b926Va1bt/Z5fsuWLcv8ESA3N1dPPPGEJk6cqDp16njv37Rpk+Li4nTuuecqLy9PPXr00IQJE5SQkFChms26TpynrkOh223aWlE7HNkmATOgXcJsaJMwG9okzKgmtMvy1lbloTs/P99njrUk7+2CggKf0H20Y4ODg1VQUOBz3+uvv66GDRtqyJAh3vucTqfq1aun3r1764orrpDdbtfkyZN14403auHChbJareWuOTY2otzHVoeoyFBJktVqVVycuWtF7WD27wxqJ9olzIY2CbOhTcKM/KFdVnnoDg0NVWFhoc99ntthYWE+94eEhKioqMjnvqKiIp/j3G63PvjgAyUlJfnMLbdarZo3b57Pcx944AH16tVLycnJZXrQjycjI9eU86UNo7QR5uWWvn92u1Pp6bnVXBVqM0+bNOt3BrUT7RJmQ5uE2dAmYUY1oV16ajyRKg/drVq10oEDB5Senq64uDhJUnJyshISEhQR4Vtw69at9dtvv/nct3XrVp952ps2bfJZPM0jLS1N8+bNU1JSkjekl5SUSCrtLa8It7tmLFLmlrtG1An/V1O+M6hdaJcwG9okzIY2CTPyh3ZZ5ft0N2vWTF27dtWjjz6qvLw8paSkaPbs2Ro5cmSZY4cPH641a9ZoyZIlcjgcWrJkidasWaMLLrjAe8y6devUoUOHMsPQ69Spo8WLF2v69OkqLi5WZmamHn74YfXq1UtNmjSp9NdZlbxzumt4YwQAAAAAf1PloVuSZs6cKYfDoYEDB+rSSy9V3759NXbsWEmle28vWrRIUukCa88995zmzJmj7t27a/bs2Zo1a5aaN2/uPVdKSorPomwewcHBevnll5WcnKw+ffpo0KBBCg8P14wZM6rkNVYl4+BO3WRuAAAAADAXw+2mf/RE0tPNOY/AMKS4uAgt3ZCim979Rc1iQvT+dd2ruyzUYp42adbvDGon2iXMhjYJs6FNwoxqQrv01Hgi1dLTjZPLs3ycWRsjAAAAANRWhG4/4Fm1ncwNAAAAAOZC6PYDhxZSI3YDAAAAgJkQuv2Ad3h5tVYBAAAAADgSodsPeIeXk7oBAAAAwFQI3X6Anm4AAAAAMCdCtx9gTjcAAAAAmBOh2w8YYng5AAAAAJgRodsPeHu6q7cMAAAAAMARCN1+wMLwcgAAAAAwJUK3Xzg4vLyaqwAAAAAA+CJ0+4FDC6lVbx0AAAAAAF+Ebj/AlmEAAAAAYE6Ebj9gMTyrlxO7AQAAAMBMCN1+hMwNAAAAAOZC6PYDbBkGAAAAAOZE6PYDDC8HAAAAAHMidPsBFlIDAAAAAHMidPsDtgwDAAAAAFMidPuBQz3dpG4AAAAAMBNCtx84NKe7mgsBAAAAAPggdPsBg+HlAAAAAGBKhG4/wvByAAAAADAXQrcfYHg5AAAAAJgTodsPsGUYAAAAAJgTodsPHJrTTewGAAAAADMhdPsRIjcAAAAAmAuh2w945nS7SN0AAAAAYCqEbj9geCd1k7oBAAAAwEwI3X6AhdQAAAAAwJwI3X7AYMswAAAAADAlQrcfoKcbAAAAAMyJ0O0HvHO6xbZhAAAAAGAmhG4/YOhQ6iZyAwAAAIB5ELr9wOE93WwbBgAAAADmQej2A4eHblZTAwAAAADzIHT7AYaXAwAAAIA5Ebr9gO9CatVXBwAAAADAF6HbD/jO6SZ1AwAAAIBZELr9wOHDywEAAAAA5kHo9gM+66hVWxUAAAAAgCMRuv0Ac7oBAAAAwJwI3X7AOCx1M6cbAAAAAMyD0O0HmNENAAAAAOZE6PYDDC8HAAAAAHMidPsBhpcDAAAAgDkRuv0Aq5cDAAAAgDkRuv2AQeoGAAAAAFMidPsB38xN6gYAAAAAsyB0+wHfOd3VWAgAAAAAwAeh2094YjeZGwAAAADMg9DtJ7yd3axeDgAAAACmQej2E54h5gwvBwAAAADzIHT7CYaXAwAAAID5ELr9hNVSGruddHUDAAAAgGlUS+jOyMjQ2LFj1a1bN/Xs2VNTpkyRw+E46rHLli3TsGHD1KVLFw0ZMkTffvut9zGXy6XExER16dJFiYmJ3p+CggJJUkFBge677z717NlTXbt21fjx45Wfn18lr7GqhQVaJUn5JUd/HwEAAAAAVa9aQve4ceMUGhqq77//Xh988IFWrlypefPmlTlux44duu2223T77bdr7dq1uu222zRu3Djt3btXkrR161bZ7XatWbNGGzZs8P6EhoZKkiZPnqy0tDQtXbpUX3zxhdLS0jRt2rSqfKlVJjzIJknKL3ZWcyUAAAAAAI8qD907d+7UmjVrdM899ygkJESNGzfW2LFjNX/+/DLHLly4UN26ddPZZ58tm82m8847T927d9e7774rSdq0aZPatGmjwMDAMs8tLCzUJ598oqSkJEVHRys2NlZ33323FixYoMLCwkp/nVXtUE83oRsAAAAAzKLKQ/eWLVsUHR2t+Ph4730tWrRQamqqcnJyfI7dunWrWrdu7XNfy5YttXnzZkmlobu4uFgXX3yxTj/9dI0aNUrr16+XVBru7Xa7z/NbtGihoqIi7dixo5JeXfXxhO68YoaXAwAAAIBZ2Kr6gvn5+QoJCfG5z3O7oKBAkZGRxz02ODjYO2c7ODhYp556qm6//XZFRUVp/vz5GjNmjBYtWqS8vDxJ8g41P/w6FZ3X7d0D22Q8dRnGYcPL7Q7T1gv/d3ibBMyCdgmzoU3CbGiTMKOa0C7LW1uVh+7Q0NAyw7s9t8PCwnzuDwkJUVFRkc99RUVF3uMmTJjg89iYMWO0YMECLVu2TKeddpr33J7jPdcJDw+vUM2xsREVOr6qxcZGKCYyuPSGzaa4OHPXC/9n9u8MaifaJcyGNgmzoU3CjPyhXVZ56G7VqpUOHDig9PR0xcXFSZKSk5OVkJCgiAjfN7R169b67bfffO7bunWrOnbsKEmaPn26Bg0apPbt23sfLykpUVBQkJo3b66AgABt3bpVnTt39l4nICBAzZo1q1DNGRm5cptwJy7DKG2EGRm5sh0scG9WvtLTc6u5MtRWh7dJM35nUDvRLmE2tEmYDW0SZlQT2qWnxhOp8tDdrFkzde3aVY8++qgmTZqkrKwszZ49WyNHjixz7PDhw/Xqq69qyZIlOvfcc/XFF19ozZo1uv/++yVJf/31l9auXasZM2YoKipKL774ovLy8nTOOecoJCREQ4YM0bRp0/TMM89IkqZNm6ahQ4cqODi4QjW73TLtBy2V1hbuWUit2GnqWlE7mP07g9qJdgmzoU3CbGiTMCN/aJfVsmXYzJkz5XA4NHDgQF166aXq27evxo4dK0lKTEzUokWLJJUufPbcc89pzpw56t69u2bPnq1Zs2apefPmkqTHHntMTZo00QUXXKCePXtqzZo1evXVVxUdHS1Jmjhxopo1a6Zhw4Zp8ODBatSokR588MHqeMmVLiyw9O8neaxeDgAAAACmYbjdNf3vBpUvPd2cQxoMQ4qLi1B6eq4+2Jiqx7/aqgEtYzX1gg7VXRpqqcPbpBm/M6idaJcwG9okzIY2CTOqCe3SU+OJVEtPN06+cHq6AQAAAMB0CN1+IizIM6ebfboBAAAAwCwI3X7C09OdT083AAAAAJgGodtPeHq68+jpBgAAAADTIHT7iTB6ugEAAADAdAjdfiL8YE93scMlh9NVzdUAAAAAACRCt98IPdjTLbGCOQAAAACYBaHbT9gshoJtpR9nfgnzugEAAADADAjdfiQ86OBe3cX0dAMAAACAGRC6/UhY4MG9uunpBgAAAABTIHT7kbCDPd359HQDAAAAgCkQuv1I+MGe7jx6ugEAAADAFAjdfoSebgAAAAAwF0K3Hzk0p5vQDQAAAABmQOj2I4dWL2d4OQAAAACYAaHbj9DTDQAAAADmQuj2I2wZBgAAAADmQuj2I4eGl9PTDQAAAABmQOj2I56ebuZ0AwAAAIA5ELr9SERwaU93LqEbAAAAAEyB0O1HIg8OL88tInQDAAAAgBkQuv1IRHCAJHq6AQAAAMAsCN1+xNPTnV/ilMPlruZqAAAAAACEbj8SfnBOtyTlMcQcAAAAAKododuP2CyGdwXzHIaYAwAAAEC1I3T7mQjvYmr2aq4EAAAAAEDo9jOebcPo6QYAAACA6kfo9jORwWwbBgAAAABmQej2M57h5dmEbgAAAACodoRuP0NPNwAAAACYB6Hbz0QEBUiScgjdAAAAAFDtCN1+xtvTXczq5QAAAABQ3QjdfsYTuunpBgAAAIDqR+j2M4d6ugndAAAAAFDdCN1+JoKebgAAAAAwDUK3n4kMYvVyAAAAADALQrefiQguXb2c4eUAAAAAUP0I3X7G09OdX+KUw+mq5moAAAAAoHYjdPuZ8INzuiV6uwEAAACguhG6/YzNYigs0CqJxdQAAAAAoLoRuv1QRBDbhgEAAACAGRC6/RDbhgEAAACAORC6/VBkMNuGAQAAAIAZELr9UOTBbcMOFNqruRIAAAAAqN0I3X7CtneDAlKWS5LqRwZJknZnF1VnSQAAAABQ69lOfAjMzijOVp0PhkmS0kf/rKYxoZKknVkF1VkWAAAAANR69HT7gaAtn3j/25K/V03rhEiS/s4qrK6SAAAAAAAidPuFoD/e8/63pThbTQ6G7tTsItmdruoqCwAAAABqPUJ3Tbf/LwXsXe+9aRRnKy4sUKEBVrnc0q4DzOsGAAAAgOpC6K7pflvoc9MozpZhGN7e7r+Z1w0AAAAA1YbQXdPFtZQrtJ6c4Q0llQ4vl6SmMaWhe2cm87oBAAAAoLoQumu6jhcrc/R6lTQbKKm0p1vSYT3dhG4AAAAAqC6Ebj/hCoqSdFhPdx22DQMAAACA6kbo9hPug6Hb29MdQ083AAAAAFQ3QrefKBO6Dw4vzyywK6fIXm11AQAAAEBtRuj2E0cOLw8LtCkhIkiStGV/frXVBQAAAAC1WbWE7oyMDI0dO1bdunVTz549NWXKFDkcjqMeu2zZMg0bNkxdunTRkCFD9O2333ofKy4u1pQpU9SvXz917dpVl1xyiVatWuV9/Oeff1bbtm2VmJjo/Rk1alSlv77qcGRPtyS1qhsmidANAAAAANWlWkL3uHHjFBoaqu+//14ffPCBVq5cqXnz5pU5bseOHbrtttt0++23a+3atbrttts0btw47d27V5I0bdo0rV+/Xu+++67WrFmjSy65RDfffLNSU1MlSZs2bVL37t21YcMG78/8+fOr8qVWGVdQtCTJUnRY6K4XLknasj+vOkoCAAAAgFqvykP3zp07tWbNGt1zzz0KCQlR48aNNXbs2KOG4YULF6pbt246++yzZbPZdN5556l79+569913JZX2dCclJal+/fqyWq269NJLFRgYqN9++01Saeju2LFjlb6+6uLT0+12S5Ja09MNAAAAANXKVtUX3LJli6KjoxUfH++9r0WLFkpNTVVOTo4iIyO992/dulWtW7f2eX7Lli21efNmSdKkSZN8Hlu5cqVyc3PVtm1bSaWhOy4uTueee67y8vLUo0cPTZgwQQkJCRWq2TAqdHiV8dRlGJJCDoZuV4kMV5FkC1Hrgz3dyen5crrdsllM+kLgN3zaJGAStEuYDW0SZkObhBnVhHZZ3tqqPHTn5+crJCTE5z7P7YKCAp/QfbRjg4ODVVBQdu/pjRs3aty4cfrvf/+rxo0by+l0ql69eurdu7euuOIK2e12TZ48WTfeeKMWLlwoq9Va7ppjYyMq8hKrXGxshOQKkwyL5HYpLtQpRUYoJiZcoYFWFZQ4les21CrO3K8D/sPs3xnUTrRLmA1tEmZDm4QZ+UO7rPLQHRoaqsJC372jPbfDwsJ87g8JCVFRUZHPfUVFRWWOe//99/Xoo48qKSlJ1113nSTJarWWmSf+wAMPqFevXkpOTi7Tg348GRm5nhHbpmIYpY3QU19MYKQsxQeUu+FjBaSuVn7v+9UiNlSb0nK1+q99qlP+vzMA/8iRbRIwA9olzIY2CbOhTcKMakK79NR4IlUeulu1aqUDBw4oPT1dcXFxkqTk5GQlJCQoIsK34NatW3vnZ3ts3brVO0/b6XTq4Ycf1hdffKHnnntOvXv39h6XlpamefPmKSkpyRvSS0pKJJX2lleE2y3TftDSofrcQVFS8QGF/ThFlpIcOep2Vqu6/bQpLVdb9udrUNvqrhS1hdm/M6idaJcwG9okzIY2CTPyh3ZZ5QupNWvWTF27dtWjjz6qvLw8paSkaPbs2Ro5cmSZY4cPH641a9ZoyZIlcjgcWrJkidasWaMLLrhAkvTYY49p+fLl+vDDD30CtyTVqVNHixcv1vTp01VcXKzMzEw9/PDD6tWrl5o0aVIlr7WquYKjJUmWkhxJklGU6d027Le0nOoqCwAAAABqrWrZMmzmzJlyOBwaOHCgLr30UvXt21djx46VJCUmJmrRokWSShdYe+655zRnzhx1795ds2fP1qxZs9S8eXNlZmZq/vz5Sk9P19ChQ3324l60aJGCg4P18ssvKzk5WX369NGgQYMUHh6uGTNmVMdLrhKeFcw9LIWZOr1ZHUnS+l3ZysgvqY6yAAAAAKDWqvLh5ZIUFxenmTNnHvWxDRs2+Nzu27ev+vbtW+a4mJgY/fHHH8e9Ttu2bfXqq6/+80JrGNcRodsoylKj6BB1rB+hX9Ny9dWf+3XZaQ2rqToAAAAAqH2qpacblaNMT3dRliRpUNt6kqSlm/dVeU0AAAAAUJsRuv1I2dCdKUk6u01dWQxpU1qudh0oPNpTAQAAAACVgNDtR1xBkT63jYM93XFhgerWOFqS9P22zKouCwAAAABqLUK3Hznq8PKD6+t3PRi6f01lFXMAAAAAqCqEbj/iComVJDkjm0qSDGex5CgdTt6xfuke6L+ydRgAAAAAVBlCtx8padxfhR2vUe6Ax+S2BEo6tJhah/oRMiSl5hQrna3DAAAAAKBKELr9SUCo8vpPkb1xP7mCS/fn9oTusECbWsSFSWKIOQAAAABUFUK3n3IHR0uSjKJDC6d5hphvYog5AAAAAFQJQrefcoXESDrU0y1JnRqUrm6+KS23WmoCAAAAgNqG0O2n3AeHlxuHh+76paH79z25yimyV0tdAAAAAFCbELr9lCvId063JDWNCVHz2FAVO1x67vsdZZ6zJ6dITpe7qkoEAAAAAL9H6PZTnuHlRuGhOd0Ww9CEs1tKkhb8kqZfDltQbXlyhoa9tEbPfb+9agsFAAAAAD9G6PZT7uCyPd2SdFqjaA3rEC9JemTpXyp2uCRJX2zeJ0lauClNRXZnFVYKAAAAAP6L0O2nvFuGFWeVeSyp/ymKDQvU9swCvfDjDrndbv309wFJUl6xU8uTM6qyVAAAAADwW4RuP+VdSK2wbOiODgnQ/85pJUmav3aXFv26R5kFhxZWW/L7vqopEgAAAAD8HKHbT7mOMbzco1+LWJ3Xvp7ckh7/aqskqXlMqCRp1Y5MpeeXqNjh0nsbUpVZUFIlNQMAAACAvyF0+6mjbRl2pLF9mivIZpHj4IrlF3RKUKf6EXK6pa/+3K+5q3Zq6jdbNXM5i6sBAAAAwD9B6PZT3p5ue57kLD7qMfERQbossaH3do+m0Tq3bT1JpQureYaZ/7QzS243W4kBAAAAQEURuv2UOyhKrsBISVLg38uOedy1PRqrcXSw2sWHq0VcmAa2jpMhaVNarvbklob1fXkl2p1dVBVlAwAAAIBfIXT7K8Oioo5XSZJC182SjtFTHRFs03vXdddroxJlMQzVDQ9SYqOoMset35VdqeUCAAAAgD8idPuxgs7Xy20NUsDeDQrYveKYx9kshgzD8N4+p01d7393qh8hidANAAAAAP8EoduPuUPrqqj95ZKkkI1zyv28s1vXVXRIgFrEhWpMr6aSpA0pByqjRAAAAADwa7bqLgCVq/DUMQrZ9JoC//5Olvw9coUlnPA50aEBWjC6u6wWQ265ZTWk1Jxi7c4uVMOokCqoGgAAAAD8Az3dfs4ZfYrs9XvIcLsU9OeH5X5eRLBNoYFWhQXadGrD0jneT3+7jVXMAQAAAKACCN21QFHbSyRJwZvfP+aCasdz15ktFGA1tDw5Qwt/STvZ5QEAAACA3yJ01wLFLYfKbQuWLWuroheOUMjPL1fo+W3qhevWPs0lSU9/t03bMwoqo0wAAAAA8DuE7lrAHRihojalvd0BaT8p/IeHZBRmVOgcV3RtqJ5No1XscOn/Fv+hEoerMkoFAAAAAL9C6K4l8vpPUdbIT+WMbCJJsu37pULPtxiGJg5uo6hgm/7an6/nf9xRCVUCAAAAgH8hdNcWhkWO+C6yJ3SVJAXsPxS6A7ctVUDK8hOeom54kB4Y1FqS9ObaXVq9M6tyagUAAAAAP0HormUc9TpLOtTTbTmwXZGfXa+oxddJ9sITPr9/yziNOLW+JOnBJZv11rpdOlBgr7yCAQAAAKAGI3TXMo66nSRJtoM93UE7vpQhtwxnsWwZv/se7HYpYPeKMmF83IBT1Dw2VJkFdk3/bpuueH2dtqbna+3fB7Q8OUNut1s7Mwv01LfJSs8vqZLXBQAAAABmZKvuAlC17HEd5ZYha16ajPx9Ctz+hfcx276f5Tg4/FySgn9/SxHfTVDBaf9Vfq8J3vtDAqyae0UXLfl9n97dsFt/ZxVq1Ovr5Dq4G9mEs1vqnfW7tSOzNKzfdWaLY9bjcLn1xeZ9+vS3vYoIsumR89sqwMrfggAAAAD4B9JNbRMYJmedlqX/uWu5AtJ+8j4UsH+TLDl/K2jLIsntVuDObyVJtr0bypwmPMimSxMbaO4VXdSpfoQsboesFkOS9PhXW72Be83Bed9Ol1vuo+wR/vhXWzTxsz/1098H9M2WdL3+U8rJfb0AAAAAUI0I3bWQo96pkqTQtTNluJ1yG1ZJpfO8o5aMVuQXYxW4/XMF7FknSbLm7DzmuaJCAvTqaTv1Z8gY/XBuuro2jvJ5fFtGgdalHNDZs1fovk//kOuw4P3Xvjwt2rRHkjSobV1J0txVf2tnJvuAAwAAAPAPhO5ayF6viyTJdmCbJKmo7cjS25l/ypaxWZIUumGOLIXpkiRLXqrkLD7m+cJ3L5fVbVedtGWaNKStOtWP0FXdGqldfLgk6f8Wb1ZesVNf/5Wuuav+9j7vuR+2yy3p7NZ1Nfm8tjq9WR2VON26Y+Gv2rI/72S/bAAAAACocoTuWqi4zcUq7HSt7PU6yxHTRgXd7pArpK7PMQF71nr/23C7ZM3ZdczzWXJLH7Nmb1e9iCDN/U+ikvqfou5N6kiSz2JqL67YqY83pem9Dbu1YnuWrBZDt/RpJsMw9L9zWql+ZJBSDhTpurc26pfUnJP5sgEAAACgyhG6ayF3UKTy+j2iA5csVtYVX8sV2Uj2g0POJcltCSjzHGv2jmOez+oJ3Qe2+9zfo2m0979PbRCpEafWl1vSI19s0dRvkiVJV3VrpCZ1QiRJ9SOD9fqVp6l7k2gVO1x66LPNKrI7y9bvdmvjrmwlp+fL7nQds66fd2frP6+v07KtGcc8BgAAAAAqE6uXQ1LpVmJBO7+Wo05rOep1UvCfH0qS3LZgGY4iWY41r9vlLB1+LslSlCmj6IDcwdGSpM4NIhVks6jY4dKV3Rqpf8tY1Y8M0gs/7pDTLd1yRjNd17Oxz+miQwL0xLD2uvy1tUo5UKTr3/lZhXanQgOsOiUuVGNOb6r5a3dpwS9pkqTwIKvuO7uVzm1bT5L05Z/7tfbvA7qtX3M99W2ytuzP18TPNmv+1aepYVRIJbxzAAAAAHBshG5Ikoo6Xilbxh8q7Hy9jMIMb+gubj5YwVs+kjX76KHbkr9XhsvhvW3N3i5HcKIkKTjAqomD22jXgUL1bxkri2Ho2p5N1LdFrApKnOrUIPKo54wItun+c1vr9gW/6s99h+Z2b96Xp6V/7JPTLRmSQgOtyit26v7Fm/XbnlwN65CgiZ9tlt3p1q9pOfprf74kKb/EqQcW/6kXL+8s28EV1gEAAACgKhC6IUlyhSUo57xXSm+U5MsR00bOiIayNzi9NHQfo6fbmuu7xZf1wDY54hO9t89pU/fIp6hFXNgJ6+ndPEaTz2urtJwitY+PUJHDpXfW79LalGxJ0v+d21rnd4jXnBU79OrqFL21brcW/Jwmu7N0dXRP4B7YOk6rdmRpU1qOPv9jr4Z2SFCh3amQAKvyih2a+NmfOrVBpK7p0fiYtQAAAADAP0XoRlmBYcq64mtJUkDK95JK53QbBemSLUjuwAjvoZYyoXu7jPx9pccE/Lvh3IPb1fO53bdFjL76c7/Cgmw6o3mMJGlsn+ZqERumhz7/U0UOl8KDrBrWIUFvr98tm8XQHQNa6PM/9unZ77fr1dUpSssp1osrdmrC2S2VW+TQ8uQMLU/OUNt64erZrM6/qhcAAAAAjkToxnE5o5pKkmxZWxX7Wg/JYlVhh1Eq6HGX3IERsubu9jk+aPtShW54XvYGPZU9/K2TWovFMLxztw83qF09RYcEaO7qv3V198bq1byO6oYHqlF0iOIjgjSyS329/lOK/s4q1IsrSnvsn/1+u4JsVu85Ji39U4+c306t6oYpPIivBQAAAICTg3SB43KFN/D+t+EqkVxS6M8vy5qTopzzXpElp7Sn2x7XUQHpv8qW8YckKTBluSx5qT7Pr0w9m9Xx6am+qvuh4eJhgTZdflpDb+AOtlmUV+xUXrFTcWGBCg6waNeBIt347s+yWgz1bBqtga3rakDLWEUGl13JvTzcbrcMg/njAAAAQG3HlmE4PotNzrAESZIzsomyh7wkt2FV0PalCtzxtXe7MHvjvmWeGrjjq6qr0+2Wbc86yVF01IevOK2hujWO0iVdGmjahR2891+W2EDTLuigPqfEKD4iSE6XWyu2Z2ny0r80+IVVemnFTm3cla2Jn23WR7+kye0unTP+S2qOHvniL726+m+l55eooMTpfezZ77dr2EtrtHpHVoVfxqe/7dH075KPulUaAAAAgJrHcHuSAo4pPT1XZnyXDEOKi4uo9PoCt32uwL+/U/7p98odXEdhP05W6MY5ckY2lZxFsubv1YFhbyrqk6tkyC23NUiGs1glTQYoe9ibpbUWZcltC5aswaXhWJKjfjcFbv9SYaselyV3l1yRTXTgwvfkDi7tsbZk75SsgXKF1z9hjcGbXlPE8vtV0OUm5Z/xwAmPn/bNVv21L09PXdhREcGHBnzsyCzQ13/t15d/7ldyekGZ5yU2ipLL5dbPqTllHmsYFawBLeM0f13pHyKCbBbNvLijTmsUrZ/+ztLK7Vm6ukdjRYcc6j13u91al5Kt3GKHtmcU6Pkfd0iSbuzVVDf0blrmGkV2p35JzVHXxtGymnAl9qpqk0BF0C5hNrRJmA1tEmZUE9qlp8YTHkfoPjGzftDV1RCNkjzVeau/rPl7vfdlXLVKkZ/fKFvG78o96ylFfnW73JZAZYz5RdbMvxT90SWSSldJ96yEnt/1NoVufFGGs9h7npImA1TY/gqFbHpNgbtXyBUUpawrvpErLP7Q9fP3KWz1kypqf4UcCV0lSXXeOVe2jN/ljGyqzKt+/Nev0e1264vN+/XE11tVUOJQ7+YxWrUzy7s6usWQzm1bTzszC/TH3rwyz68fGaS0nGKFBlh1+4BT9PS3ySp2uNQsJkTTL+qoRtEhKihx6slvtmrxb3vLPD/IZtEH13VTQmSw9770vGIlLfhVW/bn6/z29fTQkLZHrT23yKGPNqVp/a5sJUQEqWP9SJ3ZKk6hgVY5nC6NW/irJGnGRR1ls57cwS414Zcjah/aJcyGNgmzoU3CjGpCuyR0n0Rm/aCrsyEGpK5S9MKR3tv7b9khozBTlqIsOWNaq878frJlb1fugCcU/PtbCtj3s/dYt8Xms7d3SZMBKuh8g6I+GyPjKMPDC9tepryBT3lvR3yZpOC/FsgVHKOsy7+UUXRAMe8M9D6eceWPckWV7SX+J3KLHCp2OBUXHqSt6flavjVD8RFB6twwUo2iS1dnL7Q7Vexw6bnvt+ujTXs0sHWcHhrcRnd+9Jt++vuA91yGJM/HFB5Uuse4VBrgm8aE6u/MAo3p1VQ/7czSht05qhceqGYxobrpjGaKjwjSje9sVGrOoT9Q3NirqRwulzLy7XLLrTNbxWn3gSI9/+MO5Zf4Dk8PDbAqqX9zGYahx77cIkl64NzWGt4pwee4zIIS1QkJ8JmPXpH56TXhlyNqH9olzIY2CbOhTcKMakK7JHSfRGb9oKu7IYb8/IrCf5goR2xbZV3uO387dPU0ha2d4b3tCoxQ9vmvyVKUIXuDXgpb9bhCfntTzvCGyrrsc7mD6yjozwWK+Hqc3MExKmz/HznqdlTU5zfKLUP2Jv1lFKSrsPP1ivjmThlulySppHE/OWLaKPTnl7zXyuvzkGx7N8oVFq/80ydI1sMWQ3M5FLT1E4X8/IqM4mzlDJ4jZ1z7k/aeZBWUKPpgaC20O5X04SZt3J2jRtHBenJ4ez3+1Vb9ctjQ9PqRQXpgUGt1b1LHG243783VtW9tlNNV+qGGBVpVLyJI2zMK1Dg6WKc3i9H7G1OPW8cpsaEa2iFeWQV2LUvO0N9ZhTIkRQTblFNU+gePBpFBenBwG/2SmqM6IQH6dmu6VmzPUrfGUZp8XlvFhQdp8W979cgXf6lV3TCd2SpODaOCtWFXtjal5arPKTG6tkdjBQccWgX+my379Xduia44NcFndfjK5HK7ZWHROhxHdf+uBI5Em4TZ0CZhRjWhXRK6TyKzftBmaIgBqavlDK8vV2QT3wecJYr4+g4Fb/lYkpTX+wEVJt506HG3S4F/fyd73U5yh9b13m3k75M7OFqyBkqSIr68TcF/LSxzXXt8omwZf/j0jDti28mW8Yd3TrkkFTc5U7nnPit3UJRs+zcp/Nt7FbD/F+9zXIGRKux0jSSp8NQxcofGHf2FluTLlr1dtv2/KHD7l3IHRSn3zCe9dR6LPXWjCr5/RrZTL1NYu8Gl17E7tTu7SHGhgYoKsfn0Itv2bpQtY7P+ih+q1NwSzV31tzbsLg3pMaEBevU/iYqPCNL9i//Qb2m56to4Sk3qhCqzoERLft8nl9ut//ZtrhGd68viKlHUktFSSb4eDJ+kz37dpa6Wv/RHSDcVu63KLLAfs+46IQEa0r6e3t2Q6g3/ktTI2Kdcd6iyFS5JqhceqHPb1tMFnRJU7HDpmjfXy+mW+pwSo5vPaKat+/PVpVGkooID9OO2TG3Yna2sArv+07WhOjeMOu57l1/iUFaBXdEhAUfdxu3PfXl66ttk/bUvT7f3P0WnxIbq2y0Z6lA/Qme1iivXnHe3u3ThvITIILWICzvh8Yez5KQo5Ms7VNLmItk7jipdTE9uhQWyKYTZmOF3JXA42iTMhjYJM6oJ7ZLQfRKZ9YM2fUN0ORW6fraMgn2li5udIKAejVGYobBVj8sV0Ui2PesUtPMbSVLWpZ/Lkpeq8O8flDV3l1xB0coZ9LyiF13hfa7bEiDDZZfbFiJnZBPZMv8sLSsoSoWdb1BgynIFpK3xHm9P6KYDF74vw56ngL0bZBRlqaT5ubLtWa/Iz2+Sxe47d7vg1DHK7/uw97Yle6dCfpmrwN0rZCnYJ3v8aQpMWS7DWSy3YVXuwKdU3GakzzksOSkK2vKxXKH15IpoqKhPr5bhLFZ+t9tV0PMe5eXlaff869TcvlX2thcrpmFrWQozJItV7sBIOWJayVG3o2QJlPWvRVJRtpynXikZFoWtfEyh658rrbX1SGUmr1Ij5y7tiTpNKxqO0caf1yhV9RTctKeyHEFqFZqvc+qX6LFNwfo1/dDw/3Pb1FWXRlHKTl6pu/fcpRJLsD7r8Iymbo7W3tzSP24E2SyqFx4oa/Z2dTaSFWA4VU8HFGdka5u7vlKUoAJXgH5zN1OBgmWzGBrYOk5/7stTdEiATm0QqQEt49SxfoSKHS49/vVWLf1tt+6zva0Qi0MpHW/T1vwQbUrNUaDNopwihwoL82WXTa6jbMLQICpYg9vVU/8WsWpdL1y2gwHc7nTJajG8PeNv/JSimcu3y2oxdE2XWKXkG3K63N4/CtidLk39Zqv255Xo3oEtZbMY2pSWqzOa1ZHxweVqkLlSRQrQ+Pi5+nyXTU6XS9fF71D3Hv11Wqtm3nocLresho47TH9HRoFW7czS4Lb1FB164q3q0vNLlFfkULPY0BMe63S5tWpHllrWDVN8RNBRj/Hnbe5M/7vyROwFUsCJP2fUHDW+TcLv0CaPwe0ufXNQLWpCuyR0n0Rm/aBrQkM8qVwOhfwyV66gaBW3u9R7X8CuH+UKry9nVHPFvtJJFnueipsOVEG3JEV8O94btt2GVcUthyrvjIlyh9WT7IUK3fiCLPn7FLTlI1lKcmWP6yBbxmYZ7tI50Y6o5rIU7JfFnidXcB05o1vIEddBIb++Vvp4dAtZ8vfIFdlE1qwtPnPVPZwRjWXNTfGezxnTWoazWJbc1NLn6OgfXl6fhxSQ8r2Cdn593LfFGZYgR92OCjq4RVtxi/NV0rCXwr9/0DsMvyIcUc31Q/3r5Ez+RqEBFrXtcb5cDXsoavE1sh3YVvq2B4Qpt/PN2uBuqY1/bVN+5m4lWrZqsOUnWYxjN8YcW4zmR9ygr/cE6RzrOp1vXa1trvp6zzlAbS1/yxUQpkW2wdqRK80MeFbnWUv/KJLujtRE+7Va7OqpINk1wfa2rrZ+oUJruPaEd9RfWU796j5FyU2v0KrdxbIVZegy63fqbElWA0umdoe00+qwM/XmnoZqVTdcM4a11M97izV+0e+yyKnJtld1qfU7PeW4VM87h0uSTm8arWhXlj5PMWTIpUHBf+gvR7y2OeJ0XfQGTSya6n1dC5x9dKd9rO62vav/2j7WD84O+qDNTHVvWkd/bd6oHn+/qD9s7bSzyUjd1L+tYsMCtSk1R63rhamwxKkpX27RD9syJUldG0fp+UtOLQ3AbpesGZsle6GWpZRo2s+GujSuozv6n6IrXl+nzPwSPX9pZ2UVlOjNtbt0Yce66t28jn7ckaPODaLULDZUDqdLD33+p5Zu3q/Bgb/ojvq/K6TvOBm5uxX9w4NaHdJPT5WM0PbMQjWtE6KXzjRUFNVCWw+41DM4RYYtWM6Y1qWfu9uthRtTFB0arLNa160xIb1G/a50lihs5eOypf+m3DOfVPBfCxS69hkVdL9LBd1uq+7q8E+4HJIMyXJoyk2NapN+ypqVrODf5quo3WVyxrY56jGWnF1yhcZKtpAqrq7q0SaP4HYp+Lc3FbbqSTkjm6ig+x0qaXZ2pQdwoyRXcjm8O/lIkkrySzuurMf4g7y9UAF71smavV32hK4nddpkZTCKshS65mk54juruM1IGUUHZNgL5IpoUPbYGtAuCd0nkVk/6JrQEKta6E/TFbjtc+UMeVmuyMal+3en/SRrfppKGvWVOyTmqM8LTF6iqM9v9N52RJ8ioyRf1oLSlcVLGvZS9rD53t76sBWPKHTDC2XOU9KkvwrbXSFXaD0FpiyXK6yeitqPUtiqx0rnkbvKDukuadhLtow/ZSnKlD2hqxx1Winkj3e8j7utQcrvcbcC0lbLcNrlComV5JalIF229F9lKSrdD9xtWCXD4nONoraXyG0JUMjvb5UG5bOeUujGF2XNSZG9bkfZDmzzribvttjktgaX6dE/nDMsXs7olgrcfZwV4hv3VL4rSO7QurKExsqV/qeseWkKLMmStXD/sZ93UKY7XDIMxShXbkugcoIbKKpghyTpQJ3OCi7co+Cisiu+S5IrpK7s4fVly9gsq6ukzOOvO85RmFGoi6w/ar2rlX50ddR5EdvUqnCj95h3692h+btidb/tTfW0bNa3rkSFBUg9nBvkcFu03t1anY1kBRl2rQnuqx5F30uS9ja9UPE7P/KeZ1TJfcp0R+qNwMcUZ5ROEchwR2it0Ukl1jBFlexRHWuhChWsV0sGqrFlv4ZY1mi7O0GWRj31e7pdl+sznWLf4j1ntjtUP7o6amNQd/1eEKlcd4gCQ6P0d1GQWrp36lHbK6pj5Gq+82x9Y+mti8/qpw9/z9bGv9P1H+vXmmh7XRbDrQPuMAWrRMFGaVt5wTFMTzgu04O2N3SdbalSFK9ljo660lb6B5814QO1r+lw5aRt0ZCMecp0R+i9+DuV0OFMNakTqlPr2hS4e4Vs+37Wn0V19Kc9Xt1aN1VkXCPZs/eo3uqHZMnfo+zeE7VyZ7as279Wp7qBiomJlSO6hTI3LFR81lplB9RTbnRbFSX0lKskX8FF+5WQ0FCKbiZXVDMFbP5Qlv2/ydnwdKUFNNbuffsV32mQ6jZsIUmyZ+5Q0YpnFdEkUUaboaXTStLWKnTjHAW1OUvpTS6Q21baY2zJS1XIptdkTf9d7sBwuSKbyhHXXpJbxcH19N7+JooPMzRQaxSy80vJ7VZR+8tlb3iG5CpRQNpauQNClR55qqJcWQpOWyV3QJgCdq9Q8J8LJEeh3MHRKm45VIUdrpQrqpkkyXpgm8JWTJHdGqJ1TW5Umxibgov3y1HvVFmzdyj8x8neETglAVEKtGdLktwylD38Ldkb91VA6moFbvtMRR2vljP6lEMN3O2Sbe8GBexeKVvGH3LEJ6qw49XlG2Xkdpdu3RgSJwX4BgxL/l5Z03+Xo17nY/4Ote37WUZJnhxxHUqnCB3OWSJZAo7+j1WnXZb8vXKF1ZWsB0dhOIoUuOsH2dJ/kyUnRbLY5KjXWUXtLi89h9utoC0fy5b+q9wB4SpuOVTOOi0PvY6cv+WKaChZjj3Vw8jfp+AtH8sVFCln9Clyh8TKGdmk9DnOYhkleXKHxB46Z16qrLkpMgozJEuA3LZgGcU5cofGyV6/h2QcfSeIgN0rFPHV7ZKkvL6TJEugLHlpsjfpq5imrZX591bJaZdbhmRYjl+3s1iGvcD3H+Mni+cfEZ7PyO1W4LYlCvz7O7mDYyRnsazZO2XN+VsyLMrvfodKWpx3Ui5tFGYq5NfXZCnMkCswQkXtLvN+X7zHFOyXDMuhz+Qg64FtCtkwR846LVTc4vzS9+8Itn0/y5q5RYajSPZGveWMPkUBqasUuWSMLMXZcgVFKXv4W3LU6+zzvJCfX1b4Dw/JEX2KDoxYKHdwHQXs+kGBO76SI66DittecszP/aRwuyRHsc/3MXD7lwre/J4subtlb9Rb+T3vLQ1ibrcC0tbIGdXs0G4vTrsC9qwtbVdBUbIe2CZb5p+yZu+U3fN9CgiRbd8vCv7jXZWccq6iThuq9P05sqX8qKCtn8gZ2Vj2JgPkCo6RpWCfrAe2yRUWL0dsu0O/CxxFpd/dg23HKEiXLfNP2et1kQJ9p23Z9m6QLeNPlTTqo8Bd3ysgZbkKO1/v3YlG9gIFpq6S22KTM6a1XGEHF3t1uxW47TMZ9nwVtxwm2YJ1JFvaWgXs26iSJgO8vw8suamyZW5WSZMBpZ+V2y2jOFuBO79WyG9vSo4iOeu0lL3hGXIHhCjor48kwyJnZBMF7vza29HgUdK4vwo7jFLAvl/kCoqQo24nOeI6ypa1RYHJi+WMaa2itpdJhkWB25cqaNtncsS2V2Gna0vfH8N67OB88P2J+uRKWYqz5QytJ1dYvAxHsWxZf8kVFKXilsNKX5ujSLaMP+SMaiZHQleFfzdB1rzStX7cFpsKut8hud1yWwNV2OUmnz/4Hc4oylLQlkWSxaai9lf8q/YckLpaAalrVNTmYp/wbNv3i0I2vihnVFMVJI6VLXubIj+/2ftvz8KO1yhoy0cy7PnKGfKKSpoN9DlvTcg6hO6TyKwfdE1oiDVJ8K9vypb5h4raXSFH3Y4yCjMV/v0DMopzlHvOLN9/SLqcCtq6SO7ASDkjGsmas1Ou0HpyxHc55vmNklwFpHwvS1Gm3JYAucIS5KzTSq6IBjKKcxSw6weVNO4vWawKWz1Vtn0/S4ahgq63y964z9FP6ihSyG/zS/+P67RbJLdLEd/cI1dwHZU0O1sFXW6S5FbIL3Nlb9xPjnqnHuUchTIcRXIHhMqwFyjiu3sVsOtHFbe6QK6gaAWmLJNt3y+SYSj7/Ndkb9RbwX8uUOC2z72v2xUWL1d4/f9v787joyrvxY9/zpl9JvuekISEbOwQCJsoVSqislQRW1uvVXuvt7d46+2t0tpfF2qp1lq31ltaq9bayr115XqxWGmL4sYisgpEkkBIIPskk8nsM+ec3x8DQ9MAamtIgO/79eL1Is85M/Occ75z5nyf85znIVx5Nemjp588JmMhnO89gv2DF+I/qKmlhMZ8DuuRt7G0bCKcOQ7a9+DqO3Y33ZpC37yfESmag/O9R3C+90iiJ4HuyKRv7gPo9vR4z4SID8f7T8UvCI+J5kwiWHE1bXoqRsOfqer4wymPjWGyESm+GNuhV0+5jqaYMBknRoXfZ5uE6wvPkrP1hzj2/u7Ees5cTIF23OZcnLFeHITwpVRgivpxBE8/AN7JBAwbnUYqWUovLiX84S/4Gx7DhYMItmMJtps0MvEAUK+UUG40AhC05+I4RWPGqfxFq6bJyOEL1jew6cGPXbdPQtiw8EHRddSZyplz+CGyj22bplo56JzMKP97/Y7bR/WGNoESpY1itX9DkYGCjoqJ+Htu1UczXm3EycCZFxKvUUyERl9LIBIj9eBazMbpj2MfTtx6MiVq/Hi4rSPIjBwlpDrxJ5WS6d0LgG5Pxz3pq7Tt3UCe0kOG1okp0NHvvbTkQrS0MrSkfLozp/FU+0gOtHbz/eSXKFA9xDKrMHmbsbRvTyQ93vm/JFo4G+peIXnnKuxde+LbYbYTKZyDqe8IhsVBNHsisZyJWJrfxHHgxRP1z59NS84ljHAa2BrXY2l9F93iwm/NpjaQDCYbI5IgR2vH5DuKYujo1hQiRXNQYgEsre+iRvoG7JdQ+SL8M76Bc9djON7/7Yn9q1oJTFlGLHcyjj1PYm3aiJZcRKjy6mOP45jjY48kj4hfxMdCJL92R79pLwG0pBGEK6/Gvv9/UINutJTieEOkr+X0jZEpxURGzMKwp2PqbQTFRKezknR/Pc5Dfzh1jyNFjSdW/Y5XEX2XPoSp+wAmzyEixReju/KwHH0b17afoQa7iBTMxLC4MPfUY1hcaKnFhEvnY+7cg6VlC8FJtxCuugb0KPb9z2Jteg0tuZBY3hQiRXOwHv4L1sOvoUT9qL5WzJ6G+G+A2UFo1JV02EtRGl+n2LvtlNsMEBkxO36B7cxG0cJ0drRgdaSQ5rTi2L4qftet8ELQo5h76tHSSonmTEbLGI310KtYmt9ATy3B5GlADff2O56x7HGYu/ahOzIwbOmY3fswzA76Lr6XcNU1KJE+7PufxbX5XpRY/NxjKCrBSbcQnPTPKBEfJs8h7Pt/j63xT/33sTMn8T05PgaMbnERmnATocqrQLXg2PkrHPv+O/GaWHolSiyAqe9IoiyaM4lo7hTUsAezez+6I4tY9nhi2RPQUksxTBYsLVviSW5+DdHCizBsKUA8QUx683uYPAfB7CBSMAPdlYN9/7MYFifhisXYGl5B9bUQLZhOuOIzKFoU11vf79c7LlI0h9Doa7HXPo+1eSO6PYPeK5/A2vQ6jr2rUYNdpzx+usUVj9lj22QoJpQLvkr0wIZ+Y9+cjIFCLH8axEJYOnejOzLRUkbG91H3ARRDR0sqwD/zm/F9YUvFcuStYz3w+p+PDdVCYNrXiWWNwfXW9zH3NibiwDf7O8Ryq3FuewRb4/p4vR3ZREZeEm8kMzQULYLJcxDbwVdOHJu8GqIFM3DsfhIlFiA88tNoaWXY961GjfpPu23991ESgRl3oAY64jdOtA//DdYd2RAL9jtnHI8zw+wkUjgbw+JCiQZQon50ZzbRwgtAj+HadC9qxHuadz81zZmDnlSApWNnv/Lg+BvxzVmJuW17/NwW6SMy4gIsbe9ha1h3Ygykknn0ffpBDJMd5/b/AhQCNbeh+lqxtGxBTylCSx2JYU1G9TZj6dgZP7c7MtFSSkh68zsoeix+fZdWFm8kjIUw+dtO7AezI/F91a0pA7bVMNnwXfBtDEcW5vYdYLISGn8DGaPGDOtcR5LuT9BwPdCSdIszRQn1oET86CmFp1/vH41JLYr1yJvotlRi2RP63aEzuT/A0rIJLa2MaO6UAS3oaGGszW8BBporP9696q/urFkb/0zyX75OxJHLH7L/lemOFrIizeiuPMJlV6Jljsb19g+wH3gJJeIlUjib4OQv49j9a5SIF9+ce+LP+7e9R6Rg1on31zWsjX/Gvm81hi0V/6xvkbH6YpRYAIj3ZPBe8TiG2YlxZAt7t/0Zq6JTUT6GD7xmTB27mNH9EthS8E34Euu27iY7dJBSV5RdWil3911JWlY+P7qyknKtnnf+/AzZPe9R7IiQqoaIBnpJVfzxcQNGf4GenFnkNPye2NGdJBsnftB0ezqByV8mOOmfsb5zH2AQmf3/sNc+i+vtHyYuEJ5yfJGpjnYqw3t4MfPfaNZzmO9fQ4ZnN6oRpWvcLWQHD5J/8Jl+u/+IkcUWfTQFdFNq7sKh+0hV4vvgL1o1R4wsbjT/CR8ONtnn8H5fEjmKhyq1mVqjmPCY67BGPbg6d1Ac2ENQTeJQLAOX5qFSOUK50sIOvZw/6VOZqe4nXfGRZIox1qjvV496vQANlSr1xMXxRn0yI2lNJLHHbdLG8rI+ExtRKpQjVJpaiBkK1UodViV+YdhupPGidhF2InzOvDGRXLcb6aTjTaxXrxcQxkIHGfzRfgUpBWMoiDRS1vwsc0x7+n3um9p4AC4yvU/QsNJFOkVKOwHDxnp9Kj+NXUNEdXCv9Qn2Rgv4aexqnrGuZKJ6KH4sDQW3OYdsbWAjSUh18b59ClpaGZPda7GF3QPW0QwF02keA9FRiaFi5VgjFwp+SxbJ0VP3VNFQ8VpySI+2nXKdUzEUdUBiGrTn0p4+jVZTPj09bi7zv4SZExfrOgqbkuYzweUhpX3zx/5MgGhqKWFHHkpvE9awG4t+moYT1UyvNZ9DQQc2k0GGRaNXt1MUO4zTOP1F/HOxObhJ4WbTq/So6SRlFePq2o5i6IQNC5pqxqIqKHoUs3HqAS4/jnB6FeaQG9Npkq4PfQ8s+EZ/HqtZBdWCljKSN9zJ2Nu3Mq/nGRRj4ONUf69Y5ljCpfOwtG3HeuTN066r2TNQo/5EshDNn46BgrV1y8nXR+WgczKFqTbsbVvijTyKiSO58zDPu5usDV/FevSdk742MOkW7B88n+hRpltTCRbPxXF4/cdK3iCeqIZTRxGKGaT6D53y0bIP01O2lDcCRVzZ/ovTxiyAbs/AsCajhD1oqSVoGZVoSQWYa1/A5juebKvEssZh6TxxnjLMdkKVV2PytWBu34US9WPYUoillaMG2hOJ8Sk/9yQJ1XGd5nyyY63EnDnoGZVYj7zV/7WOLHRrMubeQ/3KDdWC7sjsl8T1W45CLG8K5vadAxL7v6Ul5RMa90ViGZWYO9/H2rQBJewlXLEYw5oSbzDKn0ak9DIMazKGYeBv+4CC9350rAv3NJRYAHPnHsy9jRgmG+FRl2M9dmMF4uMHhSuvwtr4l36NNR8mmj8N7/xfovrbUANd6IbBB+oockKHyGt/nXBvOyFNJ3nEOGyNf8bSsYtQ+SJ8l9yHZnbRvOHnpNc/z+FoChcrO1EVo1+y+7diGVWYehsTjU+aPQPLsUcioxlVmI8t+yi05MIB22qgEC5fiKVjFyZvE4ZqIVJ6Gd45d+Pa9SscO39FcNwNmLxNJ32c0lDNKFfcR1fpZ4dtriNJ9ydouCa1knSL4WbYx6Qei3fvGuRnsmz7n8Hx/u8ITryJcOWSj9Vlyx+J4fZHKU53EI7p7GnxMj4/OTE1W0zT2d0af2bbpCp4glFSrKAaWr8ud7qu0+XuIE/1gNmOnjLylNuthHux1z6Hbs8gXLXklHX768HWTJ6D2N//HW2tTaw8OpmN+kSmFKXzr7NGUl2YSps3BBE/Vs3Py43QGYgyI93H5LISnEkpPL+zJT5dndPKZVXZTChIGfB53YEIP9t4EH9EY15VNpNGpJLptNAbiuGymlCBV//wO4pb/sBEUyOR1FGE56/ix2+14z2ymy8mb2e7L43H/bOxmlVm5xpEYjrecIywpjJ7bCkXl2exscHNml2t9ASjmFSFa0b4+FbSWsxZVbyavIQX9nnY3NiDikY6PkpTVd7tTaJEaWNl7lscMFdxz9HxaMbJj/Ns836WKK/TaaThzpqBvXwu00emka538R/rWmjo1Umjj6hqY0pJHgvH5XLhqEwsJoUNdV38cX8HOU6V0kgt3a0Hed2bz1Eji59YfkmVcoRXmcn2WCm9hovdRhkR4t0XXQSZpe4jBT+V6hFmqfsYrzZiQudtfRwvazMpU1poNTLZrldwwCjkLstTLDW9AcQfZ/iNNp+nYvPpJplZ6j7GKY00Gnk4CTFRPcgE9RAGCvdFP8d2o5JCpYPrTK9RpTQTxMb7egnr9BmY0chXuvlMiUGKFTY0eGmMZdBuyudoLJka5QOmqnV0k0ydPoIdRjnGXw2SOEPZz/ctT1GstBPEdmyMh5mAwVXq21xlepsCpYsGo4BHYkuYqn7AROUgLWSiYlCgdDFCcZONhxQlwCZ9LN+J/jNe4o8b2Alzk+lVLje9y/9qs1mjXcj4Y9vWo6TTairAExn4/bET5hJ1JxNtrWTQRzN5aJEg49RGGo1c/qJNYRcVlGW5aO0N4IvogEIKPmzE6CQViL9vMgHuszzKFaZ3adaz2WKM4VPqLizEaDayeVa7mDf0SVyqbiOGmQ+MInIdUE0t06Lv0mJk0WEt4rOxtViONQZ1Gam84ljICFuQSaF3yQw306um89/aXA7FMvCQgpFRwa4ugxF0sMT0Jumqn2a1iGfCM/A6ivGFY8T0+MwMfeF4ol1m6uA/srYzKfwuhhalJwzdRjIpip9MvKw3pvOudTpVod0EsVFrFFOmtDBZPcgYtYkDegHPaheTSw9hxYqp8gq6Ajrbmnv4tP0AE5N95FXMINfix+hro8E2Huvep/lc8JlEg1GzOoI15it5O3UxF5Rls9C+m5zNP8AVOorPcNCi5LFXH8nPows4aBSQl2zji2NMdByp47mWDPw4sJlVXGaFi9jG9axjnNKIy/CzUZ/Eo7EF+PIuYLRxkFnuF9iojecVbRphrExI8nFj6g7yzX48MTOveXIwhdyMVw4xQT1EntKNkzD7jWK6bCWMDe+kVO2fLG6wXUrlwjuJBroJ7nuZ7rbDPNE3nQz6uMS0k3f1KjbqE1lk28nVls2MitbxtPoZVkauIxwzGKc0cmfyH6l0BbGk5NA++p9Jf2cFeX176LYWsGXkrexNvog97UGO9oa4bHQ2s0rSebOhm99tPUSxfoQpOSYqy6r4xa4wXzU9z0L1bfZmzKex5POMLRlJdyBCfZeflt4Q3lCMqG4Qjem4wu1MDG7GZLHhzbuAApOXg4fq2dMVo4FCrp05jtEHH2ek5x2SDB8pBACDX8YWs0pbTBJBHI4kFowvIK3uGWaH36DK1EJnxjS2j/4mKWlZWHb+mllNv0BXLfTlTOdp2+fZ7MumOryN8abDFJncKCYrdruDnPR01NELiWVP4GBjA5v/8Ciz9Pd4RZtOd8YUVhirsNuTaB1/K69FR7P1SJD32/pw+yPEdIOJ+cnMrcxm8fg8IprO2we7aesLoSoK5Vku/mf7Ud5t8jCtOI1rJxegANuaPTR2B7ik2MbU4nTcUSsj7FEKQx/gVtJZ1+Jkw8FeRqaamZvdx75eG9ZAGxO1PaQ5zHRHrWxo9FNiHOViewMmm5MWayk/i3wG1ZHCnLJM6jp8vHWwm55gFAWoyHZR1+nHID5LzFcvKmGUI0Ctz8FbB7vZUNdFXeeJxqAbTa9ylyU+BlHAsPEn5QJ6rHnUGHvxOYvZn3sV/9uZi6tnLyuMn1OlxJPtDiMNG5FEw/khUymWWB95dGNWdLwkUWsU02Abx2ijgerodt5JWUDtpO9RpB8hAw9WmwNv1ESrnkZTNAVPXx/O7n3sDufR4LfQ5QtTmOZgTkkyzX0a9a1dfDb8HJNNjYywhWixV5ASbGZSdCd7868m95pHhud1JZJ0f6KGawIx7BMccd6RmDw/dfSFsZpV0hwfPur6mRaJ6dR29FFTkUs0EDplXIaiGnWdfkoznSedoq7VG+IvB7qYkJ/M+PwUXnq/jWhM59rqAlRFIabphGI6nmCUw91BXqvvwu2PcF31CMblJ7O7xUtZ1sDR48Mxndr2PhRFoTjN8ZFGrq/v8rO+toP1tZ3EdIOHl4znzQY3v323mZkjM6jMcbG3tY89rV66A1FsZpWxuUncPLOYWXkqplA3DbFc9nX48IU1cpKsdPojPLmliWA0xpIRfmpKcxlXNQar2cz6Dzp5dX8HVrOKy2qiNxSjNxglGNW4uDyLK8bm0NAV4PW6Lna1eFk6KZ9Uh4Uf/7kOl83MTdOLKE53kJ9ipzInPt3grqO9fP1/9+INxZO4imwXVTlJNHT58QSjOK0mnBYzqQ4zUwpTKclw0tYX5tkdRzncHWBeVQ7hmM7GBjcKUJzu4DMT8tjd4uX1+vjd/XSHheJ0ByWZTkoznIRiGt3+KJ3+CLuO9tIdiGJSoDDNQWmmE6fVhC+s0ewJ0huM33H2BKMcnzXRZla59aJSbGb12ECISexv6+OV/f279NvMKnddUYVJUTjUHeDTldkUpzto7wvz3T/sT0wDWZGTxC0zi3llfwd7W+Pv936Ll4xwE0eVXGaNymFuZRZJVjM7jvTyf++30RuKoSqQnWQjqumJqR+zXFYCEY1AVKNEaWW00kynkcr7RilhjvcYMsjBg4ckIlhQFfirGSGpKU5j0bhc5pRl0huK8qX/3jlgaslMl5XidAc7jvTyt26aXsTo3CQe39REfVf8wl9VIC/ZRordwqHuAOFYvEdDbrKN/7x4FOtrO9lQ99HvxmfgJUvpRUOlwSjgeIPFCQYKRr8Gmwn5KXQHIhztPXFX2GZWSbWb6fD97dgfBhY0opgH7J+PIjfZRpbLSprDgoHBpkM9ifvZxZY+RmkN5KQmscefyv5I9oDXm1UFu0XFF9YYk5uEoijsa4s/amEjkjiW4/KSOej2E4z27yFiI8IUtY739MpE49up/D3b92FO9p4Wk0KK3YJZVZhenEZWkpVX93fQ4v3wu6cqOgb0O54nowBj8pLJcFrY1NiDphsUpzvo9IUH7KPTSbWbCcX0RJz+PRwW9WN95kfhsprwR07cvTepSmI6V5tZ7Vdfl9XE9TWFXFKexd1/OgBtu4hhps4YgcbJn+2OM/iUupsJykHeTFlILODhRu1F3tbH8ZI+G5vZhFkFPRIgiI2//u7ZCRPi5DOj/KNy6GHphRP40sxRw/a6clgn3W63m+9+97ts3boVk8nE4sWL+eY3v4nZPPBCZ+PGjdx///00NzeTn5/PN77xDS655JLE8scee4zf/e53eL1eJkyYwF133cWoUfGBZQKBACtXrmTDhg3EYjE+/elPs2LFClyujzcf73BNICTBEcONxKQYjs7XuDQMA39Ew2U1feSR5j/JqeM6+sK4bKZTzl3vC8do6gkyMsPxkee3NwwDzSAxDWBPIILDYkr0BAFo84ZwWk2k2E+ddBiGQYcvQobTgsV06gv6qKbT6YsQielkuqwk2wfWs8sfwROMEtN0Ippx2sYTwzDoDkSxW1RGFqThdvv6xWQgorGrpZeKLBdZSQMbaHoCEbJcVswmlVBU47mdLbj9Ub40s4iYbrC9uTfe0JDhRDMMWnpDNHYHOOQO0OYN0xOMUpbl5JKKLMoyXXT4wrzX7KEyJ4kxuf0vGpt7gmxt6mFKYRpJNhOt3jAV2S7sZpUth3uo6/TjC8dIdViozE5ialFqInaaeoJ0+sJU5SQlGrFiukGXL4w3FGNkhhObOb7f97R4eWF3K1kuK5ePySEc1dh8uIfX69xohkGyzUyK3UxZloulkwto6glQ2+5jRKodu9lEY3eAF3a1cqg7QEW2i9mlGcwfk4MnECWi6cwYmU44pvPy3nZ2HPFgAP96wUhKMpw09QTRdANViScxmw/3UNvex6WV2YzLT+blve3YLSamFaeRZjdjNqkowP72Pva09tHpC2M3m5hZks7EgpQBDXb1nX72tfUxpSiVwjRH4vu1t62Pr695n+5A/A5mebaLacVpXDdlBDlJNrr8EbKTrIRjOr94qxGrWWVeZTaBqIZZVRifn8zR3hC/336U7Ud6OeoJoRsGpZlOpo9Mpy8Uoy8cw2JSKM1wkuaw8N/bj9LRF2ZMXjKLx+dSlObgqy/sIRzTufXCUpLtZho8IQKBCA1dAfa2eclwWqnKSaIwzU6604LVpGI2qVhNCmZVoTsQ5YgnSG8whstm4kszinmtrovfbG1mxsh0PjelgPF5KfFHFP7me7VmdyubG3uYWpRGQaqdDXVdeIJRwlGNDl+EFLuZK8fm8np9F7tbvMwuzeDSymxMqkKrN0RLbyjeaNnh63d3F2B2aQY/XDCaUFRjze42/rCvnba+MIZhMC4vmVmlGUwqSGFEmh1Nh7cPdfPcjqM0e+INMxXZLsbkJhGI6Oxt81KVk8QXphaybl87+9t9qAqUZ7koyXDyyv4OWr0h0p0WWr3hY/EEVTlJXDk2l/ouPwc6fIzOTSLTGW/gbOoJEo7pfHZyAWVZTrYc9tDlj2AYBhMLUmjpDbG1yUNZlos5ZRlUj0il1Rv/rk4uTMUw4OGNDbzb5CGqGTgsKjNGpjO7NIOLK7ISDeC9wSiPvHGIzCQr86qyMQwDtz9Cpy9+jLv8YSaPSGViQQpOq4n2vjChqM6MkWkc7Q3x6y1NhKI6Ock2vjB1BC6ribcOdpPhtJCdZOOIJ0RUiyf8e1q9HOwK4A3H8Iai+MMaqQ4zWS4rmS4rWS4bWUlW8pJt5KXYyHBa2XGkl10tvRSnOxmfl0xlThIfdPjYebQXk6KQ5rQwPj+Z6VW5A86Vw8mwTrpvuOEGcnNzWblyJV1dXXzlK1/hqquu4l/+5V/6rdfY2MjixYt58MEHufjii1m/fj3f+ta3WL9+Pbm5uaxZs4aHHnqIJ554guLiYh566CHeeust1q5di6IofOtb36K1tZWHH34YTdP42te+Rnl5OStWrPhY9R2uF2rn64WkGL4kJsVwJHEphhuJyU+WYRhENWNAcjecRTWd3mCUNIcF82kafgaTJxhF0w0yXdYBManpBib172uA+yQb7z6KTl+YTY09dPSFubgii/Ksk99cO129YrrBtqYeku0Wxh7rZfBxBaMaLb2heIOQ5XR3lT8ZoajGEU+I4nTHWRX7H8fZcK78qEn3GT9Chw8fZuvWrSxfvhyHw0FRURHLli1j9erVA9Zds2YNNTU1XHrppZjNZq688kqmTZvGM8/EB/B59tln+cIXvkBFRQU2m43bb7+dlpYWtmzZQjAYZO3atdx2222kpaWRmZnJHXfcwYsvvkgwODSj7AohhBBCiE+WoihnXdJhMalkJdmGLOEGSHNYyHSdfErBvzfhBs5owg3xxy0Wj8/jX2aNPGXCDaevl1lVmFmSwbi85L+7/g6LibIs1xlJuAHsFhPl2a6zLvbPVx+tP9cnqK6ujrS0NHJzcxNlZWVltLS04PV6SUk5MZhOfX09lZWV/V5fXl5ObW1tYvktt9ySWGaxWCgpKaG2tpa0tDSi0Wi/15eVlREKhWhsbGTMmDEfuc5n+NzxkR2v13Ctnzj/SEyK4UjiUgw3EpNiuJGYFMPR2RCXH7VuZzzp9vv9OByOfmXH/w4EAv2S7pOta7fbCQQCH7rc54tPf+N0Ogd8jt//8aZ5yMz88C4DQ2m410+cfyQmxXAkcSmGG4lJMdxITIrh6FyIyzOedDudzgHdu4///bcDnDkcDkKh/nMQhkKhxHqnW3482Q4Gg4n1j39OUlLSx6qz2z08nyNQlHgQDtf6ifOPxKQYjiQuxXAjMSmGG4lJMRydDXF5vI4f5own3RUVFXg8Hrq6usjKygKgoaGBvLw8kpP7V7iyspK9e/f2K6uvr2f8+PGJ96qrq0uMZh6NRmlsbKSyspLS0lIsFgv19fVMmjQp8TnHu6B/HIbBsD3QMPzrJ84/EpNiOJK4FMONxKQYbiQmxXB0LsTlGX/yvqSkhKlTp3LPPffg8/lobm5m1apVLF26dMC6ixcvZuvWraxbt45YLMa6devYunUrn/nMZwC45pprePrpp6mtrSUcDvPAAw+QlZVFTU0NDoeDK664gvvvv5/u7m66u7u5//77WbhwIXa7/UxvthBCCCGEEEKI89CQDHf3s5/9LDFv9mc/+1kuuugili1bBkB1dTX/93//B8QHPvv5z3/Oo48+yrRp01i1ahWPPPIIpaWlACxdupSbbrqJW2+9lZkzZ7Jv3z4effRRLJb4/HQrVqygpKSERYsWcfnll1NYWMj3vve9odhkIYQQQgghhBDnoSGZp/tsM1znhjsb5q4T5xeJSTEcSVyK4UZiUgw3EpNiODob4nLYztMthBBCCCGEEEKcLyTpFkIIIYQQQgghBokk3UIIIYQQQgghxCCRpFsIIYQQQgghhBgkknQLIYQQQgghhBCDRJJuIYQQQgghhBBikEjSLYQQQgghhBBCDBJJuoUQQgghhBBCiEEiSbcQQgghhBBCCDFIJOkWQgghhBBCCCEGiSTdQgghhBBCCCHEIJGkWwghhBBCCCGEGCSSdAshhBBCCCGEEIPEPNQVOBsoylDX4OSO12u41k+cfyQmxXAkcSmGG4lJMdxITIrh6GyIy49aN8UwDGNwqyKEEEIIIYQQQpyfpHu5EEIIIYQQQggxSCTpFkIIIYQQQgghBokk3UIIIYQQQgghxCCRpFsIIYQQQgghhBgkknQLIYQQQgghhBCDRJJuIYQQQgghhBBikEjSLYQQQgghhBBCDBJJuoUQQgghhBBCiEEiSfdZyu12s2zZMmpqapgxYwZ33303sVhsqKslzgPd3d3MmzePLVu2JMp27drFtddeS3V1NXPnzuW5557r95o1a9Ywb948Jk+ezJIlS9ixY8eZrrY4B9XW1nLzzTczffp0Zs+ezTe+8Q26u7sBiUkxdDZt2sS1117LlClTmD17NitXriQUCgESl2JoaZrGDTfcwJ133pkok5gUQ2XdunWMHTuW6urqxL/ly5cD52hcGuKs9E//9E/G7bffbgQCAaOpqclYsGCB8dhjjw11tcQ5btu2bcall15qVFZWGps3bzYMwzA8Ho8xffp04+mnnzai0ajxzjvvGNXV1cauXbsMwzCMzZs3G9XV1ca2bduMSCRiPPnkk8aMGTOMQCAwlJsiznLBYNCYPXu28dOf/tQIh8NGd3e3ccsttxhf/vKXJSbFkHG73caECROMF154wdA0zWhvbzcWLlxo/PSnP5W4FEPu4YcfNkaPHm1885vfNAxDfr/F0Lr33nuNO++8c0D5uRqXcqf7LHT48GG2bt3K8uXLcTgcFBUVsWzZMlavXj3UVRPnsDVr1nDHHXfwn//5n/3K169fT1paGtdffz1ms5lZs2axaNGiRDw+99xzLFiwgKlTp2KxWLjppptIT09n3bp1Q7EZ4hzR0tLC6NGjufXWW7FaraSnp/O5z32Od999V2JSDJmMjAzeeecdlixZgqIoeDwewuEwGRkZEpdiSG3atIn169dz2WWXJcokJsVQ2rNnD+PHjx9Qfq7GpSTdZ6G6ujrS0tLIzc1NlJWVldHS0oLX6x3Cmolz2YUXXsif/vQnrrzyyn7ldXV1VFZW9isrLy+ntrYWgPr6+tMuF+LvMWrUKB5//HFMJlOi7NVXX2XcuHESk2JIJSUlAfCpT32KRYsWkZ2dzZIlSyQuxZBxu918+9vf5oEHHsDhcCTKJSbFUNF1nb179/L6669zySWXMGfOHL773e/S29t7zsalJN1nIb/f3++kCST+DgQCQ1ElcR7Izs7GbDYPKD9ZPNrt9kQsfthyIf5RhmHw0EMP8dprr/Htb39bYlIMC+vXr+eNN95AVVVuu+02iUsxJHRdZ/ny5dx8882MHj263zKJSTFUuru7GTt2LPPnz2fdunX8/ve/p7GxkeXLl5+zcSlJ91nI6XQSDAb7lR3/2+VyDUWVxHnM4XAkBgk6LhQKJWLxw5YL8Y/w+XzcdtttrF27lqeffpqqqiqJSTEs2O12cnNzWb58OW+++abEpRgSjz76KFarlRtuuGHAMolJMVSysrJYvXo1S5cuxeFwUFBQwPLly3njjTcwDOOcjEtJus9CFRUVeDweurq6EmUNDQ3k5eWRnJw8hDUT56PKykrq6ur6ldXX11NRUQHE4/V0y4X4ezU1NXHNNdfg8/l4/vnnqaqqAiQmxdDZvn07l19+OZFIJFEWiUSwWCyUl5dLXIoz7qWXXmLr1q3U1NRQU1PDyy+/zMsvv0xNTY2cK8WQqa2t5f7778cwjERZJBJBVVUmTpx4TsalJN1noZKSEqZOnco999yDz+ejubmZVatWsXTp0qGumjgPzZs3j66uLn7zm98QjUbZvHkza9eu5ZprrgFg6dKlrF27ls2bNxONRvnNb36D2+1m3rx5Q1xzcTbr7e3lxhtvZMqUKTzxxBNkZGQklklMiqFSVVVFKBTigQceIBKJcPToUX784x+zdOlS5s+fL3Epzrg//vGPbN++nW3btrFt2zYWLlzIwoUL2bZtm5wrxZBJS0tj9erVPP7448RiMVpaWvjJT37C1Vdffc6eKxXjr5sYxFmjq6uLH/zgB2zZsgVVVbnqqqu44447+g0qJMRgqaqq4re//S0zZswA4iNQ3n333Rw4cICMjAyWLVvGkiVLEuu/9NJL/OIXv6C9vZ3y8nK+853vMGnSpKGqvjgHPPnkk9x77704HA4URem3bMeOHRKTYsjU19dzzz33sGfPHpKTk1m0aFFilH2JSzHUjs/Rfe+99wLy+y2GztatW3nwwQc5cOAANpuNBQsWsHz5cmw22zkZl5J0CyGEEEIIIYQQg0S6lwshhBBCCCGEEINEkm4hhBBCCCGEEGKQSNIthBBCCCGEEEIMEkm6hRBCCCGEEEKIQSJJtxBCCCGEEEIIMUgk6RZCCCGEEEIIIQaJJN1CCCGEEEIIIcQgkaRbCCGEEEIIIYQYJOahroAQQgghBt/cuXPp7OzEbB740//YY49RU1MzKJ975513AnDvvfcOyvsLIYQQw50k3UIIIcR54q677mLJkiVDXQ0hhBDivCLdy4UQQgjB3Llz+a//+i/mz59PdXU1119/PfX19Ynl27Zt4/rrr6empoa5c+fy8MMPE4lEEsufeuop5s2bR3V1NUuWLGHTpk2JZW63m9tuu40ZM2Zw4YUX8vTTT5/RbRNCCCGGkiTdQgghhADgmWee4eGHH2bTpk2UlZXxb//2b0SjUQ4ePMjNN9/MZZddxjvvvMOTTz7Jhg0buO+++wB48cUXWbVqFffddx/vvfcen//85/nKV76Cx+MBYPPmzVx33XVs3ryZ22+/nR/+8Ie0t7cP4ZYKIYQQZ45iGIYx1JUQQgghxOCaO3cubrcbi8XSrzw/P5+1a9cyd+5cvvjFL3LTTTcBEAwGqamp4de//jWbN2/mzTff5Pnnn0+8buPGjdx2223s2LGDG2+8kerqar7+9a8nlm/fvp2xY8fy/e9/H4/Hwy9/+UsAIpEIEyZMYPXq1YP2HLkQQggxnMgz3UIIIcR5YsWKFad9pnvkyJGJ/zscDtLS0ujs7MTtdlNUVNRv3cLCQkKhEG63m87OTgoKCvotnzJlSuL/aWlpif9brVYANE37RzZFCCGEOGtI93IhhBBCAPTr8u33++np6SE/P58RI0bQ1NTUb92mpiasViupqank5+fT2trab/lDDz1EQ0PDGam3EEIIMZxJ0i2EEEIIAJ588kkOHz5MMBjkRz/6EaNGjaK6upoFCxbQ0NDAU089RSQSoampiQcffJBFixZhtVpZsmQJzzzzDLt370bXdV544QVWr15Nenr6UG+SEEIIMeSke7kQQghxnlixYgUrV64cUL5s2TIApk6dyq233kpLSwvTpk3jV7/6FaqqUlhYyOOPP86DDz7II488gt1uZ+HChXzta18DYNGiRXi9XpYvX05nZyfl5eU89thjZGRknMnNE0IIIYYlGUhNCCGEEMydO5d///d/l3m8hRBCiE+YdC8XQgghhBBCCCEGiSTdQgghhBBCCCHEIJHu5UIIIYQQQgghxCCRO91CCCGEEEIIIcQgkaRbCCGEEEIIIYQYJJJ0CyGEEEIIIYQQg0SSbiGEEEIIIYQQYpBI0i2EEEIIIYQQQgwSSbqFEEIIIYQQQohBIkm3EEIIIYQQQggxSCTpFkIIIYQQQgghBokk3UIIIYQQQgghxCD5/z8K6tnJW+3oAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "True: [0.04221747 0.5579899  0.43734697 0.03586017 0.49723685]\n",
      "Predicted: [0.0464725  0.4792248  0.47814238 0.04446491 0.45720947]\n",
      "\n",
      "Sample 2:\n",
      "True: [0.00378544 0.33897007 0.5518356  0.00888209 0.33450213]\n",
      "Predicted: [0.01151737 0.3314762  0.5573982  0.01761271 0.32727852]\n",
      "\n",
      "Sample 3:\n",
      "True: [0.01155882 0.328477   0.96993846 0.01599594 0.319793  ]\n",
      "Predicted: [0.00793231 0.3571034  0.93590915 0.01518686 0.36063454]\n",
      "\n",
      "Sample 4:\n",
      "True: [0.0112426  0.3724491  0.9354029  0.01557722 0.3721758 ]\n",
      "Predicted: [0.00717761 0.36638048 0.9171765  0.0145117  0.37040153]\n",
      "\n",
      "Sample 5:\n",
      "True: [0.01051408 0.36150497 0.8055652  0.01456426 0.36226568]\n",
      "Predicted: [ 0.00099619  0.29801908  0.82547605 -0.00091948  0.28790268]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:05:13.372412Z",
     "start_time": "2024-09-25T08:05:13.354112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(f'{file_model_path}Array/train_losses.npy', train_losses)\n",
    "np.save(f'{file_model_path}Array/test_losses.npy', val_losses)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:35:59.708154Z",
     "start_time": "2024-09-25T08:35:59.703834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_hyperparameters = best_trial.params  # Extract best hyperparameters\n",
    "\n",
    "# Save the dictionary containing the best hyperparameters\n",
    "torch.save(best_hyperparameters, f'{file_model_path}Model\\\\Best_Hyperparameters.pth')"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LOAD ARRAY**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:16.007800Z",
     "start_time": "2024-09-26T15:27:15.794460Z"
    }
   },
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the arrays\n",
    "y_test = np.load(f'{file_model_path}Array/y_test_true.npy')\n",
    "y_pred = np.load(f'{file_model_path}Array/y_test_pred.npy')\n",
    "y_train = np.load(f'{file_model_path}Array/y_train_true.npy')\n",
    "y_pred_train = np.load(f'{file_model_path}Array/y_train_pred.npy')\n",
    "test_indices = np.load(f'{file_model_path}Array/test_indices.npy')\n",
    "train_indices = np.load(f'{file_model_path}Array/train_indices.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load the scalers\n",
    "scaler_X = joblib.load('D:/Cloud Folder/OneDrive - UGM 365/(00) Penelitian Ibuk/Model/Scaler_X.joblib')\n",
    "scaler_y = joblib.load('D:/Cloud Folder/OneDrive - UGM 365/(00) Penelitian Ibuk/Model/Scaler_y.joblib')\n",
    "\n",
    "# Inverse transform the predictions and true values to original scale\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_orig = scaler_y.inverse_transform(y_pred)\n",
    "y_train_orig = scaler_y.inverse_transform(y_train)\n",
    "y_pred_train_orig = scaler_y.inverse_transform(y_pred_train)\n",
    "\n",
    "# Change shape array\n",
    "y_test = y_test.reshape(6006,3)\n",
    "y_pred = y_pred.reshape(6006,3)\n",
    "y_train = y_train.reshape(23562,3)\n",
    "y_pred_train = y_pred_train.reshape(23562,3)\n",
    "\n",
    "y_test_orig = y_test_orig.reshape(6006, 3)\n",
    "y_pred_orig = y_pred_orig.reshape(6006, 3)\n",
    "y_train_orig = y_train_orig.reshape(23562, 3)\n",
    "y_pred_train_orig = y_pred_train_orig.reshape(23562, 3)\n",
    "\n",
    "\n",
    "\n",
    "# Create the arrays\n",
    "array_y_test = []\n",
    "array_y_pred = []\n",
    "array_y_test_orig = []\n",
    "array_y_pred_orig = []\n",
    "array_y_train = []\n",
    "array_y_pred_train = []\n",
    "array_y_train_orig = []\n",
    "array_y_pred_train_orig = []\n",
    "\n",
    "pred_array = []  # Initialize pred_array\n",
    "results_array = []  # Initialize result_array\n",
    "\n",
    "# Create 154 output array\n",
    "for i in range(154):\n",
    "    array_y_test_orig.append(y_test_orig[i::154])\n",
    "    array_y_pred_orig.append(y_pred_orig[i::154])\n",
    "    array_y_test.append(y_test[i::154])\n",
    "    array_y_pred.append(y_pred[i::154])\n",
    "    array_y_train.append(y_train[i::154])\n",
    "    array_y_pred_train.append(y_pred_train[i::154])\n",
    "    array_y_train_orig.append(y_train_orig[i::154])\n",
    "    array_y_pred_train_orig.append(y_pred_train_orig[i::154])\n",
    "\n",
    "for i in range(154):\n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(array_y_train[i], array_y_pred_train[i])\n",
    "    mae_train = mean_absolute_error(array_y_train[i], array_y_pred_train[i])\n",
    "    mse_train = mean_squared_error(array_y_train[i], array_y_pred_train[i])\n",
    "\n",
    "    r2_test = r2_score(array_y_test[i], array_y_pred[i])\n",
    "    mae_test = mean_absolute_error(array_y_test[i], array_y_pred[i])\n",
    "    mse_test = mean_squared_error(array_y_test[i], array_y_pred[i])\n",
    "\n",
    "    array1 = np.array(test_indices)\n",
    "    array2 = np.array(array_y_pred_orig[i])\n",
    "    array3 = np.array(array_y_test_orig[i])\n",
    "\n",
    "    # Calculate the percentage error where y_test is not zero\n",
    "    percentage_error = np.zeros_like(array2)  # Initialize with zeros\n",
    "    mask = array3 != 0  # Mask where y_test is not zero\n",
    "\n",
    "    # Calculate percentage error only where y_test is not zero\n",
    "    percentage_error[mask] = np.abs((array2[mask] - array3[mask]) / array3[mask]) * 100\n",
    "\n",
    "    # Initialize mean error array\n",
    "    mean_error = np.zeros(array3.shape[0])\n",
    "\n",
    "    # Calculate mean error for each row, ignoring zeros in y_test\n",
    "    for j in range(array3.shape[0]):\n",
    "        if np.any(mask[j]):  # Check if there are any valid entries in the row\n",
    "            mean_error[j] = np.mean(percentage_error[j][mask[j]])\n",
    "\n",
    "    # Append the mean percentage error as a new column to percentage_error\n",
    "    percentage_error_with_mean = np.hstack((percentage_error, mean_error.reshape(-1, 1)))\n",
    "\n",
    "    frame = np.hstack((array1.reshape(39,1), array2, array3, percentage_error_with_mean))\n",
    "    pred_array.append(frame)\n",
    "    results_array.append((r2_train, mae_train, mse_train, r2_test, mae_test, mse_test))\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    'R2 (Train)', 'MAE (Train)', 'MSE (Train)',\n",
    "    'R2 (Test)', 'MAE (Test)', 'MSE (Test)'\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "results_df = pd.DataFrame(results_array, columns=columns)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KOrmS7YH70pR",
    "outputId": "63646c58-a2dc-41b5-95fd-5ad227679416",
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:17.231141Z",
     "start_time": "2024-09-26T15:27:17.227030Z"
    }
   },
   "source": [
    "np_array = np.array(pred_array)\n",
    "\n",
    "# Stack the arrays along a new axis to create a 3D array of shape (154, 39, 7)\n",
    "combined_array = np.stack(np_array)\n",
    "\n",
    "# Transpose the array to swap the first two axes, resulting in a shape of (39, 154, 7)\n",
    "predict_np_transpose = combined_array.transpose(1, 0, 2)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "J8V-BNZrQpn1",
    "outputId": "f3a4640d-1589-418a-da75-1185557e100e",
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:17.938455Z",
     "start_time": "2024-09-26T15:27:17.932035Z"
    }
   },
   "source": [
    "results_df.describe"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of      R2 (Train)  MAE (Train)  MSE (Train)  R2 (Test)  MAE (Test)  MSE (Test)\n",
       "0      0.938339     0.041686     0.003666   0.859681    0.058801    0.007850\n",
       "1      0.958285     0.034274     0.002572   0.888430    0.045609    0.004841\n",
       "2      0.965809     0.029159     0.001733   0.911394    0.042712    0.004096\n",
       "3      0.973858     0.027680     0.001404   0.907848    0.046115    0.004746\n",
       "4      0.977123     0.026427     0.001301   0.931264    0.040137    0.003863\n",
       "..          ...          ...          ...        ...         ...         ...\n",
       "149    0.958529     0.031444     0.001817   0.932779    0.035989    0.002152\n",
       "150    0.966761     0.029854     0.001632   0.937346    0.036211    0.002260\n",
       "151    0.960723     0.033249     0.002154   0.793899    0.051103    0.008376\n",
       "152    0.959721     0.033480     0.002180   0.938565    0.037712    0.002403\n",
       "153    0.958732     0.033346     0.002306   0.931740    0.039680    0.003023\n",
       "\n",
       "[154 rows x 6 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rN8jmG69GnB4",
    "outputId": "baf94381-7849-4852-9277-5605c8ba5f18",
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:34.069384Z",
     "start_time": "2024-09-26T15:27:33.997219Z"
    }
   },
   "source": [
    "df = results_df\n",
    "\n",
    "# Define the path to save the Excel file\n",
    "excel_file_path = f'{file_model_path}Excel/Test Results.xlsx'\n",
    "\n",
    "# Save the DataFrame to an Excel file using the xlsxwriter engine\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, sheet_name='Results', index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {excel_file_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to D:\\Cloud Folder\\OneDrive - UGM 365\\(00) Penelitian Ibuk\\Model\\240924 - Model ANN\\Excel/Test Results.xlsx\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hrswYV67AjMK",
    "outputId": "8e3cdbea-d0f5-4780-ea39-cd0e7066f5dd",
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:34.948447Z",
     "start_time": "2024-09-26T15:27:34.724860Z"
    }
   },
   "source": [
    "# Load the spreadsheet\n",
    "excel_file_path = f'{file_model_path}Excel/Test Results.xlsx'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary = df.describe().loc[['min', 'max', 'mean', 'std']]\n",
    "\n",
    "# Create a new sheet with the summary\n",
    "with pd.ExcelWriter(excel_file_path, mode='a', engine='openpyxl') as writer:\n",
    "    summary.to_excel(writer, sheet_name='Summary Statistics')\n",
    "\n",
    "print(\"Summary statistics have been added to the new sheet 'Summary Statistics'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics have been added to the new sheet 'Summary Statistics'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hP99L2SaX7ew",
    "outputId": "663f051a-dd95-4346-e0d9-0074b1ca94f9",
    "ExecuteTime": {
     "end_time": "2024-09-26T15:27:36.649921Z",
     "start_time": "2024-09-26T15:27:35.548258Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the path to save the Excel file\n",
    "excel_file_path = f'{file_model_path}Excel/Data Results.xlsx'\n",
    "\n",
    "# Use pandas ExcelWriter to write each array to a separate sheet\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    for i, array in enumerate(predict_np_transpose):\n",
    "        df = pd.DataFrame(array)\n",
    "        df.to_excel(writer, sheet_name=f'{i+1}', index=False)\n",
    "\n",
    "print(f\"Data saved to {excel_file_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to D:\\Cloud Folder\\OneDrive - UGM 365\\(00) Penelitian Ibuk\\Model\\240924 - Model ANN\\Excel/Data Results.xlsx\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
